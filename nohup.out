/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 12:45:40,571] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 12:45:41,556] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2026-01-09 12:45:41,633] [INFO] [runner.py:571:main] cmd = /data3/jisu/miniconda3/envs/mfm-new/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNl19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path liuhaotian/llava-v1.5-7b --version v1 --data_path /data3/jisu/LLaVA/visa_llava_instruct.json --image_folder /data3/jisu/MFM/datasets/ViSA --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 False --fp16 True --output_dir ./checkpoints/llava-v1.5-7b-mfm-lora --num_train_epochs 3 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 False --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 12:45:43,524] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 12:45:44,514] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6]}
[2026-01-09 12:45:44,514] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=7, node_rank=0
[2026-01-09 12:45:44,514] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6]})
[2026-01-09 12:45:44,514] [INFO] [launch.py:163:main] dist_world_size=7
[2026-01-09 12:45:44,514] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 12:45:48,854] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 12:45:49,045] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 12:45:49,316] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 12:45:49,324] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 12:45:49,398] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 12:45:49,406] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 12:45:49,412] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 12:45:49,417] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 12:45:49,508] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 12:45:49,518] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 12:45:49,591] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 12:45:49,591] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2026-01-09 12:45:49,596] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 12:45:49,612] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 12:45:49,621] [INFO] [comm.py:637:init_distributed] cdb=None
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s][2026-01-09 12:46:34,525] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 609912
[2026-01-09 12:46:34,641] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 609913
[2026-01-09 12:46:34,642] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 609914
[2026-01-09 12:46:34,956] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 609915
[2026-01-09 12:46:35,269] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 609916
[2026-01-09 12:46:35,582] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 609917
[2026-01-09 12:46:35,935] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 609918
[2026-01-09 12:46:36,248] [ERROR] [launch.py:321:sigkill_handler] ['/data3/jisu/miniconda3/envs/mfm-new/bin/python3.11', '-u', 'llava/train/train_mem.py', '--local_rank=6', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/data3/jisu/LLaVA/visa_llava_instruct.json', '--image_folder', '/data3/jisu/MFM/datasets/ViSA', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'False', '--fp16', 'True', '--output_dir', './checkpoints/llava-v1.5-7b-mfm-lora', '--num_train_epochs', '3', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.0', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = -9
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 12:47:08,983] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 12:47:10,001] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=3,4,5,6: setting --include=localhost:3,4,5,6
[2026-01-09 12:47:10,061] [INFO] [runner.py:571:main] cmd = /data3/jisu/miniconda3/envs/mfm-new/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMywgNCwgNSwgNl19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path liuhaotian/llava-v1.5-7b --version v1 --data_path /data3/jisu/LLaVA/visa_llava_instruct.json --image_folder /data3/jisu/MFM/datasets/ViSA --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 False --fp16 True --output_dir ./checkpoints/llava-v1.5-7b-mfm-lora --num_train_epochs 3 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 False --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 12:47:11,990] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 12:47:13,007] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [3, 4, 5, 6]}
[2026-01-09 12:47:13,007] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2026-01-09 12:47:13,007] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2026-01-09 12:47:13,007] [INFO] [launch.py:163:main] dist_world_size=4
[2026-01-09 12:47:13,007] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=3,4,5,6
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 12:47:17,111] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 12:47:17,133] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 12:47:17,191] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 12:47:17,258] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 12:47:17,302] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 12:47:17,327] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 12:47:17,389] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 12:47:17,469] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 12:47:17,469] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:  50%|█████     | 1/2 [08:58<08:58, 538.84s/it]Downloading shards:  50%|█████     | 1/2 [08:58<08:58, 538.72s/it]Downloading shards:  50%|█████     | 1/2 [08:58<08:58, 538.73s/it]Downloading shards:  50%|█████     | 1/2 [08:58<08:58, 538.71s/it]Downloading shards: 100%|██████████| 2/2 [12:12<00:00, 336.06s/it]Downloading shards: 100%|██████████| 2/2 [12:12<00:00, 366.46s/it]
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Downloading shards: 100%|██████████| 2/2 [12:12<00:00, 336.03s/it]Downloading shards: 100%|██████████| 2/2 [12:12<00:00, 366.43s/it]
Downloading shards: 100%|██████████| 2/2 [12:12<00:00, 336.07s/it]Downloading shards: 100%|██████████| 2/2 [12:12<00:00, 366.47s/it]
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.
Downloading shards: 100%|██████████| 2/2 [12:13<00:00, 336.15s/it]Downloading shards: 100%|██████████| 2/2 [12:13<00:00, 366.56s/it]
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.
[2026-01-09 12:59:48,148] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 295, num_elems = 6.76B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.14s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.29s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:38<01:38, 98.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:41<00:00, 57.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:41<00:00, 50.64s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:41<00:00, 57.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:41<00:00, 50.66s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:41<00:00, 57.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:41<00:00, 50.65s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:41<00:00, 42.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:41<00:00, 50.79s/it]
Adding LoRA adapters...
[2026-01-09 13:02:52,032] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 686, num_elems = 7.06B
Formatting inputs...Skip in lazy mode
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Parameter Offload: Total persistent parameters: 599040 in 312 params
Traceback (most recent call last):
  File "/data3/jisu/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/data3/jisu/LLaVA/llava/train/train.py", line 969, in train
    trainer.train()
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1787, in _inner_training_loop
    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer_callback.py", line 370, in on_train_begin
    return self.call_event("on_train_begin", args, state, control)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer_callback.py", line 414, in call_event
    result = getattr(callback, event)(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/integrations/integration_utils.py", line 767, in on_train_begin
    self.setup(args, state, model, **kwargs)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/integrations/integration_utils.py", line 740, in setup
    self._wandb.init(
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/wandb/sdk/wandb_init.py", line 1594, in init
    get_sentry().reraise(e)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/wandb/analytics/sentry.py", line 190, in reraise
    raise exc.with_traceback(tb)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/wandb/sdk/wandb_init.py", line 1515, in init
    wi.maybe_login(init_settings)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/wandb/sdk/wandb_init.py", line 192, in maybe_login
    wandb_login._login(
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/wandb/sdk/wandb_login.py", line 365, in _login
    key, key_status = wlogin.prompt_api_key(referrer=referrer)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/wandb/sdk/wandb_login.py", line 272, in prompt_api_key
    raise UsageError(message) from None
wandb.errors.errors.UsageError: No API key configured. Use `wandb login` to log in.
[2026-01-09 13:03:24,150] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 611204
[2026-01-09 13:03:24,150] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 611205
[2026-01-09 13:03:24,429] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 611206
[2026-01-09 13:03:24,752] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 611207
[2026-01-09 13:03:25,037] [ERROR] [launch.py:321:sigkill_handler] ['/data3/jisu/miniconda3/envs/mfm-new/bin/python3.11', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/data3/jisu/LLaVA/visa_llava_instruct.json', '--image_folder', '/data3/jisu/MFM/datasets/ViSA', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'False', '--fp16', 'True', '--output_dir', './checkpoints/llava-v1.5-7b-mfm-lora', '--num_train_epochs', '3', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.0', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 17:00:51,200] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 17:01:02,813] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=3,4,5,6: setting --include=localhost:3,4,5,6
[2026-01-09 17:01:02,873] [INFO] [runner.py:571:main] cmd = /data3/jisu/miniconda3/envs/mfm-new/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMywgNCwgNSwgNl19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero2.json --model_name_or_path liuhaotian/llava-v1.5-7b --version v1 --data_path /data3/jisu/LLaVA/visa_llava_instruct.json --image_folder /data3/jisu/MFM/datasets/ViSA --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bits 4 --bf16 False --fp16 True --output_dir ./checkpoints/llava-v1.5-7b-mfm-lora --num_train_epochs 3 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 False --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to none
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 17:01:04,850] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 17:01:05,926] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [3, 4, 5, 6]}
[2026-01-09 17:01:05,926] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2026-01-09 17:01:05,926] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2026-01-09 17:01:05,926] [INFO] [launch.py:163:main] dist_world_size=4
[2026-01-09 17:01:05,926] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=3,4,5,6
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 17:01:12,596] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 17:01:12,653] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 17:01:12,656] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 17:01:12,656] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 17:01:13,267] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 17:01:13,267] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2026-01-09 17:01:13,586] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 17:01:13,615] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 17:01:13,615] [INFO] [comm.py:637:init_distributed] cdb=None
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [03:25<03:25, 205.46s/it]Loading checkpoint shards:  50%|█████     | 1/2 [03:26<03:26, 206.21s/it]Loading checkpoint shards:  50%|█████     | 1/2 [03:26<03:26, 206.24s/it]Loading checkpoint shards:  50%|█████     | 1/2 [03:26<03:26, 206.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [04:23<00:00, 118.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [04:23<00:00, 131.88s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [04:24<00:00, 118.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [04:24<00:00, 132.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [04:24<00:00, 118.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [04:24<00:00, 132.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [04:24<00:00, 118.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [04:24<00:00, 132.03s/it]
Adding LoRA adapters...
Formatting inputs...Skip in lazy mode
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
  0%|          | 0/507 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/507 [00:16<2:18:08, 16.38s/it]                                                 {'loss': 2.3151, 'learning_rate': 1.25e-05, 'epoch': 0.01}
  0%|          | 1/507 [00:16<2:18:08, 16.38s/it]  0%|          | 2/507 [00:31<2:13:56, 15.91s/it]                                                 {'loss': 2.2569, 'learning_rate': 2.5e-05, 'epoch': 0.01}
  0%|          | 2/507 [00:31<2:13:56, 15.91s/it]  1%|          | 3/507 [00:47<2:12:19, 15.75s/it]                                                 {'loss': 1.5462, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.02}
  1%|          | 3/507 [00:47<2:12:19, 15.75s/it]  1%|          | 4/507 [01:03<2:12:01, 15.75s/it]                                                 {'loss': 0.8351, 'learning_rate': 5e-05, 'epoch': 0.02}
  1%|          | 4/507 [01:03<2:12:01, 15.75s/it]  1%|          | 5/507 [01:19<2:11:58, 15.77s/it]                                                 {'loss': 0.7695, 'learning_rate': 6.25e-05, 'epoch': 0.03}
  1%|          | 5/507 [01:19<2:11:58, 15.77s/it]  1%|          | 6/507 [01:34<2:12:02, 15.81s/it]                                                 {'loss': 0.7217, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.04}
  1%|          | 6/507 [01:34<2:12:02, 15.81s/it]  1%|▏         | 7/507 [01:50<2:12:16, 15.87s/it]                                                 {'loss': 0.186, 'learning_rate': 8.75e-05, 'epoch': 0.04}
  1%|▏         | 7/507 [01:50<2:12:16, 15.87s/it]  2%|▏         | 8/507 [02:07<2:12:28, 15.93s/it]                                                 {'loss': 0.1958, 'learning_rate': 0.0001, 'epoch': 0.05}
  2%|▏         | 8/507 [02:07<2:12:28, 15.93s/it]  2%|▏         | 9/507 [02:23<2:12:47, 16.00s/it]                                                 {'loss': 0.097, 'learning_rate': 0.00011250000000000001, 'epoch': 0.05}
  2%|▏         | 9/507 [02:23<2:12:47, 16.00s/it]  2%|▏         | 10/507 [02:39<2:13:10, 16.08s/it]                                                  {'loss': 0.1216, 'learning_rate': 0.000125, 'epoch': 0.06}
  2%|▏         | 10/507 [02:39<2:13:10, 16.08s/it]  2%|▏         | 11/507 [02:55<2:13:25, 16.14s/it]                                                  {'loss': 0.2061, 'learning_rate': 0.0001375, 'epoch': 0.06}
  2%|▏         | 11/507 [02:55<2:13:25, 16.14s/it]  2%|▏         | 12/507 [03:11<2:13:24, 16.17s/it]                                                  {'loss': 0.0813, 'learning_rate': 0.00015000000000000001, 'epoch': 0.07}
  2%|▏         | 12/507 [03:11<2:13:24, 16.17s/it]  3%|▎         | 13/507 [03:28<2:13:22, 16.20s/it]                                                  {'loss': 0.2457, 'learning_rate': 0.00016250000000000002, 'epoch': 0.08}
  3%|▎         | 13/507 [03:28<2:13:22, 16.20s/it]  3%|▎         | 14/507 [03:44<2:13:23, 16.24s/it]                                                  {'loss': 0.1239, 'learning_rate': 0.000175, 'epoch': 0.08}
  3%|▎         | 14/507 [03:44<2:13:23, 16.24s/it]  3%|▎         | 15/507 [04:00<2:13:27, 16.28s/it]                                                  {'loss': 0.286, 'learning_rate': 0.0001875, 'epoch': 0.09}
  3%|▎         | 15/507 [04:00<2:13:27, 16.28s/it]  3%|▎         | 16/507 [04:17<2:13:25, 16.30s/it]                                                  {'loss': 0.2164, 'learning_rate': 0.0002, 'epoch': 0.09}
  3%|▎         | 16/507 [04:17<2:13:25, 16.30s/it]  3%|▎         | 17/507 [04:33<2:13:28, 16.34s/it]                                                  {'loss': 0.1566, 'learning_rate': 0.00019999795305919378, 'epoch': 0.1}
  3%|▎         | 17/507 [04:33<2:13:28, 16.34s/it]  4%|▎         | 18/507 [04:50<2:13:22, 16.36s/it]                                                  {'loss': 0.1906, 'learning_rate': 0.0001999918123205744, 'epoch': 0.11}
  4%|▎         | 18/507 [04:50<2:13:22, 16.36s/it]  4%|▎         | 19/507 [05:06<2:13:17, 16.39s/it]                                                  {'loss': 0.1099, 'learning_rate': 0.00019998157803553638, 'epoch': 0.11}
  4%|▎         | 19/507 [05:06<2:13:17, 16.39s/it]  4%|▍         | 20/507 [05:23<2:13:10, 16.41s/it]                                                  {'loss': 0.0873, 'learning_rate': 0.00019996725062305934, 'epoch': 0.12}
  4%|▍         | 20/507 [05:23<2:13:10, 16.41s/it]  4%|▍         | 21/507 [05:39<2:12:59, 16.42s/it]                                                  {'loss': 0.1543, 'learning_rate': 0.00019994883066969053, 'epoch': 0.12}
  4%|▍         | 21/507 [05:39<2:12:59, 16.42s/it]  4%|▍         | 22/507 [05:55<2:12:46, 16.43s/it]                                                  {'loss': 0.1849, 'learning_rate': 0.00019992631892952107, 'epoch': 0.13}
  4%|▍         | 22/507 [05:55<2:12:46, 16.43s/it]  5%|▍         | 23/507 [06:12<2:12:37, 16.44s/it]                                                  {'loss': 0.0654, 'learning_rate': 0.0001998997163241549, 'epoch': 0.14}
  5%|▍         | 23/507 [06:12<2:12:37, 16.44s/it]  5%|▍         | 24/507 [06:28<2:12:26, 16.45s/it]                                                  {'loss': 0.1636, 'learning_rate': 0.00019986902394267118, 'epoch': 0.14}
  5%|▍         | 24/507 [06:28<2:12:26, 16.45s/it]  5%|▍         | 25/507 [06:45<2:12:16, 16.47s/it]                                                  {'loss': 0.1422, 'learning_rate': 0.00019983424304157973, 'epoch': 0.15}
  5%|▍         | 25/507 [06:45<2:12:16, 16.47s/it]  5%|▌         | 26/507 [07:01<2:12:03, 16.47s/it]                                                  {'loss': 0.0518, 'learning_rate': 0.00019979537504476944, 'epoch': 0.15}
  5%|▌         | 26/507 [07:01<2:12:03, 16.47s/it]  5%|▌         | 27/507 [07:18<2:11:53, 16.49s/it]                                                  {'loss': 0.0733, 'learning_rate': 0.00019975242154345008, 'epoch': 0.16}
  5%|▌         | 27/507 [07:18<2:11:53, 16.49s/it]  6%|▌         | 28/507 [07:34<2:11:44, 16.50s/it]                                                  {'loss': 0.0937, 'learning_rate': 0.00019970538429608714, 'epoch': 0.17}
  6%|▌         | 28/507 [07:34<2:11:44, 16.50s/it]  6%|▌         | 29/507 [07:51<2:11:27, 16.50s/it]                                                  {'loss': 0.0909, 'learning_rate': 0.00019965426522832984, 'epoch': 0.17}
  6%|▌         | 29/507 [07:51<2:11:27, 16.50s/it]  6%|▌         | 30/507 [08:07<2:11:12, 16.50s/it]                                                  {'loss': 0.0554, 'learning_rate': 0.0001995990664329323, 'epoch': 0.18}
  6%|▌         | 30/507 [08:07<2:11:12, 16.50s/it]  6%|▌         | 31/507 [08:24<2:10:43, 16.48s/it]                                                  {'loss': 0.0627, 'learning_rate': 0.00019953979016966788, 'epoch': 0.18}
  6%|▌         | 31/507 [08:24<2:10:43, 16.48s/it]  6%|▋         | 32/507 [08:40<2:10:32, 16.49s/it]                                                  {'loss': 0.0679, 'learning_rate': 0.0001994764388652366, 'epoch': 0.19}
  6%|▋         | 32/507 [08:40<2:10:32, 16.49s/it]  7%|▋         | 33/507 [08:57<2:10:14, 16.49s/it]                                                  {'loss': 0.1005, 'learning_rate': 0.00019940901511316582, 'epoch': 0.19}
  7%|▋         | 33/507 [08:57<2:10:14, 16.49s/it]  7%|▋         | 34/507 [09:13<2:10:04, 16.50s/it]                                                  {'loss': 0.1234, 'learning_rate': 0.0001993375216737042, 'epoch': 0.2}
  7%|▋         | 34/507 [09:13<2:10:04, 16.50s/it]  7%|▋         | 35/507 [09:30<2:09:31, 16.46s/it]                                                  {'loss': 0.0677, 'learning_rate': 0.00019926196147370849, 'epoch': 0.21}
  7%|▋         | 35/507 [09:30<2:09:31, 16.46s/it]  7%|▋         | 36/507 [09:46<2:09:23, 16.48s/it]                                                  {'loss': 0.0427, 'learning_rate': 0.0001991823376065238, 'epoch': 0.21}
  7%|▋         | 36/507 [09:46<2:09:23, 16.48s/it]  7%|▋         | 37/507 [10:03<2:09:09, 16.49s/it]                                                  {'loss': 0.057, 'learning_rate': 0.00019909865333185702, 'epoch': 0.22}
  7%|▋         | 37/507 [10:03<2:09:09, 16.49s/it]  7%|▋         | 38/507 [10:19<2:08:52, 16.49s/it]                                                  {'loss': 0.0703, 'learning_rate': 0.00019901091207564324, 'epoch': 0.22}
  7%|▋         | 38/507 [10:19<2:08:52, 16.49s/it]  8%|▊         | 39/507 [10:36<2:08:39, 16.49s/it]                                                  {'loss': 0.0277, 'learning_rate': 0.00019891911742990565, 'epoch': 0.23}
  8%|▊         | 39/507 [10:36<2:08:39, 16.49s/it]  8%|▊         | 40/507 [10:52<2:08:18, 16.49s/it]                                                  {'loss': 0.0568, 'learning_rate': 0.00019882327315260838, 'epoch': 0.24}
  8%|▊         | 40/507 [10:52<2:08:18, 16.49s/it]  8%|▊         | 41/507 [11:09<2:08:06, 16.49s/it]                                                  {'loss': 0.0856, 'learning_rate': 0.00019872338316750265, 'epoch': 0.24}
  8%|▊         | 41/507 [11:09<2:08:06, 16.49s/it]  8%|▊         | 42/507 [11:25<2:07:45, 16.48s/it]                                                  {'loss': 0.074, 'learning_rate': 0.0001986194515639662, 'epoch': 0.25}
  8%|▊         | 42/507 [11:25<2:07:45, 16.48s/it]  8%|▊         | 43/507 [11:42<2:07:24, 16.48s/it]                                                  {'loss': 0.0506, 'learning_rate': 0.00019851148259683585, 'epoch': 0.25}
  8%|▊         | 43/507 [11:42<2:07:24, 16.48s/it]  9%|▊         | 44/507 [11:58<2:07:04, 16.47s/it]                                                  {'loss': 0.0773, 'learning_rate': 0.0001983994806862333, 'epoch': 0.26}
  9%|▊         | 44/507 [11:58<2:07:04, 16.47s/it]  9%|▉         | 45/507 [12:15<2:06:49, 16.47s/it]                                                  {'loss': 0.0789, 'learning_rate': 0.00019828345041738413, 'epoch': 0.27}
  9%|▉         | 45/507 [12:15<2:06:49, 16.47s/it]  9%|▉         | 46/507 [12:31<2:06:35, 16.48s/it]                                                  {'loss': 0.0712, 'learning_rate': 0.00019816339654043022, 'epoch': 0.27}
  9%|▉         | 46/507 [12:31<2:06:35, 16.48s/it]  9%|▉         | 47/507 [12:48<2:06:24, 16.49s/it]                                                  {'loss': 0.0614, 'learning_rate': 0.0001980393239702351, 'epoch': 0.28}
  9%|▉         | 47/507 [12:48<2:06:24, 16.49s/it]  9%|▉         | 48/507 [13:04<2:06:02, 16.48s/it]                                                  {'loss': 0.0812, 'learning_rate': 0.00019791123778618305, 'epoch': 0.28}
  9%|▉         | 48/507 [13:04<2:06:02, 16.48s/it] 10%|▉         | 49/507 [13:20<2:05:42, 16.47s/it]                                                  {'loss': 0.0595, 'learning_rate': 0.00019777914323197064, 'epoch': 0.29}
 10%|▉         | 49/507 [13:20<2:05:42, 16.47s/it] 10%|▉         | 50/507 [13:37<2:05:09, 16.43s/it]                                                  {'loss': 0.0345, 'learning_rate': 0.00019764304571539266, 'epoch': 0.3}
 10%|▉         | 50/507 [13:37<2:05:09, 16.43s/it] 10%|█         | 51/507 [13:53<2:05:02, 16.45s/it]                                                  {'loss': 0.0327, 'learning_rate': 0.00019750295080812023, 'epoch': 0.3}
 10%|█         | 51/507 [13:53<2:05:02, 16.45s/it] 10%|█         | 52/507 [14:10<2:04:43, 16.45s/it]                                                  {'loss': 0.0306, 'learning_rate': 0.00019735886424547306, 'epoch': 0.31}
 10%|█         | 52/507 [14:10<2:04:43, 16.45s/it] 10%|█         | 53/507 [14:26<2:04:33, 16.46s/it]                                                  {'loss': 0.0835, 'learning_rate': 0.0001972107919261844, 'epoch': 0.31}
 10%|█         | 53/507 [14:26<2:04:33, 16.46s/it] 11%|█         | 54/507 [14:43<2:04:18, 16.46s/it]                                                  {'loss': 0.015, 'learning_rate': 0.00019705873991215974, 'epoch': 0.32}
 11%|█         | 54/507 [14:43<2:04:18, 16.46s/it] 11%|█         | 55/507 [14:59<2:04:07, 16.48s/it]                                                  {'loss': 0.0344, 'learning_rate': 0.00019690271442822848, 'epoch': 0.32}
 11%|█         | 55/507 [14:59<2:04:07, 16.48s/it] 11%|█         | 56/507 [15:16<2:03:49, 16.47s/it]                                                  {'loss': 0.0372, 'learning_rate': 0.0001967427218618893, 'epoch': 0.33}
 11%|█         | 56/507 [15:16<2:03:49, 16.47s/it] 11%|█         | 57/507 [15:32<2:03:37, 16.48s/it]                                                  {'loss': 0.0581, 'learning_rate': 0.00019657876876304835, 'epoch': 0.34}
 11%|█         | 57/507 [15:32<2:03:37, 16.48s/it] 11%|█▏        | 58/507 [15:49<2:03:11, 16.46s/it]                                                  {'loss': 0.0359, 'learning_rate': 0.00019641086184375145, 'epoch': 0.34}
 11%|█▏        | 58/507 [15:49<2:03:11, 16.46s/it] 12%|█▏        | 59/507 [16:05<2:02:55, 16.46s/it]                                                  {'loss': 0.0358, 'learning_rate': 0.00019623900797790912, 'epoch': 0.35}
 12%|█▏        | 59/507 [16:05<2:02:55, 16.46s/it] 12%|█▏        | 60/507 [16:22<2:02:37, 16.46s/it]                                                  {'loss': 0.0477, 'learning_rate': 0.00019606321420101512, 'epoch': 0.35}
 12%|█▏        | 60/507 [16:22<2:02:37, 16.46s/it] 12%|█▏        | 61/507 [16:38<2:02:29, 16.48s/it]                                                  {'loss': 0.0482, 'learning_rate': 0.0001958834877098586, 'epoch': 0.36}
 12%|█▏        | 61/507 [16:38<2:02:29, 16.48s/it] 12%|█▏        | 62/507 [16:54<2:02:06, 16.46s/it]                                                  {'loss': 0.0702, 'learning_rate': 0.0001956998358622293, 'epoch': 0.37}
 12%|█▏        | 62/507 [16:54<2:02:06, 16.46s/it] 12%|█▏        | 63/507 [17:11<2:01:54, 16.47s/it]                                                  {'loss': 0.0456, 'learning_rate': 0.00019551226617661648, 'epoch': 0.37}
 12%|█▏        | 63/507 [17:11<2:01:54, 16.47s/it] 13%|█▎        | 64/507 [17:28<2:01:43, 16.49s/it]                                                  {'loss': 0.0265, 'learning_rate': 0.00019532078633190095, 'epoch': 0.38}
 13%|█▎        | 64/507 [17:28<2:01:43, 16.49s/it] 13%|█▎        | 65/507 [17:44<2:01:29, 16.49s/it]                                                  {'loss': 0.0518, 'learning_rate': 0.00019512540416704094, 'epoch': 0.38}
 13%|█▎        | 65/507 [17:44<2:01:29, 16.49s/it] 13%|█▎        | 66/507 [18:01<2:01:15, 16.50s/it]                                                  {'loss': 0.0536, 'learning_rate': 0.00019492612768075092, 'epoch': 0.39}
 13%|█▎        | 66/507 [18:01<2:01:15, 16.50s/it] 13%|█▎        | 67/507 [18:17<2:00:48, 16.47s/it]                                                  {'loss': 0.0292, 'learning_rate': 0.00019472296503117437, 'epoch': 0.4}
 13%|█▎        | 67/507 [18:17<2:00:48, 16.47s/it] 13%|█▎        | 68/507 [18:33<2:00:25, 16.46s/it]                                                  {'loss': 0.0311, 'learning_rate': 0.00019451592453554955, 'epoch': 0.4}
 13%|█▎        | 68/507 [18:33<2:00:25, 16.46s/it] 14%|█▎        | 69/507 [18:50<2:00:16, 16.48s/it]                                                  {'loss': 0.0386, 'learning_rate': 0.00019430501466986933, 'epoch': 0.41}
 14%|█▎        | 69/507 [18:50<2:00:16, 16.48s/it] 14%|█▍        | 70/507 [19:06<2:00:06, 16.49s/it]                                                  {'loss': 0.046, 'learning_rate': 0.0001940902440685339, 'epoch': 0.41}
 14%|█▍        | 70/507 [19:06<2:00:06, 16.49s/it] 14%|█▍        | 71/507 [19:23<1:59:48, 16.49s/it]                                                  {'loss': 0.04, 'learning_rate': 0.0001938716215239974, 'epoch': 0.42}
 14%|█▍        | 71/507 [19:23<1:59:48, 16.49s/it] 14%|█▍        | 72/507 [19:39<1:59:37, 16.50s/it]                                                  {'loss': 0.0332, 'learning_rate': 0.00019364915598640793, 'epoch': 0.43}
 14%|█▍        | 72/507 [19:39<1:59:37, 16.50s/it] 14%|█▍        | 73/507 [19:56<1:59:26, 16.51s/it]                                                  {'loss': 0.098, 'learning_rate': 0.00019342285656324135, 'epoch': 0.43}
 14%|█▍        | 73/507 [19:56<1:59:26, 16.51s/it] 15%|█▍        | 74/507 [20:12<1:59:11, 16.52s/it]                                                  {'loss': 0.0614, 'learning_rate': 0.00019319273251892805, 'epoch': 0.44}
 15%|█▍        | 74/507 [20:12<1:59:11, 16.52s/it] 15%|█▍        | 75/507 [20:29<1:58:27, 16.45s/it]                                                  {'loss': 0.047, 'learning_rate': 0.000192958793274474, 'epoch': 0.44}
 15%|█▍        | 75/507 [20:29<1:58:27, 16.45s/it] 15%|█▍        | 76/507 [20:45<1:58:21, 16.48s/it]                                                  {'loss': 0.0442, 'learning_rate': 0.00019272104840707487, 'epoch': 0.45}
 15%|█▍        | 76/507 [20:45<1:58:21, 16.48s/it] 15%|█▌        | 77/507 [21:02<1:58:01, 16.47s/it]                                                  {'loss': 0.0431, 'learning_rate': 0.0001924795076497241, 'epoch': 0.45}
 15%|█▌        | 77/507 [21:02<1:58:01, 16.47s/it] 15%|█▌        | 78/507 [21:18<1:57:52, 16.49s/it]                                                  {'loss': 0.0717, 'learning_rate': 0.0001922341808908144, 'epoch': 0.46}
 15%|█▌        | 78/507 [21:18<1:57:52, 16.49s/it] 16%|█▌        | 79/507 [21:35<1:57:38, 16.49s/it]                                                  {'loss': 0.0601, 'learning_rate': 0.00019198507817373272, 'epoch': 0.47}
 16%|█▌        | 79/507 [21:35<1:57:38, 16.49s/it] 16%|█▌        | 80/507 [21:51<1:57:26, 16.50s/it]                                                  {'loss': 0.0523, 'learning_rate': 0.00019173220969644948, 'epoch': 0.47}
 16%|█▌        | 80/507 [21:51<1:57:26, 16.50s/it] 16%|█▌        | 81/507 [22:08<1:57:12, 16.51s/it]                                                  {'loss': 0.0327, 'learning_rate': 0.00019147558581110078, 'epoch': 0.48}
 16%|█▌        | 81/507 [22:08<1:57:12, 16.51s/it] 16%|█▌        | 82/507 [22:24<1:56:39, 16.47s/it]                                                  {'loss': 0.0406, 'learning_rate': 0.0001912152170235646, 'epoch': 0.48}
 16%|█▌        | 82/507 [22:24<1:56:39, 16.47s/it] 16%|█▋        | 83/507 [22:41<1:56:18, 16.46s/it]                                                  {'loss': 0.0296, 'learning_rate': 0.0001909511139930309, 'epoch': 0.49}
 16%|█▋        | 83/507 [22:41<1:56:18, 16.46s/it] 17%|█▋        | 84/507 [22:57<1:56:11, 16.48s/it]                                                  {'loss': 0.0165, 'learning_rate': 0.00019068328753156513, 'epoch': 0.5}
 17%|█▋        | 84/507 [22:57<1:56:11, 16.48s/it] 17%|█▋        | 85/507 [23:14<1:55:57, 16.49s/it]                                                  {'loss': 0.0402, 'learning_rate': 0.0001904117486036655, 'epoch': 0.5}
 17%|█▋        | 85/507 [23:14<1:55:57, 16.49s/it] 17%|█▋        | 86/507 [23:30<1:55:48, 16.50s/it]                                                  {'loss': 0.0624, 'learning_rate': 0.00019013650832581423, 'epoch': 0.51}
 17%|█▋        | 86/507 [23:30<1:55:48, 16.50s/it] 17%|█▋        | 87/507 [23:47<1:55:16, 16.47s/it]                                                  {'loss': 0.0438, 'learning_rate': 0.00018985757796602252, 'epoch': 0.51}
 17%|█▋        | 87/507 [23:47<1:55:16, 16.47s/it] 17%|█▋        | 88/507 [24:03<1:55:07, 16.49s/it]                                                  {'loss': 0.045, 'learning_rate': 0.00018957496894336898, 'epoch': 0.52}
 17%|█▋        | 88/507 [24:03<1:55:07, 16.49s/it] 18%|█▊        | 89/507 [24:20<1:54:51, 16.49s/it]                                                  {'loss': 0.0207, 'learning_rate': 0.0001892886928275325, 'epoch': 0.53}
 18%|█▊        | 89/507 [24:20<1:54:51, 16.49s/it] 18%|█▊        | 90/507 [24:36<1:54:40, 16.50s/it]                                                  {'loss': 0.0515, 'learning_rate': 0.00018899876133831835, 'epoch': 0.53}
 18%|█▊        | 90/507 [24:36<1:54:40, 16.50s/it] 18%|█▊        | 91/507 [24:53<1:54:25, 16.50s/it]                                                  {'loss': 0.0415, 'learning_rate': 0.0001887051863451784, 'epoch': 0.54}
 18%|█▊        | 91/507 [24:53<1:54:25, 16.50s/it] 18%|█▊        | 92/507 [25:09<1:54:07, 16.50s/it]                                                  {'loss': 0.0295, 'learning_rate': 0.00018840797986672538, 'epoch': 0.54}
 18%|█▊        | 92/507 [25:09<1:54:07, 16.50s/it] 18%|█▊        | 93/507 [25:26<1:53:42, 16.48s/it]                                                  {'loss': 0.0402, 'learning_rate': 0.0001881071540702406, 'epoch': 0.55}
 18%|█▊        | 93/507 [25:26<1:53:42, 16.48s/it] 19%|█▊        | 94/507 [25:42<1:53:30, 16.49s/it]                                                  {'loss': 0.0304, 'learning_rate': 0.00018780272127117607, 'epoch': 0.56}
 19%|█▊        | 94/507 [25:42<1:53:30, 16.49s/it] 19%|█▊        | 95/507 [25:59<1:53:17, 16.50s/it]                                                  {'loss': 0.0342, 'learning_rate': 0.00018749469393265016, 'epoch': 0.56}
 19%|█▊        | 95/507 [25:59<1:53:17, 16.50s/it] 19%|█▉        | 96/507 [26:15<1:52:44, 16.46s/it]                                                  {'loss': 0.0539, 'learning_rate': 0.00018718308466493744, 'epoch': 0.57}
 19%|█▉        | 96/507 [26:15<1:52:44, 16.46s/it] 19%|█▉        | 97/507 [26:32<1:52:32, 16.47s/it]                                                  {'loss': 0.034, 'learning_rate': 0.0001868679062249524, 'epoch': 0.57}
 19%|█▉        | 97/507 [26:32<1:52:32, 16.47s/it] 19%|█▉        | 98/507 [26:48<1:52:21, 16.48s/it]                                                  {'loss': 0.0337, 'learning_rate': 0.00018654917151572729, 'epoch': 0.58}
 19%|█▉        | 98/507 [26:48<1:52:21, 16.48s/it] 20%|█▉        | 99/507 [27:05<1:52:09, 16.49s/it]                                                  {'loss': 0.0441, 'learning_rate': 0.00018622689358588373, 'epoch': 0.58}
 20%|█▉        | 99/507 [27:05<1:52:09, 16.49s/it] 20%|█▉        | 100/507 [27:21<1:51:56, 16.50s/it]                                                   {'loss': 0.0521, 'learning_rate': 0.00018590108562909863, 'epoch': 0.59}
 20%|█▉        | 100/507 [27:21<1:51:56, 16.50s/it] 20%|█▉        | 101/507 [27:38<1:51:42, 16.51s/it]                                                   {'loss': 0.033, 'learning_rate': 0.00018557176098356405, 'epoch': 0.6}
 20%|█▉        | 101/507 [27:38<1:51:42, 16.51s/it] 20%|██        | 102/507 [27:54<1:51:19, 16.49s/it]                                                   {'loss': 0.0373, 'learning_rate': 0.0001852389331314411, 'epoch': 0.6}
 20%|██        | 102/507 [27:54<1:51:19, 16.49s/it] 20%|██        | 103/507 [28:10<1:50:49, 16.46s/it]                                                   {'loss': 0.0393, 'learning_rate': 0.00018490261569830798, 'epoch': 0.61}
 20%|██        | 103/507 [28:10<1:50:49, 16.46s/it] 21%|██        | 104/507 [28:27<1:50:34, 16.46s/it]                                                   {'loss': 0.0246, 'learning_rate': 0.0001845628224526023, 'epoch': 0.61}
 21%|██        | 104/507 [28:27<1:50:34, 16.46s/it] 21%|██        | 105/507 [28:43<1:50:26, 16.48s/it]                                                   {'loss': 0.0732, 'learning_rate': 0.0001842195673050572, 'epoch': 0.62}
 21%|██        | 105/507 [28:43<1:50:26, 16.48s/it] 21%|██        | 106/507 [29:00<1:50:14, 16.49s/it]                                                   {'loss': 0.0337, 'learning_rate': 0.00018387286430813208, 'epoch': 0.63}
 21%|██        | 106/507 [29:00<1:50:14, 16.49s/it] 21%|██        | 107/507 [29:16<1:49:55, 16.49s/it]                                                   {'loss': 0.0374, 'learning_rate': 0.00018352272765543722, 'epoch': 0.63}
 21%|██        | 107/507 [29:16<1:49:55, 16.49s/it] 21%|██▏       | 108/507 [29:33<1:49:35, 16.48s/it]                                                   {'loss': 0.0246, 'learning_rate': 0.0001831691716811526, 'epoch': 0.64}
 21%|██▏       | 108/507 [29:33<1:49:35, 16.48s/it] 21%|██▏       | 109/507 [29:49<1:49:14, 16.47s/it]                                                   {'loss': 0.0199, 'learning_rate': 0.00018281221085944126, 'epoch': 0.64}
 21%|██▏       | 109/507 [29:49<1:49:14, 16.47s/it] 22%|██▏       | 110/507 [30:06<1:49:05, 16.49s/it]                                                   {'loss': 0.0292, 'learning_rate': 0.0001824518598038567, 'epoch': 0.65}
 22%|██▏       | 110/507 [30:06<1:49:05, 16.49s/it] 22%|██▏       | 111/507 [30:22<1:48:51, 16.49s/it]                                                   {'loss': 0.0296, 'learning_rate': 0.00018208813326674444, 'epoch': 0.66}
 22%|██▏       | 111/507 [30:22<1:48:51, 16.49s/it] 22%|██▏       | 112/507 [30:39<1:48:37, 16.50s/it]                                                   {'loss': 0.0339, 'learning_rate': 0.00018172104613863835, 'epoch': 0.66}
 22%|██▏       | 112/507 [30:39<1:48:37, 16.50s/it] 22%|██▏       | 113/507 [30:55<1:48:24, 16.51s/it]                                                   {'loss': 0.0171, 'learning_rate': 0.00018135061344765088, 'epoch': 0.67}
 22%|██▏       | 113/507 [30:55<1:48:24, 16.51s/it] 22%|██▏       | 114/507 [31:12<1:47:50, 16.46s/it]                                                   {'loss': 0.0445, 'learning_rate': 0.0001809768503588578, 'epoch': 0.67}
 22%|██▏       | 114/507 [31:12<1:47:50, 16.46s/it] 23%|██▎       | 115/507 [31:28<1:47:31, 16.46s/it]                                                   {'loss': 0.0261, 'learning_rate': 0.00018059977217367755, 'epoch': 0.68}
 23%|██▎       | 115/507 [31:28<1:47:31, 16.46s/it] 23%|██▎       | 116/507 [31:45<1:47:21, 16.48s/it]                                                   {'loss': 0.0181, 'learning_rate': 0.00018021939432924454, 'epoch': 0.69}
 23%|██▎       | 116/507 [31:45<1:47:21, 16.48s/it] 23%|██▎       | 117/507 [32:01<1:47:03, 16.47s/it]                                                   {'loss': 0.0422, 'learning_rate': 0.00017983573239777748, 'epoch': 0.69}
 23%|██▎       | 117/507 [32:01<1:47:03, 16.47s/it] 23%|██▎       | 118/507 [32:18<1:46:52, 16.49s/it]                                                   {'loss': 0.0316, 'learning_rate': 0.00017944880208594155, 'epoch': 0.7}
 23%|██▎       | 118/507 [32:18<1:46:52, 16.49s/it] 23%|██▎       | 119/507 [32:34<1:46:38, 16.49s/it]                                                   {'loss': 0.0244, 'learning_rate': 0.0001790586192342057, 'epoch': 0.7}
 23%|██▎       | 119/507 [32:34<1:46:38, 16.49s/it] 24%|██▎       | 120/507 [32:51<1:46:13, 16.47s/it]                                                   {'loss': 0.0334, 'learning_rate': 0.00017866519981619394, 'epoch': 0.71}
 24%|██▎       | 120/507 [32:51<1:46:13, 16.47s/it] 24%|██▍       | 121/507 [33:07<1:46:00, 16.48s/it]                                                   {'loss': 0.0397, 'learning_rate': 0.00017826855993803147, 'epoch': 0.71}
 24%|██▍       | 121/507 [33:07<1:46:00, 16.48s/it] 24%|██▍       | 122/507 [33:24<1:45:48, 16.49s/it]                                                   {'loss': 0.07, 'learning_rate': 0.00017786871583768535, 'epoch': 0.72}
 24%|██▍       | 122/507 [33:24<1:45:48, 16.49s/it] 24%|██▍       | 123/507 [33:40<1:45:23, 16.47s/it]                                                   {'loss': 0.0548, 'learning_rate': 0.00017746568388429966, 'epoch': 0.73}
 24%|██▍       | 123/507 [33:40<1:45:23, 16.47s/it] 24%|██▍       | 124/507 [33:57<1:45:10, 16.48s/it]                                                   {'loss': 0.0427, 'learning_rate': 0.00017705948057752545, 'epoch': 0.73}
 24%|██▍       | 124/507 [33:57<1:45:10, 16.48s/it] 25%|██▍       | 125/507 [34:13<1:44:52, 16.47s/it]                                                   {'loss': 0.0509, 'learning_rate': 0.00017665012254684524, 'epoch': 0.74}
 25%|██▍       | 125/507 [34:13<1:44:52, 16.47s/it] 25%|██▍       | 126/507 [34:29<1:44:19, 16.43s/it]                                                   {'loss': 0.0382, 'learning_rate': 0.00017623762655089207, 'epoch': 0.74}
 25%|██▍       | 126/507 [34:29<1:44:19, 16.43s/it] 25%|██▌       | 127/507 [34:46<1:44:13, 16.46s/it]                                                   {'loss': 0.0253, 'learning_rate': 0.0001758220094767638, 'epoch': 0.75}
 25%|██▌       | 127/507 [34:46<1:44:13, 16.46s/it] 25%|██▌       | 128/507 [35:02<1:43:41, 16.41s/it]                                                   {'loss': 0.0451, 'learning_rate': 0.0001754032883393313, 'epoch': 0.76}
 25%|██▌       | 128/507 [35:02<1:43:41, 16.41s/it] 25%|██▌       | 129/507 [35:19<1:43:34, 16.44s/it]                                                   {'loss': 0.0368, 'learning_rate': 0.0001749814802805423, 'epoch': 0.76}
 25%|██▌       | 129/507 [35:19<1:43:34, 16.44s/it] 26%|██▌       | 130/507 [35:35<1:43:21, 16.45s/it]                                                   {'loss': 0.0406, 'learning_rate': 0.00017455660256871931, 'epoch': 0.77}
 26%|██▌       | 130/507 [35:35<1:43:21, 16.45s/it] 26%|██▌       | 131/507 [35:52<1:43:13, 16.47s/it]                                                   {'loss': 0.0508, 'learning_rate': 0.00017412867259785286, 'epoch': 0.77}
 26%|██▌       | 131/507 [35:52<1:43:13, 16.47s/it] 26%|██▌       | 132/507 [36:08<1:43:02, 16.49s/it]                                                   {'loss': 0.0106, 'learning_rate': 0.00017369770788688938, 'epoch': 0.78}
 26%|██▌       | 132/507 [36:08<1:43:02, 16.49s/it] 26%|██▌       | 133/507 [36:25<1:42:52, 16.51s/it]                                                   {'loss': 0.0218, 'learning_rate': 0.00017326372607901386, 'epoch': 0.79}
 26%|██▌       | 133/507 [36:25<1:42:52, 16.51s/it] 26%|██▋       | 134/507 [36:41<1:42:38, 16.51s/it]                                                   {'loss': 0.0616, 'learning_rate': 0.0001728267449409278, 'epoch': 0.79}
 26%|██▋       | 134/507 [36:41<1:42:38, 16.51s/it] 27%|██▋       | 135/507 [36:58<1:42:22, 16.51s/it]                                                   {'loss': 0.0512, 'learning_rate': 0.0001723867823621216, 'epoch': 0.8}
 27%|██▋       | 135/507 [36:58<1:42:22, 16.51s/it] 27%|██▋       | 136/507 [37:14<1:42:08, 16.52s/it]                                                   {'loss': 0.0578, 'learning_rate': 0.00017194385635414244, 'epoch': 0.8}
 27%|██▋       | 136/507 [37:14<1:42:08, 16.52s/it] 27%|██▋       | 137/507 [37:31<1:41:54, 16.53s/it]                                                   {'loss': 0.0354, 'learning_rate': 0.00017149798504985665, 'epoch': 0.81}
 27%|██▋       | 137/507 [37:31<1:41:54, 16.53s/it] 27%|██▋       | 138/507 [37:47<1:41:37, 16.53s/it]                                                   {'loss': 0.0283, 'learning_rate': 0.00017104918670270762, 'epoch': 0.82}
 27%|██▋       | 138/507 [37:47<1:41:37, 16.53s/it] 27%|██▋       | 139/507 [38:04<1:41:21, 16.53s/it]                                                   {'loss': 0.046, 'learning_rate': 0.00017059747968596836, 'epoch': 0.82}
 27%|██▋       | 139/507 [38:04<1:41:21, 16.53s/it] 28%|██▊       | 140/507 [38:20<1:41:04, 16.53s/it]                                                   {'loss': 0.0142, 'learning_rate': 0.00017014288249198934, 'epoch': 0.83}
 28%|██▊       | 140/507 [38:20<1:41:04, 16.53s/it] 28%|██▊       | 141/507 [38:37<1:40:47, 16.52s/it]                                                   {'loss': 0.0612, 'learning_rate': 0.00016968541373144156, 'epoch': 0.83}
 28%|██▊       | 141/507 [38:37<1:40:47, 16.52s/it] 28%|██▊       | 142/507 [38:53<1:40:30, 16.52s/it]                                                   {'loss': 0.0098, 'learning_rate': 0.0001692250921325544, 'epoch': 0.84}
 28%|██▊       | 142/507 [38:53<1:40:30, 16.52s/it] 28%|██▊       | 143/507 [39:10<1:39:51, 16.46s/it]                                                   {'loss': 0.0261, 'learning_rate': 0.0001687619365403492, 'epoch': 0.84}
 28%|██▊       | 143/507 [39:10<1:39:51, 16.46s/it] 28%|██▊       | 144/507 [39:26<1:39:41, 16.48s/it]                                                   {'loss': 0.0538, 'learning_rate': 0.0001682959659158676, 'epoch': 0.85}
 28%|██▊       | 144/507 [39:26<1:39:41, 16.48s/it] 29%|██▊       | 145/507 [39:43<1:39:30, 16.49s/it]                                                   {'loss': 0.0142, 'learning_rate': 0.0001678271993353953, 'epoch': 0.86}
 29%|██▊       | 145/507 [39:43<1:39:30, 16.49s/it] 29%|██▉       | 146/507 [39:59<1:39:17, 16.50s/it]                                                   {'loss': 0.0244, 'learning_rate': 0.00016735565598968114, 'epoch': 0.86}
 29%|██▉       | 146/507 [39:59<1:39:17, 16.50s/it] 29%|██▉       | 147/507 [40:16<1:39:01, 16.50s/it]                                                   {'loss': 0.0297, 'learning_rate': 0.00016688135518315144, 'epoch': 0.87}
 29%|██▉       | 147/507 [40:16<1:39:01, 16.50s/it] 29%|██▉       | 148/507 [40:32<1:38:45, 16.51s/it]                                                   {'loss': 0.024, 'learning_rate': 0.00016640431633311973, 'epoch': 0.87}
 29%|██▉       | 148/507 [40:32<1:38:45, 16.51s/it] 29%|██▉       | 149/507 [40:49<1:38:30, 16.51s/it]                                                   {'loss': 0.0328, 'learning_rate': 0.00016592455896899177, 'epoch': 0.88}
 29%|██▉       | 149/507 [40:49<1:38:30, 16.51s/it] 30%|██▉       | 150/507 [41:05<1:38:06, 16.49s/it]                                                   {'loss': 0.0189, 'learning_rate': 0.00016544210273146607, 'epoch': 0.89}
 30%|██▉       | 150/507 [41:05<1:38:06, 16.49s/it] 30%|██▉       | 151/507 [41:22<1:37:55, 16.50s/it]                                                   {'loss': 0.0383, 'learning_rate': 0.0001649569673717298, 'epoch': 0.89}
 30%|██▉       | 151/507 [41:22<1:37:55, 16.50s/it] 30%|██▉       | 152/507 [41:38<1:37:28, 16.47s/it]                                                   {'loss': 0.0395, 'learning_rate': 0.0001644691727506503, 'epoch': 0.9}
 30%|██▉       | 152/507 [41:38<1:37:28, 16.47s/it] 30%|███       | 153/507 [41:55<1:37:08, 16.47s/it]                                                   {'loss': 0.0267, 'learning_rate': 0.00016397873883796182, 'epoch': 0.9}
 30%|███       | 153/507 [41:55<1:37:08, 16.47s/it] 30%|███       | 154/507 [42:11<1:36:59, 16.49s/it]                                                   {'loss': 0.0277, 'learning_rate': 0.00016348568571144815, 'epoch': 0.91}
 30%|███       | 154/507 [42:11<1:36:59, 16.49s/it] 31%|███       | 155/507 [42:28<1:36:46, 16.50s/it]                                                   {'loss': 0.0478, 'learning_rate': 0.0001629900335561206, 'epoch': 0.92}
 31%|███       | 155/507 [42:28<1:36:46, 16.50s/it] 31%|███       | 156/507 [42:44<1:36:25, 16.48s/it]                                                   {'loss': 0.0486, 'learning_rate': 0.0001624918026633916, 'epoch': 0.92}
 31%|███       | 156/507 [42:44<1:36:25, 16.48s/it] 31%|███       | 157/507 [43:01<1:36:12, 16.49s/it]                                                   {'loss': 0.025, 'learning_rate': 0.00016199101343024403, 'epoch': 0.93}
 31%|███       | 157/507 [43:01<1:36:12, 16.49s/it] 31%|███       | 158/507 [43:17<1:35:58, 16.50s/it]                                                   {'loss': 0.0213, 'learning_rate': 0.00016148768635839623, 'epoch': 0.93}
 31%|███       | 158/507 [43:17<1:35:58, 16.50s/it] 31%|███▏      | 159/507 [43:34<1:35:45, 16.51s/it]                                                   {'loss': 0.0351, 'learning_rate': 0.0001609818420534627, 'epoch': 0.94}
 31%|███▏      | 159/507 [43:34<1:35:45, 16.51s/it] 32%|███▏      | 160/507 [43:50<1:35:31, 16.52s/it]                                                   {'loss': 0.0414, 'learning_rate': 0.00016047350122411037, 'epoch': 0.95}
 32%|███▏      | 160/507 [43:50<1:35:31, 16.52s/it] 32%|███▏      | 161/507 [44:07<1:35:05, 16.49s/it]                                                   {'loss': 0.0478, 'learning_rate': 0.00015996268468121102, 'epoch': 0.95}
 32%|███▏      | 161/507 [44:07<1:35:05, 16.49s/it] 32%|███▏      | 162/507 [44:23<1:34:49, 16.49s/it]                                                   {'loss': 0.0361, 'learning_rate': 0.00015944941333698913, 'epoch': 0.96}
 32%|███▏      | 162/507 [44:23<1:34:49, 16.49s/it] 32%|███▏      | 163/507 [44:40<1:34:24, 16.47s/it]                                                   {'loss': 0.0546, 'learning_rate': 0.00015893370820416593, 'epoch': 0.96}
 32%|███▏      | 163/507 [44:40<1:34:24, 16.47s/it] 32%|███▏      | 164/507 [44:56<1:34:00, 16.44s/it]                                                   {'loss': 0.0332, 'learning_rate': 0.00015841559039509896, 'epoch': 0.97}
 32%|███▏      | 164/507 [44:56<1:34:00, 16.44s/it] 33%|███▎      | 165/507 [45:13<1:33:50, 16.46s/it]                                                   {'loss': 0.031, 'learning_rate': 0.00015789508112091803, 'epoch': 0.97}
 33%|███▎      | 165/507 [45:13<1:33:50, 16.46s/it] 33%|███▎      | 166/507 [45:29<1:33:41, 16.49s/it]                                                   {'loss': 0.0357, 'learning_rate': 0.00015737220169065655, 'epoch': 0.98}
 33%|███▎      | 166/507 [45:29<1:33:41, 16.49s/it] 33%|███▎      | 167/507 [45:46<1:33:16, 16.46s/it]                                                   {'loss': 0.0288, 'learning_rate': 0.00015684697351037936, 'epoch': 0.99}
 33%|███▎      | 167/507 [45:46<1:33:16, 16.46s/it] 33%|███▎      | 168/507 [46:02<1:33:04, 16.47s/it]                                                   {'loss': 0.0073, 'learning_rate': 0.00015631941808230638, 'epoch': 0.99}
 33%|███▎      | 168/507 [46:02<1:33:04, 16.47s/it] 33%|███▎      | 169/507 [46:19<1:32:51, 16.48s/it]                                                   {'loss': 0.0219, 'learning_rate': 0.00015578955700393227, 'epoch': 1.0}
 33%|███▎      | 169/507 [46:19<1:32:51, 16.48s/it] 34%|███▎      | 170/507 [46:36<1:33:40, 16.68s/it]                                                   {'loss': 0.0481, 'learning_rate': 0.0001552574119671423, 'epoch': 1.0}
 34%|███▎      | 170/507 [46:36<1:33:40, 16.68s/it] 34%|███▎      | 171/507 [46:52<1:33:06, 16.63s/it]                                                   {'loss': 0.0267, 'learning_rate': 0.00015472300475732426, 'epoch': 1.01}
 34%|███▎      | 171/507 [46:52<1:33:06, 16.63s/it] 34%|███▍      | 172/507 [47:08<1:32:21, 16.54s/it]                                                   {'loss': 0.0416, 'learning_rate': 0.00015418635725247666, 'epoch': 1.02}
 34%|███▍      | 172/507 [47:09<1:32:21, 16.54s/it] 34%|███▍      | 173/507 [47:25<1:32:03, 16.54s/it]                                                   {'loss': 0.0197, 'learning_rate': 0.00015364749142231303, 'epoch': 1.02}
 34%|███▍      | 173/507 [47:25<1:32:03, 16.54s/it] 34%|███▍      | 174/507 [47:42<1:31:45, 16.53s/it]                                                   {'loss': 0.0423, 'learning_rate': 0.00015310642932736253, 'epoch': 1.03}
 34%|███▍      | 174/507 [47:42<1:31:45, 16.53s/it] 35%|███▍      | 175/507 [47:58<1:31:27, 16.53s/it]                                                   {'loss': 0.0314, 'learning_rate': 0.00015256319311806671, 'epoch': 1.03}
 35%|███▍      | 175/507 [47:58<1:31:27, 16.53s/it] 35%|███▍      | 176/507 [48:15<1:31:10, 16.53s/it]                                                   {'loss': 0.0418, 'learning_rate': 0.0001520178050338729, 'epoch': 1.04}
 35%|███▍      | 176/507 [48:15<1:31:10, 16.53s/it] 35%|███▍      | 177/507 [48:31<1:30:42, 16.49s/it]                                                   {'loss': 0.0237, 'learning_rate': 0.0001514702874023236, 'epoch': 1.05}
 35%|███▍      | 177/507 [48:31<1:30:42, 16.49s/it] 35%|███▌      | 178/507 [48:47<1:30:21, 16.48s/it]                                                   {'loss': 0.0289, 'learning_rate': 0.00015092066263814243, 'epoch': 1.05}
 35%|███▌      | 178/507 [48:47<1:30:21, 16.48s/it] 35%|███▌      | 179/507 [49:04<1:30:08, 16.49s/it]                                                   {'loss': 0.0104, 'learning_rate': 0.0001503689532423166, 'epoch': 1.06}
 35%|███▌      | 179/507 [49:04<1:30:08, 16.49s/it] 36%|███▌      | 180/507 [49:20<1:29:55, 16.50s/it]                                                   {'loss': 0.0212, 'learning_rate': 0.00014981518180117557, 'epoch': 1.06}
 36%|███▌      | 180/507 [49:20<1:29:55, 16.50s/it] 36%|███▌      | 181/507 [49:37<1:29:34, 16.49s/it]                                                   {'loss': 0.041, 'learning_rate': 0.00014925937098546652, 'epoch': 1.07}
 36%|███▌      | 181/507 [49:37<1:29:34, 16.49s/it] 36%|███▌      | 182/507 [49:53<1:29:08, 16.46s/it]                                                   {'loss': 0.0461, 'learning_rate': 0.0001487015435494263, 'epoch': 1.08}
 36%|███▌      | 182/507 [49:53<1:29:08, 16.46s/it] 36%|███▌      | 183/507 [50:10<1:28:59, 16.48s/it]                                                   {'loss': 0.0167, 'learning_rate': 0.00014814172232984968, 'epoch': 1.08}
 36%|███▌      | 183/507 [50:10<1:28:59, 16.48s/it] 36%|███▋      | 184/507 [50:26<1:28:44, 16.48s/it]                                                   {'loss': 0.0187, 'learning_rate': 0.0001475799302451547, 'epoch': 1.09}
 36%|███▋      | 184/507 [50:26<1:28:44, 16.48s/it] 36%|███▋      | 185/507 [50:43<1:28:31, 16.49s/it]                                                   {'loss': 0.0295, 'learning_rate': 0.0001470161902944442, 'epoch': 1.09}
 36%|███▋      | 185/507 [50:43<1:28:31, 16.49s/it] 37%|███▋      | 186/507 [50:59<1:28:02, 16.46s/it]                                                   {'loss': 0.0334, 'learning_rate': 0.00014645052555656431, 'epoch': 1.1}
 37%|███▋      | 186/507 [50:59<1:28:02, 16.46s/it] 37%|███▋      | 187/507 [51:16<1:27:54, 16.48s/it]                                                   {'loss': 0.0142, 'learning_rate': 0.00014588295918915978, 'epoch': 1.1}
 37%|███▋      | 187/507 [51:16<1:27:54, 16.48s/it] 37%|███▋      | 188/507 [51:32<1:27:38, 16.48s/it]                                                   {'loss': 0.0248, 'learning_rate': 0.0001453135144277257, 'epoch': 1.11}
 37%|███▋      | 188/507 [51:32<1:27:38, 16.48s/it] 37%|███▋      | 189/507 [51:49<1:27:23, 16.49s/it]                                                   {'loss': 0.0437, 'learning_rate': 0.0001447422145846565, 'epoch': 1.12}
 37%|███▋      | 189/507 [51:49<1:27:23, 16.49s/it] 37%|███▋      | 190/507 [52:05<1:27:08, 16.50s/it]                                                   {'loss': 0.0264, 'learning_rate': 0.00014416908304829142, 'epoch': 1.12}
 37%|███▋      | 190/507 [52:05<1:27:08, 16.50s/it] 38%|███▊      | 191/507 [52:22<1:26:54, 16.50s/it]                                                   {'loss': 0.0169, 'learning_rate': 0.00014359414328195703, 'epoch': 1.13}
 38%|███▊      | 191/507 [52:22<1:26:54, 16.50s/it] 38%|███▊      | 192/507 [52:38<1:26:40, 16.51s/it]                                                   {'loss': 0.0148, 'learning_rate': 0.00014301741882300672, 'epoch': 1.13}
 38%|███▊      | 192/507 [52:38<1:26:40, 16.51s/it] 38%|███▊      | 193/507 [52:55<1:26:15, 16.48s/it]                                                   {'loss': 0.0404, 'learning_rate': 0.000142438933281857, 'epoch': 1.14}
 38%|███▊      | 193/507 [52:55<1:26:15, 16.48s/it] 38%|███▊      | 194/507 [53:11<1:26:01, 16.49s/it]                                                   {'loss': 0.0134, 'learning_rate': 0.00014185871034102116, 'epoch': 1.15}
 38%|███▊      | 194/507 [53:11<1:26:01, 16.49s/it] 38%|███▊      | 195/507 [53:28<1:25:31, 16.45s/it]                                                   {'loss': 0.0498, 'learning_rate': 0.00014127677375413942, 'epoch': 1.15}
 38%|███▊      | 195/507 [53:28<1:25:31, 16.45s/it] 39%|███▊      | 196/507 [53:44<1:25:22, 16.47s/it]                                                   {'loss': 0.0106, 'learning_rate': 0.00014069314734500675, 'epoch': 1.16}
 39%|███▊      | 196/507 [53:44<1:25:22, 16.47s/it] 39%|███▉      | 197/507 [54:01<1:25:08, 16.48s/it]                                                   {'loss': 0.0147, 'learning_rate': 0.00014010785500659736, 'epoch': 1.16}
 39%|███▉      | 197/507 [54:01<1:25:08, 16.48s/it] 39%|███▉      | 198/507 [54:17<1:24:53, 16.48s/it]                                                   {'loss': 0.0113, 'learning_rate': 0.0001395209207000867, 'epoch': 1.17}
 39%|███▉      | 198/507 [54:17<1:24:53, 16.48s/it] 39%|███▉      | 199/507 [54:34<1:24:41, 16.50s/it]                                                   {'loss': 0.03, 'learning_rate': 0.00013893236845387042, 'epoch': 1.18}
 39%|███▉      | 199/507 [54:34<1:24:41, 16.50s/it] 39%|███▉      | 200/507 [54:50<1:24:26, 16.50s/it]                                                   {'loss': 0.0122, 'learning_rate': 0.0001383422223625807, 'epoch': 1.18}
 39%|███▉      | 200/507 [54:50<1:24:26, 16.50s/it] 40%|███▉      | 201/507 [55:07<1:24:02, 16.48s/it]                                                   {'loss': 0.0201, 'learning_rate': 0.00013775050658609988, 'epoch': 1.19}
 40%|███▉      | 201/507 [55:07<1:24:02, 16.48s/it] 40%|███▉      | 202/507 [55:23<1:23:33, 16.44s/it]                                                   {'loss': 0.0486, 'learning_rate': 0.00013715724534857127, 'epoch': 1.19}
 40%|███▉      | 202/507 [55:23<1:23:33, 16.44s/it] 40%|████      | 203/507 [55:39<1:23:24, 16.46s/it]                                                   {'loss': 0.0621, 'learning_rate': 0.00013656246293740766, 'epoch': 1.2}
 40%|████      | 203/507 [55:39<1:23:24, 16.46s/it] 40%|████      | 204/507 [55:56<1:23:14, 16.48s/it]                                                   {'loss': 0.0329, 'learning_rate': 0.0001359661837022968, 'epoch': 1.21}
 40%|████      | 204/507 [55:56<1:23:14, 16.48s/it] 40%|████      | 205/507 [56:13<1:23:01, 16.49s/it]                                                   {'loss': 0.0176, 'learning_rate': 0.0001353684320542046, 'epoch': 1.21}
 40%|████      | 205/507 [56:13<1:23:01, 16.49s/it] 41%|████      | 206/507 [56:29<1:22:45, 16.50s/it]                                                   {'loss': 0.0149, 'learning_rate': 0.0001347692324643759, 'epoch': 1.22}
 41%|████      | 206/507 [56:29<1:22:45, 16.50s/it] 41%|████      | 207/507 [56:46<1:22:30, 16.50s/it]                                                   {'loss': 0.018, 'learning_rate': 0.00013416860946333255, 'epoch': 1.22}
 41%|████      | 207/507 [56:46<1:22:30, 16.50s/it] 41%|████      | 208/507 [57:02<1:22:01, 16.46s/it]                                                   {'loss': 0.0336, 'learning_rate': 0.00013356658763986917, 'epoch': 1.23}
 41%|████      | 208/507 [57:02<1:22:01, 16.46s/it] 41%|████      | 209/507 [57:18<1:21:47, 16.47s/it]                                                   {'loss': 0.0138, 'learning_rate': 0.00013296319164004644, 'epoch': 1.23}
 41%|████      | 209/507 [57:18<1:21:47, 16.47s/it] 41%|████▏     | 210/507 [57:35<1:21:36, 16.48s/it]                                                   {'loss': 0.0234, 'learning_rate': 0.0001323584461661823, 'epoch': 1.24}
 41%|████▏     | 210/507 [57:35<1:21:36, 16.48s/it] 42%|████▏     | 211/507 [57:51<1:21:15, 16.47s/it]                                                   {'loss': 0.0316, 'learning_rate': 0.00013175237597584045, 'epoch': 1.25}
 42%|████▏     | 211/507 [57:51<1:21:15, 16.47s/it] 42%|████▏     | 212/507 [58:08<1:21:02, 16.48s/it]                                                   {'loss': 0.0067, 'learning_rate': 0.00013114500588081698, 'epoch': 1.25}
 42%|████▏     | 212/507 [58:08<1:21:02, 16.48s/it] 42%|████▏     | 213/507 [58:24<1:20:48, 16.49s/it]                                                   {'loss': 0.0084, 'learning_rate': 0.00013053636074612457, 'epoch': 1.26}
 42%|████▏     | 213/507 [58:24<1:20:48, 16.49s/it] 42%|████▏     | 214/507 [58:41<1:20:23, 16.46s/it]                                                   {'loss': 0.0222, 'learning_rate': 0.00012992646548897442, 'epoch': 1.26}
 42%|████▏     | 214/507 [58:41<1:20:23, 16.46s/it] 42%|████▏     | 215/507 [58:57<1:20:01, 16.44s/it]                                                   {'loss': 0.0473, 'learning_rate': 0.0001293153450777564, 'epoch': 1.27}
 42%|████▏     | 215/507 [58:57<1:20:01, 16.44s/it] 43%|████▎     | 216/507 [59:14<1:19:52, 16.47s/it]                                                   {'loss': 0.0212, 'learning_rate': 0.00012870302453101657, 'epoch': 1.28}
 43%|████▎     | 216/507 [59:14<1:19:52, 16.47s/it] 43%|████▎     | 217/507 [59:30<1:19:32, 16.46s/it]                                                   {'loss': 0.036, 'learning_rate': 0.00012808952891643326, 'epoch': 1.28}
 43%|████▎     | 217/507 [59:30<1:19:32, 16.46s/it] 43%|████▎     | 218/507 [59:47<1:19:20, 16.47s/it]                                                   {'loss': 0.0252, 'learning_rate': 0.00012747488334979062, 'epoch': 1.29}
 43%|████▎     | 218/507 [59:47<1:19:20, 16.47s/it] 43%|████▎     | 219/507 [1:00:03<1:19:07, 16.48s/it]                                                     {'loss': 0.0245, 'learning_rate': 0.00012685911299395046, 'epoch': 1.29}
 43%|████▎     | 219/507 [1:00:03<1:19:07, 16.48s/it] 43%|████▎     | 220/507 [1:00:20<1:18:47, 16.47s/it]                                                     {'loss': 0.0191, 'learning_rate': 0.00012624224305782215, 'epoch': 1.3}
 43%|████▎     | 220/507 [1:00:20<1:18:47, 16.47s/it] 44%|████▎     | 221/507 [1:00:36<1:18:27, 16.46s/it]                                                     {'loss': 0.0274, 'learning_rate': 0.0001256242987953306, 'epoch': 1.31}
 44%|████▎     | 221/507 [1:00:36<1:18:27, 16.46s/it] 44%|████▍     | 222/507 [1:00:53<1:18:17, 16.48s/it]                                                     {'loss': 0.0223, 'learning_rate': 0.00012500530550438232, 'epoch': 1.31}
 44%|████▍     | 222/507 [1:00:53<1:18:17, 16.48s/it] 44%|████▍     | 223/507 [1:01:09<1:17:57, 16.47s/it]                                                     {'loss': 0.0545, 'learning_rate': 0.00012438528852582988, 'epoch': 1.32}
 44%|████▍     | 223/507 [1:01:09<1:17:57, 16.47s/it] 44%|████▍     | 224/507 [1:01:25<1:17:37, 16.46s/it]                                                     {'loss': 0.0354, 'learning_rate': 0.00012376427324243432, 'epoch': 1.32}
 44%|████▍     | 224/507 [1:01:25<1:17:37, 16.46s/it] 44%|████▍     | 225/507 [1:01:42<1:17:25, 16.47s/it]                                                     {'loss': 0.0181, 'learning_rate': 0.00012314228507782614, 'epoch': 1.33}
 44%|████▍     | 225/507 [1:01:42<1:17:25, 16.47s/it] 45%|████▍     | 226/507 [1:01:58<1:17:14, 16.49s/it]                                                     {'loss': 0.0479, 'learning_rate': 0.00012251934949546447, 'epoch': 1.34}
 45%|████▍     | 226/507 [1:01:58<1:17:14, 16.49s/it] 45%|████▍     | 227/507 [1:02:15<1:17:00, 16.50s/it]                                                     {'loss': 0.0101, 'learning_rate': 0.00012189549199759453, 'epoch': 1.34}
 45%|████▍     | 227/507 [1:02:15<1:17:00, 16.50s/it] 45%|████▍     | 228/507 [1:02:31<1:16:42, 16.50s/it]                                                     {'loss': 0.0539, 'learning_rate': 0.00012127073812420375, 'epoch': 1.35}
 45%|████▍     | 228/507 [1:02:31<1:16:42, 16.50s/it] 45%|████▌     | 229/507 [1:02:48<1:16:22, 16.48s/it]                                                     {'loss': 0.0241, 'learning_rate': 0.00012064511345197607, 'epoch': 1.35}
 45%|████▌     | 229/507 [1:02:48<1:16:22, 16.48s/it] 45%|████▌     | 230/507 [1:03:04<1:16:08, 16.49s/it]                                                     {'loss': 0.02, 'learning_rate': 0.00012001864359324489, 'epoch': 1.36}
 45%|████▌     | 230/507 [1:03:04<1:16:08, 16.49s/it] 46%|████▌     | 231/507 [1:03:21<1:15:54, 16.50s/it]                                                     {'loss': 0.0356, 'learning_rate': 0.00011939135419494456, 'epoch': 1.36}
 46%|████▌     | 231/507 [1:03:21<1:15:54, 16.50s/it] 46%|████▌     | 232/507 [1:03:37<1:15:35, 16.49s/it]                                                     {'loss': 0.0147, 'learning_rate': 0.00011876327093756047, 'epoch': 1.37}
 46%|████▌     | 232/507 [1:03:37<1:15:35, 16.49s/it] 46%|████▌     | 233/507 [1:03:54<1:15:21, 16.50s/it]                                                     {'loss': 0.0121, 'learning_rate': 0.00011813441953407754, 'epoch': 1.38}
 46%|████▌     | 233/507 [1:03:54<1:15:21, 16.50s/it] 46%|████▌     | 234/507 [1:04:10<1:15:04, 16.50s/it]                                                     {'loss': 0.0129, 'learning_rate': 0.0001175048257289278, 'epoch': 1.38}
 46%|████▌     | 234/507 [1:04:10<1:15:04, 16.50s/it] 46%|████▋     | 235/507 [1:04:27<1:14:34, 16.45s/it]                                                     {'loss': 0.0187, 'learning_rate': 0.00011687451529693624, 'epoch': 1.39}
 46%|████▋     | 235/507 [1:04:27<1:14:34, 16.45s/it] 47%|████▋     | 236/507 [1:04:43<1:14:15, 16.44s/it]                                                     {'loss': 0.0356, 'learning_rate': 0.00011624351404226572, 'epoch': 1.39}
 47%|████▋     | 236/507 [1:04:43<1:14:15, 16.44s/it] 47%|████▋     | 237/507 [1:05:00<1:14:05, 16.46s/it]                                                     {'loss': 0.015, 'learning_rate': 0.00011561184779736061, 'epoch': 1.4}
 47%|████▋     | 237/507 [1:05:00<1:14:05, 16.46s/it] 47%|████▋     | 238/507 [1:05:16<1:13:52, 16.48s/it]                                                     {'loss': 0.0277, 'learning_rate': 0.00011497954242188913, 'epoch': 1.41}
 47%|████▋     | 238/507 [1:05:16<1:13:52, 16.48s/it] 47%|████▋     | 239/507 [1:05:33<1:13:34, 16.47s/it]                                                     {'loss': 0.0321, 'learning_rate': 0.00011434662380168486, 'epoch': 1.41}
 47%|████▋     | 239/507 [1:05:33<1:13:34, 16.47s/it] 47%|████▋     | 240/507 [1:05:49<1:13:21, 16.48s/it]                                                     {'loss': 0.0174, 'learning_rate': 0.00011371311784768673, 'epoch': 1.42}
 47%|████▋     | 240/507 [1:05:49<1:13:21, 16.48s/it] 48%|████▊     | 241/507 [1:06:06<1:13:03, 16.48s/it]                                                     {'loss': 0.0125, 'learning_rate': 0.00011307905049487855, 'epoch': 1.42}
 48%|████▊     | 241/507 [1:06:06<1:13:03, 16.48s/it][E ProcessGroupNCCL.cpp:916] [Rank 3] NCCL watchdog thread terminated with exception: CUDA error: an illegal instruction was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fe46e992617 in /data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7fe46e94d98d in /data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7fe47a59e128 in /data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x80 (0x7fe3f7c75250 in /data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x58 (0x7fe3f7c79078 in /data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x250 (0x7fe3f7c8f910 in /data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x78 (0x7fe3f7c8fc18 in /data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xdbbf4 (0x7fe43ecc7bf4 in /data3/jisu/miniconda3/envs/mfm-new/bin/../lib/libstdc++.so.6)
frame #8: <unknown function> + 0x94ac3 (0x7fe47b3d2ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: <unknown function> + 0x126850 (0x7fe47b464850 in /lib/x86_64-linux-gnu/libc.so.6)

[2026-01-09 18:12:36,565] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 656002
[2026-01-09 18:12:36,892] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 656003
[2026-01-09 18:12:37,257] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 656004
[2026-01-09 18:12:37,581] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 656005
[2026-01-09 18:12:37,581] [ERROR] [launch.py:321:sigkill_handler] ['/data3/jisu/miniconda3/envs/mfm-new/bin/python3.11', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/data3/jisu/LLaVA/visa_llava_instruct.json', '--image_folder', '/data3/jisu/MFM/datasets/ViSA', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bits', '4', '--bf16', 'False', '--fp16', 'True', '--output_dir', './checkpoints/llava-v1.5-7b-mfm-lora', '--num_train_epochs', '3', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.0', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none'] exits with return code = -6
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 18:46:33,558] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 18:46:45,180] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=3,4,5,6: setting --include=localhost:3,4,5,6
[2026-01-09 18:46:45,240] [INFO] [runner.py:571:main] cmd = /data3/jisu/miniconda3/envs/mfm-new/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMywgNCwgNSwgNl19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero2.json --model_name_or_path liuhaotian/llava-v1.5-7b --version v1 --data_path /data3/jisu/LLaVA/visa_llava_instruct.json --image_folder /data3/jisu/MFM/datasets/ViSA --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bits 4 --bf16 False --fp16 True --output_dir ./checkpoints/llava-v1.5-7b-mfm-lora --num_train_epochs 3 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 False --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to none
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 18:46:47,271] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 18:46:48,326] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [3, 4, 5, 6]}
[2026-01-09 18:46:48,326] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2026-01-09 18:46:48,326] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2026-01-09 18:46:48,326] [INFO] [launch.py:163:main] dist_world_size=4
[2026-01-09 18:46:48,326] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=3,4,5,6
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 18:46:55,325] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 18:46:55,332] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 18:46:55,366] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 18:46:55,382] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 18:46:56,143] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 18:46:56,298] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 18:46:56,298] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2026-01-09 18:46:56,328] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 18:46:56,329] [INFO] [comm.py:637:init_distributed] cdb=None
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [02:02<02:02, 122.65s/it]Loading checkpoint shards:  50%|█████     | 1/2 [02:03<02:03, 123.09s/it]Loading checkpoint shards:  50%|█████     | 1/2 [02:03<02:03, 123.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [02:03<02:03, 123.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [02:46<00:00, 76.28s/it] Loading checkpoint shards: 100%|██████████| 2/2 [02:46<00:00, 83.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [02:46<00:00, 76.32s/it] Loading checkpoint shards: 100%|██████████| 2/2 [02:46<00:00, 83.38s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [02:46<00:00, 76.21s/it] Loading checkpoint shards: 100%|██████████| 2/2 [02:46<00:00, 83.24s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [02:46<00:00, 76.29s/it] Loading checkpoint shards: 100%|██████████| 2/2 [02:46<00:00, 83.35s/it]
Adding LoRA adapters...
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Formatting inputs...Skip in lazy mode
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
  0%|          | 0/507 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/507 [00:16<2:21:10, 16.74s/it]                                                 {'loss': 2.3151, 'learning_rate': 1.25e-05, 'epoch': 0.01}
  0%|          | 1/507 [00:16<2:21:10, 16.74s/it]  0%|          | 2/507 [00:32<2:15:02, 16.04s/it]                                                 {'loss': 2.2569, 'learning_rate': 2.5e-05, 'epoch': 0.01}
  0%|          | 2/507 [00:32<2:15:02, 16.04s/it]  1%|          | 3/507 [00:47<2:12:44, 15.80s/it]                                                 {'loss': 1.547, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.02}
  1%|          | 3/507 [00:47<2:12:44, 15.80s/it]  1%|          | 4/507 [01:03<2:12:02, 15.75s/it]                                                 {'loss': 0.8359, 'learning_rate': 5e-05, 'epoch': 0.02}
  1%|          | 4/507 [01:03<2:12:02, 15.75s/it]  1%|          | 5/507 [01:19<2:11:49, 15.76s/it]                                                 {'loss': 0.7591, 'learning_rate': 6.25e-05, 'epoch': 0.03}
  1%|          | 5/507 [01:19<2:11:49, 15.76s/it]  1%|          | 6/507 [01:35<2:11:45, 15.78s/it]                                                 {'loss': 0.7197, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.04}
  1%|          | 6/507 [01:35<2:11:45, 15.78s/it]  1%|▏         | 7/507 [01:50<2:11:49, 15.82s/it]                                                 {'loss': 0.1855, 'learning_rate': 8.75e-05, 'epoch': 0.04}
  1%|▏         | 7/507 [01:50<2:11:49, 15.82s/it]  2%|▏         | 8/507 [02:06<2:11:57, 15.87s/it]                                                 {'loss': 0.1955, 'learning_rate': 0.0001, 'epoch': 0.05}
  2%|▏         | 8/507 [02:06<2:11:57, 15.87s/it]  2%|▏         | 9/507 [02:23<2:12:27, 15.96s/it]                                                 {'loss': 0.0971, 'learning_rate': 0.00011250000000000001, 'epoch': 0.05}
  2%|▏         | 9/507 [02:23<2:12:27, 15.96s/it]  2%|▏         | 10/507 [02:39<2:12:47, 16.03s/it]                                                  {'loss': 0.1217, 'learning_rate': 0.000125, 'epoch': 0.06}
  2%|▏         | 10/507 [02:39<2:12:47, 16.03s/it]  2%|▏         | 11/507 [02:55<2:13:08, 16.10s/it]                                                  {'loss': 0.2048, 'learning_rate': 0.0001375, 'epoch': 0.06}
  2%|▏         | 11/507 [02:55<2:13:08, 16.10s/it]  2%|▏         | 12/507 [03:11<2:13:07, 16.14s/it]                                                  {'loss': 0.081, 'learning_rate': 0.00015000000000000001, 'epoch': 0.07}
  2%|▏         | 12/507 [03:11<2:13:07, 16.14s/it]  3%|▎         | 13/507 [03:28<2:13:05, 16.16s/it]                                                  {'loss': 0.2393, 'learning_rate': 0.00016250000000000002, 'epoch': 0.08}
  3%|▎         | 13/507 [03:28<2:13:05, 16.16s/it]  3%|▎         | 14/507 [03:44<2:13:15, 16.22s/it]                                                  {'loss': 0.1238, 'learning_rate': 0.000175, 'epoch': 0.08}
  3%|▎         | 14/507 [03:44<2:13:15, 16.22s/it]  3%|▎         | 15/507 [04:00<2:13:25, 16.27s/it]                                                  {'loss': 0.2862, 'learning_rate': 0.0001875, 'epoch': 0.09}
  3%|▎         | 15/507 [04:00<2:13:25, 16.27s/it]  3%|▎         | 16/507 [04:17<2:13:26, 16.31s/it]                                                  {'loss': 0.2163, 'learning_rate': 0.0002, 'epoch': 0.09}
  3%|▎         | 16/507 [04:17<2:13:26, 16.31s/it]  3%|▎         | 17/507 [04:33<2:13:30, 16.35s/it]                                                  {'loss': 0.1557, 'learning_rate': 0.00019999795305919378, 'epoch': 0.1}
  3%|▎         | 17/507 [04:33<2:13:30, 16.35s/it]  4%|▎         | 18/507 [04:49<2:13:21, 16.36s/it]                                                  {'loss': 0.1903, 'learning_rate': 0.0001999918123205744, 'epoch': 0.11}
  4%|▎         | 18/507 [04:49<2:13:21, 16.36s/it]  4%|▎         | 19/507 [05:06<2:13:24, 16.40s/it]                                                  {'loss': 0.1093, 'learning_rate': 0.00019998157803553638, 'epoch': 0.11}
  4%|▎         | 19/507 [05:06<2:13:24, 16.40s/it]  4%|▍         | 20/507 [05:22<2:13:22, 16.43s/it]                                                  {'loss': 0.0866, 'learning_rate': 0.00019996725062305934, 'epoch': 0.12}
  4%|▍         | 20/507 [05:22<2:13:22, 16.43s/it]  4%|▍         | 21/507 [05:39<2:13:07, 16.44s/it]                                                  {'loss': 0.1588, 'learning_rate': 0.00019994883066969053, 'epoch': 0.12}
  4%|▍         | 21/507 [05:39<2:13:07, 16.44s/it]  4%|▍         | 22/507 [05:55<2:12:52, 16.44s/it]                                                  {'loss': 0.1835, 'learning_rate': 0.00019992631892952107, 'epoch': 0.13}
  4%|▍         | 22/507 [05:55<2:12:52, 16.44s/it]  5%|▍         | 23/507 [06:12<2:12:46, 16.46s/it]                                                  {'loss': 0.0664, 'learning_rate': 0.0001998997163241549, 'epoch': 0.14}
  5%|▍         | 23/507 [06:12<2:12:46, 16.46s/it]  5%|▍         | 24/507 [06:28<2:12:35, 16.47s/it]                                                  {'loss': 0.1655, 'learning_rate': 0.00019986902394267118, 'epoch': 0.14}
  5%|▍         | 24/507 [06:28<2:12:35, 16.47s/it]  5%|▍         | 25/507 [06:45<2:12:27, 16.49s/it]                                                  {'loss': 0.1453, 'learning_rate': 0.00019983424304157973, 'epoch': 0.15}
  5%|▍         | 25/507 [06:45<2:12:27, 16.49s/it]  5%|▌         | 26/507 [07:01<2:12:11, 16.49s/it]                                                  {'loss': 0.0528, 'learning_rate': 0.00019979537504476944, 'epoch': 0.15}
  5%|▌         | 26/507 [07:01<2:12:11, 16.49s/it]  5%|▌         | 27/507 [07:18<2:11:55, 16.49s/it]                                                  {'loss': 0.0742, 'learning_rate': 0.00019975242154345008, 'epoch': 0.16}
  5%|▌         | 27/507 [07:18<2:11:55, 16.49s/it]  6%|▌         | 28/507 [07:34<2:11:42, 16.50s/it]                                                  {'loss': 0.0939, 'learning_rate': 0.00019970538429608714, 'epoch': 0.17}
  6%|▌         | 28/507 [07:34<2:11:42, 16.50s/it]  6%|▌         | 29/507 [07:51<2:11:25, 16.50s/it]                                                  {'loss': 0.0911, 'learning_rate': 0.00019965426522832984, 'epoch': 0.17}
  6%|▌         | 29/507 [07:51<2:11:25, 16.50s/it]  6%|▌         | 30/507 [08:07<2:11:11, 16.50s/it]                                                  {'loss': 0.0599, 'learning_rate': 0.0001995990664329323, 'epoch': 0.18}
  6%|▌         | 30/507 [08:07<2:11:11, 16.50s/it]  6%|▌         | 31/507 [08:24<2:10:40, 16.47s/it]                                                  {'loss': 0.0627, 'learning_rate': 0.00019953979016966788, 'epoch': 0.18}
  6%|▌         | 31/507 [08:24<2:10:40, 16.47s/it]  6%|▋         | 32/507 [08:40<2:10:31, 16.49s/it]                                                  {'loss': 0.0678, 'learning_rate': 0.0001994764388652366, 'epoch': 0.19}
  6%|▋         | 32/507 [08:40<2:10:31, 16.49s/it]  7%|▋         | 33/507 [08:57<2:10:17, 16.49s/it]                                                  {'loss': 0.0979, 'learning_rate': 0.00019940901511316582, 'epoch': 0.19}
  7%|▋         | 33/507 [08:57<2:10:17, 16.49s/it]  7%|▋         | 34/507 [09:13<2:10:04, 16.50s/it]                                                  {'loss': 0.1245, 'learning_rate': 0.0001993375216737042, 'epoch': 0.2}
  7%|▋         | 34/507 [09:13<2:10:04, 16.50s/it]  7%|▋         | 35/507 [09:30<2:09:30, 16.46s/it]                                                  {'loss': 0.0692, 'learning_rate': 0.00019926196147370849, 'epoch': 0.21}
  7%|▋         | 35/507 [09:30<2:09:30, 16.46s/it]  7%|▋         | 36/507 [09:46<2:09:25, 16.49s/it]                                                  {'loss': 0.0403, 'learning_rate': 0.0001991823376065238, 'epoch': 0.21}
  7%|▋         | 36/507 [09:46<2:09:25, 16.49s/it]  7%|▋         | 37/507 [10:03<2:09:12, 16.50s/it]                                                  {'loss': 0.0539, 'learning_rate': 0.00019909865333185702, 'epoch': 0.22}
  7%|▋         | 37/507 [10:03<2:09:12, 16.50s/it]  7%|▋         | 38/507 [10:19<2:08:53, 16.49s/it]                                                  {'loss': 0.0709, 'learning_rate': 0.00019901091207564324, 'epoch': 0.22}
  7%|▋         | 38/507 [10:19<2:08:53, 16.49s/it]  8%|▊         | 39/507 [10:36<2:08:40, 16.50s/it]                                                  {'loss': 0.0308, 'learning_rate': 0.00019891911742990565, 'epoch': 0.23}
  8%|▊         | 39/507 [10:36<2:08:40, 16.50s/it]  8%|▊         | 40/507 [10:52<2:08:18, 16.48s/it]                                                  {'loss': 0.0559, 'learning_rate': 0.00019882327315260838, 'epoch': 0.24}
  8%|▊         | 40/507 [10:52<2:08:18, 16.48s/it]  8%|▊         | 41/507 [11:09<2:08:07, 16.50s/it]                                                  {'loss': 0.0839, 'learning_rate': 0.00019872338316750265, 'epoch': 0.24}
  8%|▊         | 41/507 [11:09<2:08:07, 16.50s/it]  8%|▊         | 42/507 [11:25<2:07:46, 16.49s/it]                                                  {'loss': 0.0732, 'learning_rate': 0.0001986194515639662, 'epoch': 0.25}
  8%|▊         | 42/507 [11:25<2:07:46, 16.49s/it]  8%|▊         | 43/507 [11:42<2:07:23, 16.47s/it]                                                  {'loss': 0.0517, 'learning_rate': 0.00019851148259683585, 'epoch': 0.25}
  8%|▊         | 43/507 [11:42<2:07:23, 16.47s/it]  9%|▊         | 44/507 [11:58<2:07:03, 16.47s/it]                                                  {'loss': 0.0749, 'learning_rate': 0.0001983994806862333, 'epoch': 0.26}
  9%|▊         | 44/507 [11:58<2:07:03, 16.47s/it]  9%|▉         | 45/507 [12:15<2:06:46, 16.46s/it]                                                  {'loss': 0.0772, 'learning_rate': 0.00019828345041738413, 'epoch': 0.27}
  9%|▉         | 45/507 [12:15<2:06:46, 16.46s/it]  9%|▉         | 46/507 [12:31<2:06:30, 16.47s/it]                                                  {'loss': 0.0731, 'learning_rate': 0.00019816339654043022, 'epoch': 0.27}
  9%|▉         | 46/507 [12:31<2:06:30, 16.47s/it]  9%|▉         | 47/507 [12:48<2:06:22, 16.48s/it]                                                  {'loss': 0.0618, 'learning_rate': 0.0001980393239702351, 'epoch': 0.28}
  9%|▉         | 47/507 [12:48<2:06:22, 16.48s/it]  9%|▉         | 48/507 [13:04<2:06:03, 16.48s/it]                                                  {'loss': 0.0814, 'learning_rate': 0.00019791123778618305, 'epoch': 0.28}
  9%|▉         | 48/507 [13:04<2:06:03, 16.48s/it] 10%|▉         | 49/507 [13:20<2:05:44, 16.47s/it]                                                  {'loss': 0.0597, 'learning_rate': 0.00019777914323197064, 'epoch': 0.29}
 10%|▉         | 49/507 [13:20<2:05:44, 16.47s/it] 10%|▉         | 50/507 [13:37<2:05:19, 16.45s/it]                                                  {'loss': 0.0347, 'learning_rate': 0.00019764304571539266, 'epoch': 0.3}
 10%|▉         | 50/507 [13:37<2:05:19, 16.45s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 10%|█         | 51/507 [14:01<2:21:30, 18.62s/it]                                                  {'loss': 0.0328, 'learning_rate': 0.00019750295080812023, 'epoch': 0.3}
 10%|█         | 51/507 [14:01<2:21:30, 18.62s/it] 10%|█         | 52/507 [14:17<2:16:00, 17.94s/it]                                                  {'loss': 0.0307, 'learning_rate': 0.00019735886424547306, 'epoch': 0.31}
 10%|█         | 52/507 [14:17<2:16:00, 17.94s/it] 10%|█         | 53/507 [14:33<2:12:29, 17.51s/it]                                                  {'loss': 0.0823, 'learning_rate': 0.0001972107919261844, 'epoch': 0.31}
 10%|█         | 53/507 [14:33<2:12:29, 17.51s/it] 11%|█         | 54/507 [14:50<2:09:56, 17.21s/it]                                                  {'loss': 0.0148, 'learning_rate': 0.00019705873991215974, 'epoch': 0.32}
 11%|█         | 54/507 [14:50<2:09:56, 17.21s/it] 11%|█         | 55/507 [15:06<2:08:03, 17.00s/it]                                                  {'loss': 0.035, 'learning_rate': 0.00019690271442822848, 'epoch': 0.32}
 11%|█         | 55/507 [15:06<2:08:03, 17.00s/it] 11%|█         | 56/507 [15:23<2:06:40, 16.85s/it]                                                  {'loss': 0.0373, 'learning_rate': 0.0001967427218618893, 'epoch': 0.33}
 11%|█         | 56/507 [15:23<2:06:40, 16.85s/it] 11%|█         | 57/507 [15:39<2:05:39, 16.75s/it]                                                  {'loss': 0.0574, 'learning_rate': 0.00019657876876304835, 'epoch': 0.34}
 11%|█         | 57/507 [15:39<2:05:39, 16.75s/it] 11%|█▏        | 58/507 [15:56<2:04:39, 16.66s/it]                                                  {'loss': 0.0349, 'learning_rate': 0.00019641086184375145, 'epoch': 0.34}
 11%|█▏        | 58/507 [15:56<2:04:39, 16.66s/it] 12%|█▏        | 59/507 [16:12<2:04:00, 16.61s/it]                                                  {'loss': 0.0366, 'learning_rate': 0.00019623900797790912, 'epoch': 0.35}
 12%|█▏        | 59/507 [16:12<2:04:00, 16.61s/it] 12%|█▏        | 60/507 [16:29<2:03:22, 16.56s/it]                                                  {'loss': 0.0496, 'learning_rate': 0.00019606321420101512, 'epoch': 0.35}
 12%|█▏        | 60/507 [16:29<2:03:22, 16.56s/it] 12%|█▏        | 61/507 [16:45<2:03:02, 16.55s/it]                                                  {'loss': 0.0486, 'learning_rate': 0.0001958834877098586, 'epoch': 0.36}
 12%|█▏        | 61/507 [16:45<2:03:02, 16.55s/it] 12%|█▏        | 62/507 [17:02<2:02:33, 16.52s/it]                                                  {'loss': 0.0708, 'learning_rate': 0.0001956998358622293, 'epoch': 0.37}
 12%|█▏        | 62/507 [17:02<2:02:33, 16.52s/it] 12%|█▏        | 63/507 [17:18<2:02:14, 16.52s/it]                                                  {'loss': 0.0485, 'learning_rate': 0.00019551226617661648, 'epoch': 0.37}
 12%|█▏        | 63/507 [17:18<2:02:14, 16.52s/it] 13%|█▎        | 64/507 [17:35<2:01:58, 16.52s/it]                                                  {'loss': 0.0263, 'learning_rate': 0.00019532078633190095, 'epoch': 0.38}
 13%|█▎        | 64/507 [17:35<2:01:58, 16.52s/it] 13%|█▎        | 65/507 [17:51<2:01:44, 16.53s/it]                                                  {'loss': 0.0504, 'learning_rate': 0.00019512540416704094, 'epoch': 0.38}
 13%|█▎        | 65/507 [17:51<2:01:44, 16.53s/it] 13%|█▎        | 66/507 [18:08<2:01:27, 16.53s/it]                                                  {'loss': 0.0538, 'learning_rate': 0.00019492612768075092, 'epoch': 0.39}
 13%|█▎        | 66/507 [18:08<2:01:27, 16.53s/it] 13%|█▎        | 67/507 [18:24<2:00:56, 16.49s/it]                                                  {'loss': 0.0249, 'learning_rate': 0.00019472296503117437, 'epoch': 0.4}
 13%|█▎        | 67/507 [18:24<2:00:56, 16.49s/it] 13%|█▎        | 68/507 [18:41<2:00:33, 16.48s/it]                                                  {'loss': 0.0334, 'learning_rate': 0.00019451592453554955, 'epoch': 0.4}
 13%|█▎        | 68/507 [18:41<2:00:33, 16.48s/it] 14%|█▎        | 69/507 [18:57<2:00:22, 16.49s/it]                                                  {'loss': 0.0386, 'learning_rate': 0.00019430501466986933, 'epoch': 0.41}
 14%|█▎        | 69/507 [18:57<2:00:22, 16.49s/it] 14%|█▍        | 70/507 [19:14<2:00:10, 16.50s/it]                                                  {'loss': 0.0453, 'learning_rate': 0.0001940902440685339, 'epoch': 0.41}
 14%|█▍        | 70/507 [19:14<2:00:10, 16.50s/it] 14%|█▍        | 71/507 [19:30<1:59:56, 16.51s/it]                                                  {'loss': 0.0402, 'learning_rate': 0.0001938716215239974, 'epoch': 0.42}
 14%|█▍        | 71/507 [19:30<1:59:56, 16.51s/it] 14%|█▍        | 72/507 [19:47<1:59:42, 16.51s/it]                                                  {'loss': 0.0384, 'learning_rate': 0.00019364915598640793, 'epoch': 0.43}
 14%|█▍        | 72/507 [19:47<1:59:42, 16.51s/it] 14%|█▍        | 73/507 [20:03<1:59:28, 16.52s/it]                                                  {'loss': 0.0972, 'learning_rate': 0.00019342285656324135, 'epoch': 0.43}
 14%|█▍        | 73/507 [20:03<1:59:28, 16.52s/it] 15%|█▍        | 74/507 [20:20<1:59:13, 16.52s/it]                                                  {'loss': 0.0652, 'learning_rate': 0.00019319273251892805, 'epoch': 0.44}
 15%|█▍        | 74/507 [20:20<1:59:13, 16.52s/it] 15%|█▍        | 75/507 [20:36<1:58:31, 16.46s/it]                                                  {'loss': 0.0495, 'learning_rate': 0.000192958793274474, 'epoch': 0.44}
 15%|█▍        | 75/507 [20:36<1:58:31, 16.46s/it] 15%|█▍        | 76/507 [20:53<1:58:20, 16.47s/it]                                                  {'loss': 0.047, 'learning_rate': 0.00019272104840707487, 'epoch': 0.45}
 15%|█▍        | 76/507 [20:53<1:58:20, 16.47s/it] 15%|█▌        | 77/507 [21:09<1:57:58, 16.46s/it]                                                  {'loss': 0.0424, 'learning_rate': 0.0001924795076497241, 'epoch': 0.45}
 15%|█▌        | 77/507 [21:09<1:57:58, 16.46s/it] 15%|█▌        | 78/507 [21:26<1:57:49, 16.48s/it]                                                  {'loss': 0.0729, 'learning_rate': 0.0001922341808908144, 'epoch': 0.46}
 15%|█▌        | 78/507 [21:26<1:57:49, 16.48s/it] 16%|█▌        | 79/507 [21:42<1:57:38, 16.49s/it]                                                  {'loss': 0.0634, 'learning_rate': 0.00019198507817373272, 'epoch': 0.47}
 16%|█▌        | 79/507 [21:42<1:57:38, 16.49s/it] 16%|█▌        | 80/507 [21:59<1:57:24, 16.50s/it]                                                  {'loss': 0.0528, 'learning_rate': 0.00019173220969644948, 'epoch': 0.47}
 16%|█▌        | 80/507 [21:59<1:57:24, 16.50s/it] 16%|█▌        | 81/507 [22:15<1:57:10, 16.50s/it]                                                  {'loss': 0.0321, 'learning_rate': 0.00019147558581110078, 'epoch': 0.48}
 16%|█▌        | 81/507 [22:15<1:57:10, 16.50s/it] 16%|█▌        | 82/507 [22:32<1:56:39, 16.47s/it]                                                  {'loss': 0.0439, 'learning_rate': 0.0001912152170235646, 'epoch': 0.48}
 16%|█▌        | 82/507 [22:32<1:56:39, 16.47s/it] 16%|█▋        | 83/507 [22:48<1:56:19, 16.46s/it]                                                  {'loss': 0.0327, 'learning_rate': 0.0001909511139930309, 'epoch': 0.49}
 16%|█▋        | 83/507 [22:48<1:56:19, 16.46s/it] 17%|█▋        | 84/507 [23:05<1:56:09, 16.48s/it]                                                  {'loss': 0.0189, 'learning_rate': 0.00019068328753156513, 'epoch': 0.5}
 17%|█▋        | 84/507 [23:05<1:56:09, 16.48s/it] 17%|█▋        | 85/507 [23:21<1:55:59, 16.49s/it]                                                  {'loss': 0.0381, 'learning_rate': 0.0001904117486036655, 'epoch': 0.5}
 17%|█▋        | 85/507 [23:21<1:55:59, 16.49s/it] 17%|█▋        | 86/507 [23:38<1:55:47, 16.50s/it]                                                  {'loss': 0.0634, 'learning_rate': 0.00019013650832581423, 'epoch': 0.51}
 17%|█▋        | 86/507 [23:38<1:55:47, 16.50s/it] 17%|█▋        | 87/507 [23:54<1:55:12, 16.46s/it]                                                  {'loss': 0.0397, 'learning_rate': 0.00018985757796602252, 'epoch': 0.51}
 17%|█▋        | 87/507 [23:54<1:55:12, 16.46s/it] 17%|█▋        | 88/507 [24:11<1:55:02, 16.47s/it]                                                  {'loss': 0.0493, 'learning_rate': 0.00018957496894336898, 'epoch': 0.52}
 17%|█▋        | 88/507 [24:11<1:55:02, 16.47s/it] 18%|█▊        | 89/507 [24:27<1:54:51, 16.49s/it]                                                  {'loss': 0.0245, 'learning_rate': 0.0001892886928275325, 'epoch': 0.53}
 18%|█▊        | 89/507 [24:27<1:54:51, 16.49s/it] 18%|█▊        | 90/507 [24:44<1:54:38, 16.49s/it]                                                  {'loss': 0.0461, 'learning_rate': 0.00018899876133831835, 'epoch': 0.53}
 18%|█▊        | 90/507 [24:44<1:54:38, 16.49s/it] 18%|█▊        | 91/507 [25:00<1:54:23, 16.50s/it]                                                  {'loss': 0.0381, 'learning_rate': 0.0001887051863451784, 'epoch': 0.54}
 18%|█▊        | 91/507 [25:00<1:54:23, 16.50s/it] 18%|█▊        | 92/507 [25:17<1:54:08, 16.50s/it]                                                  {'loss': 0.0277, 'learning_rate': 0.00018840797986672538, 'epoch': 0.54}
 18%|█▊        | 92/507 [25:17<1:54:08, 16.50s/it] 18%|█▊        | 93/507 [25:33<1:53:45, 16.49s/it]                                                  {'loss': 0.0414, 'learning_rate': 0.0001881071540702406, 'epoch': 0.55}
 18%|█▊        | 93/507 [25:33<1:53:45, 16.49s/it] 19%|█▊        | 94/507 [25:50<1:53:29, 16.49s/it]                                                  {'loss': 0.031, 'learning_rate': 0.00018780272127117607, 'epoch': 0.56}
 19%|█▊        | 94/507 [25:50<1:53:29, 16.49s/it] 19%|█▊        | 95/507 [26:06<1:53:15, 16.50s/it]                                                  {'loss': 0.0345, 'learning_rate': 0.00018749469393265016, 'epoch': 0.56}
 19%|█▊        | 95/507 [26:06<1:53:15, 16.50s/it] 19%|█▉        | 96/507 [26:22<1:52:49, 16.47s/it]                                                  {'loss': 0.0534, 'learning_rate': 0.00018718308466493744, 'epoch': 0.57}
 19%|█▉        | 96/507 [26:22<1:52:49, 16.47s/it] 19%|█▉        | 97/507 [26:39<1:52:40, 16.49s/it]                                                  {'loss': 0.0339, 'learning_rate': 0.0001868679062249524, 'epoch': 0.57}
 19%|█▉        | 97/507 [26:39<1:52:40, 16.49s/it] 19%|█▉        | 98/507 [26:55<1:52:24, 16.49s/it]                                                  {'loss': 0.0381, 'learning_rate': 0.00018654917151572729, 'epoch': 0.58}
 19%|█▉        | 98/507 [26:55<1:52:24, 16.49s/it] 20%|█▉        | 99/507 [27:12<1:52:10, 16.50s/it]                                                  {'loss': 0.045, 'learning_rate': 0.00018622689358588373, 'epoch': 0.58}
 20%|█▉        | 99/507 [27:12<1:52:10, 16.50s/it] 20%|█▉        | 100/507 [27:29<1:51:56, 16.50s/it]                                                   {'loss': 0.0552, 'learning_rate': 0.00018590108562909863, 'epoch': 0.59}
 20%|█▉        | 100/507 [27:29<1:51:56, 16.50s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 20%|█▉        | 101/507 [27:52<2:05:16, 18.51s/it]                                                   {'loss': 0.0353, 'learning_rate': 0.00018557176098356405, 'epoch': 0.6}
 20%|█▉        | 101/507 [27:52<2:05:16, 18.51s/it] 20%|██        | 102/507 [28:08<2:00:40, 17.88s/it]                                                   {'loss': 0.0384, 'learning_rate': 0.0001852389331314411, 'epoch': 0.6}
 20%|██        | 102/507 [28:08<2:00:40, 17.88s/it] 20%|██        | 103/507 [28:24<1:57:19, 17.42s/it]                                                   {'loss': 0.0387, 'learning_rate': 0.00018490261569830798, 'epoch': 0.61}
 20%|██        | 103/507 [28:24<1:57:19, 17.42s/it] 21%|██        | 104/507 [28:41<1:55:02, 17.13s/it]                                                   {'loss': 0.0248, 'learning_rate': 0.0001845628224526023, 'epoch': 0.61}
 21%|██        | 104/507 [28:41<1:55:02, 17.13s/it] 21%|██        | 105/507 [28:57<1:53:34, 16.95s/it]                                                   {'loss': 0.073, 'learning_rate': 0.0001842195673050572, 'epoch': 0.62}
 21%|██        | 105/507 [28:57<1:53:34, 16.95s/it] 21%|██        | 106/507 [29:14<1:52:23, 16.82s/it]                                                   {'loss': 0.0343, 'learning_rate': 0.00018387286430813208, 'epoch': 0.63}
 21%|██        | 106/507 [29:14<1:52:23, 16.82s/it] 21%|██        | 107/507 [29:30<1:51:22, 16.71s/it]                                                   {'loss': 0.035, 'learning_rate': 0.00018352272765543722, 'epoch': 0.63}
 21%|██        | 107/507 [29:30<1:51:22, 16.71s/it] 21%|██▏       | 108/507 [29:47<1:50:35, 16.63s/it]                                                   {'loss': 0.0261, 'learning_rate': 0.0001831691716811526, 'epoch': 0.64}
 21%|██▏       | 108/507 [29:47<1:50:35, 16.63s/it] 21%|██▏       | 109/507 [30:03<1:49:55, 16.57s/it]                                                   {'loss': 0.0228, 'learning_rate': 0.00018281221085944126, 'epoch': 0.64}
 21%|██▏       | 109/507 [30:03<1:49:55, 16.57s/it] 22%|██▏       | 110/507 [30:20<1:49:32, 16.56s/it]                                                   {'loss': 0.0282, 'learning_rate': 0.0001824518598038567, 'epoch': 0.65}
 22%|██▏       | 110/507 [30:20<1:49:32, 16.56s/it] 22%|██▏       | 111/507 [30:36<1:49:14, 16.55s/it]                                                   {'loss': 0.0281, 'learning_rate': 0.00018208813326674444, 'epoch': 0.66}
 22%|██▏       | 111/507 [30:36<1:49:14, 16.55s/it] 22%|██▏       | 112/507 [30:53<1:48:51, 16.53s/it]                                                   {'loss': 0.0341, 'learning_rate': 0.00018172104613863835, 'epoch': 0.66}
 22%|██▏       | 112/507 [30:53<1:48:51, 16.53s/it] 22%|██▏       | 113/507 [31:09<1:48:33, 16.53s/it]                                                   {'loss': 0.0153, 'learning_rate': 0.00018135061344765088, 'epoch': 0.67}
 22%|██▏       | 113/507 [31:09<1:48:33, 16.53s/it] 22%|██▏       | 114/507 [31:26<1:47:56, 16.48s/it]                                                   {'loss': 0.0456, 'learning_rate': 0.0001809768503588578, 'epoch': 0.67}
 22%|██▏       | 114/507 [31:26<1:47:56, 16.48s/it] 23%|██▎       | 115/507 [31:42<1:47:35, 16.47s/it]                                                   {'loss': 0.0254, 'learning_rate': 0.00018059977217367755, 'epoch': 0.68}
 23%|██▎       | 115/507 [31:42<1:47:35, 16.47s/it] 23%|██▎       | 116/507 [31:59<1:47:24, 16.48s/it]                                                   {'loss': 0.0188, 'learning_rate': 0.00018021939432924454, 'epoch': 0.69}
 23%|██▎       | 116/507 [31:59<1:47:24, 16.48s/it] 23%|██▎       | 117/507 [32:15<1:47:04, 16.47s/it]                                                   {'loss': 0.0462, 'learning_rate': 0.00017983573239777748, 'epoch': 0.69}
 23%|██▎       | 117/507 [32:15<1:47:04, 16.47s/it] 23%|██▎       | 118/507 [32:32<1:46:53, 16.49s/it]                                                   {'loss': 0.0327, 'learning_rate': 0.00017944880208594155, 'epoch': 0.7}
 23%|██▎       | 118/507 [32:32<1:46:53, 16.49s/it] 23%|██▎       | 119/507 [32:48<1:46:40, 16.50s/it]                                                   {'loss': 0.0289, 'learning_rate': 0.0001790586192342057, 'epoch': 0.7}
 23%|██▎       | 119/507 [32:48<1:46:40, 16.50s/it] 24%|██▎       | 120/507 [33:05<1:46:14, 16.47s/it]                                                   {'loss': 0.0349, 'learning_rate': 0.00017866519981619394, 'epoch': 0.71}
 24%|██▎       | 120/507 [33:05<1:46:14, 16.47s/it] 24%|██▍       | 121/507 [33:21<1:46:00, 16.48s/it]                                                   {'loss': 0.0394, 'learning_rate': 0.00017826855993803147, 'epoch': 0.71}
 24%|██▍       | 121/507 [33:21<1:46:00, 16.48s/it] 24%|██▍       | 122/507 [33:38<1:45:48, 16.49s/it]                                                   {'loss': 0.0656, 'learning_rate': 0.00017786871583768535, 'epoch': 0.72}
 24%|██▍       | 122/507 [33:38<1:45:48, 16.49s/it] 24%|██▍       | 123/507 [33:54<1:45:19, 16.46s/it]                                                   {'loss': 0.0547, 'learning_rate': 0.00017746568388429966, 'epoch': 0.73}
 24%|██▍       | 123/507 [33:54<1:45:19, 16.46s/it] 24%|██▍       | 124/507 [34:10<1:45:06, 16.47s/it]                                                   {'loss': 0.0399, 'learning_rate': 0.00017705948057752545, 'epoch': 0.73}
 24%|██▍       | 124/507 [34:10<1:45:06, 16.47s/it] 25%|██▍       | 125/507 [34:27<1:44:49, 16.46s/it]                                                   {'loss': 0.0506, 'learning_rate': 0.00017665012254684524, 'epoch': 0.74}
 25%|██▍       | 125/507 [34:27<1:44:49, 16.46s/it] 25%|██▍       | 126/507 [34:43<1:44:15, 16.42s/it]                                                   {'loss': 0.0357, 'learning_rate': 0.00017623762655089207, 'epoch': 0.74}
 25%|██▍       | 126/507 [34:43<1:44:15, 16.42s/it] 25%|██▌       | 127/507 [35:00<1:44:10, 16.45s/it]                                                   {'loss': 0.0259, 'learning_rate': 0.0001758220094767638, 'epoch': 0.75}
 25%|██▌       | 127/507 [35:00<1:44:10, 16.45s/it] 25%|██▌       | 128/507 [35:16<1:43:39, 16.41s/it]                                                   {'loss': 0.047, 'learning_rate': 0.0001754032883393313, 'epoch': 0.76}
 25%|██▌       | 128/507 [35:16<1:43:39, 16.41s/it] 25%|██▌       | 129/507 [35:33<1:43:32, 16.44s/it]                                                   {'loss': 0.0421, 'learning_rate': 0.0001749814802805423, 'epoch': 0.76}
 25%|██▌       | 129/507 [35:33<1:43:32, 16.44s/it] 26%|██▌       | 130/507 [35:49<1:43:19, 16.44s/it]                                                   {'loss': 0.04, 'learning_rate': 0.00017455660256871931, 'epoch': 0.77}
 26%|██▌       | 130/507 [35:49<1:43:19, 16.44s/it] 26%|██▌       | 131/507 [36:06<1:43:10, 16.46s/it]                                                   {'loss': 0.0491, 'learning_rate': 0.00017412867259785286, 'epoch': 0.77}
 26%|██▌       | 131/507 [36:06<1:43:10, 16.46s/it] 26%|██▌       | 132/507 [36:22<1:42:59, 16.48s/it]                                                   {'loss': 0.0108, 'learning_rate': 0.00017369770788688938, 'epoch': 0.78}
 26%|██▌       | 132/507 [36:22<1:42:59, 16.48s/it] 26%|██▌       | 133/507 [36:39<1:42:46, 16.49s/it]                                                   {'loss': 0.0249, 'learning_rate': 0.00017326372607901386, 'epoch': 0.79}
 26%|██▌       | 133/507 [36:39<1:42:46, 16.49s/it] 26%|██▋       | 134/507 [36:55<1:42:32, 16.49s/it]                                                   {'loss': 0.0732, 'learning_rate': 0.0001728267449409278, 'epoch': 0.79}
 26%|██▋       | 134/507 [36:55<1:42:32, 16.49s/it] 27%|██▋       | 135/507 [37:12<1:42:17, 16.50s/it]                                                   {'loss': 0.0531, 'learning_rate': 0.0001723867823621216, 'epoch': 0.8}
 27%|██▋       | 135/507 [37:12<1:42:17, 16.50s/it] 27%|██▋       | 136/507 [37:28<1:42:01, 16.50s/it]                                                   {'loss': 0.0485, 'learning_rate': 0.00017194385635414244, 'epoch': 0.8}
 27%|██▋       | 136/507 [37:28<1:42:01, 16.50s/it] 27%|██▋       | 137/507 [37:45<1:41:49, 16.51s/it]                                                   {'loss': 0.0374, 'learning_rate': 0.00017149798504985665, 'epoch': 0.81}
 27%|██▋       | 137/507 [37:45<1:41:49, 16.51s/it] 27%|██▋       | 138/507 [38:01<1:41:33, 16.51s/it]                                                   {'loss': 0.0399, 'learning_rate': 0.00017104918670270762, 'epoch': 0.82}
 27%|██▋       | 138/507 [38:01<1:41:33, 16.51s/it] 27%|██▋       | 139/507 [38:18<1:41:15, 16.51s/it]                                                   {'loss': 0.0381, 'learning_rate': 0.00017059747968596836, 'epoch': 0.82}
 27%|██▋       | 139/507 [38:18<1:41:15, 16.51s/it] 28%|██▊       | 140/507 [38:34<1:41:00, 16.51s/it]                                                   {'loss': 0.0183, 'learning_rate': 0.00017014288249198934, 'epoch': 0.83}
 28%|██▊       | 140/507 [38:34<1:41:00, 16.51s/it] 28%|██▊       | 141/507 [38:51<1:40:46, 16.52s/it]                                                   {'loss': 0.0589, 'learning_rate': 0.00016968541373144156, 'epoch': 0.83}
 28%|██▊       | 141/507 [38:51<1:40:46, 16.52s/it] 28%|██▊       | 142/507 [39:07<1:40:26, 16.51s/it]                                                   {'loss': 0.0127, 'learning_rate': 0.0001692250921325544, 'epoch': 0.84}
 28%|██▊       | 142/507 [39:07<1:40:26, 16.51s/it] 28%|██▊       | 143/507 [39:23<1:39:44, 16.44s/it]                                                   {'loss': 0.0319, 'learning_rate': 0.0001687619365403492, 'epoch': 0.84}
 28%|██▊       | 143/507 [39:23<1:39:44, 16.44s/it] 28%|██▊       | 144/507 [39:40<1:39:34, 16.46s/it]                                                   {'loss': 0.0546, 'learning_rate': 0.0001682959659158676, 'epoch': 0.85}
 28%|██▊       | 144/507 [39:40<1:39:34, 16.46s/it] 29%|██▊       | 145/507 [39:57<1:39:26, 16.48s/it]                                                   {'loss': 0.0156, 'learning_rate': 0.0001678271993353953, 'epoch': 0.86}
 29%|██▊       | 145/507 [39:57<1:39:26, 16.48s/it] 29%|██▉       | 146/507 [40:13<1:39:12, 16.49s/it]                                                   {'loss': 0.0202, 'learning_rate': 0.00016735565598968114, 'epoch': 0.86}
 29%|██▉       | 146/507 [40:13<1:39:12, 16.49s/it] 29%|██▉       | 147/507 [40:30<1:38:56, 16.49s/it]                                                   {'loss': 0.0267, 'learning_rate': 0.00016688135518315144, 'epoch': 0.87}
 29%|██▉       | 147/507 [40:30<1:38:56, 16.49s/it] 29%|██▉       | 148/507 [40:46<1:38:43, 16.50s/it]                                                   {'loss': 0.0281, 'learning_rate': 0.00016640431633311973, 'epoch': 0.87}
 29%|██▉       | 148/507 [40:46<1:38:43, 16.50s/it] 29%|██▉       | 149/507 [41:03<1:38:25, 16.50s/it]                                                   {'loss': 0.0295, 'learning_rate': 0.00016592455896899177, 'epoch': 0.88}
 29%|██▉       | 149/507 [41:03<1:38:25, 16.50s/it] 30%|██▉       | 150/507 [41:19<1:38:00, 16.47s/it]                                                   {'loss': 0.0163, 'learning_rate': 0.00016544210273146607, 'epoch': 0.89}
 30%|██▉       | 150/507 [41:19<1:38:00, 16.47s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 30%|██▉       | 151/507 [41:42<1:49:17, 18.42s/it]                                                   {'loss': 0.0306, 'learning_rate': 0.0001649569673717298, 'epoch': 0.89}
 30%|██▉       | 151/507 [41:42<1:49:17, 18.42s/it] 30%|██▉       | 152/507 [41:58<1:45:20, 17.80s/it]                                                   {'loss': 0.0386, 'learning_rate': 0.0001644691727506503, 'epoch': 0.9}
 30%|██▉       | 152/507 [41:58<1:45:20, 17.80s/it] 30%|███       | 153/507 [42:15<1:42:30, 17.37s/it]                                                   {'loss': 0.0202, 'learning_rate': 0.00016397873883796182, 'epoch': 0.9}
 30%|███       | 153/507 [42:15<1:42:30, 17.37s/it] 30%|███       | 154/507 [42:31<1:40:40, 17.11s/it]                                                   {'loss': 0.0251, 'learning_rate': 0.00016348568571144815, 'epoch': 0.91}
 30%|███       | 154/507 [42:31<1:40:40, 17.11s/it] 31%|███       | 155/507 [42:48<1:39:19, 16.93s/it]                                                   {'loss': 0.0415, 'learning_rate': 0.0001629900335561206, 'epoch': 0.92}
 31%|███       | 155/507 [42:48<1:39:19, 16.93s/it] 31%|███       | 156/507 [43:04<1:38:09, 16.78s/it]                                                   {'loss': 0.0489, 'learning_rate': 0.0001624918026633916, 'epoch': 0.92}
 31%|███       | 156/507 [43:04<1:38:09, 16.78s/it] 31%|███       | 157/507 [43:21<1:37:25, 16.70s/it]                                                   {'loss': 0.0264, 'learning_rate': 0.00016199101343024403, 'epoch': 0.93}
 31%|███       | 157/507 [43:21<1:37:25, 16.70s/it] 31%|███       | 158/507 [43:37<1:36:47, 16.64s/it]                                                   {'loss': 0.0256, 'learning_rate': 0.00016148768635839623, 'epoch': 0.93}
 31%|███       | 158/507 [43:37<1:36:47, 16.64s/it] 31%|███▏      | 159/507 [43:54<1:36:17, 16.60s/it]                                                   {'loss': 0.0355, 'learning_rate': 0.0001609818420534627, 'epoch': 0.94}
 31%|███▏      | 159/507 [43:54<1:36:17, 16.60s/it] 32%|███▏      | 160/507 [44:10<1:35:51, 16.58s/it]                                                   {'loss': 0.0396, 'learning_rate': 0.00016047350122411037, 'epoch': 0.95}
 32%|███▏      | 160/507 [44:10<1:35:51, 16.58s/it] 32%|███▏      | 161/507 [44:27<1:35:19, 16.53s/it]                                                   {'loss': 0.0554, 'learning_rate': 0.00015996268468121102, 'epoch': 0.95}
 32%|███▏      | 161/507 [44:27<1:35:19, 16.53s/it] 32%|███▏      | 162/507 [44:43<1:34:58, 16.52s/it]                                                   {'loss': 0.0344, 'learning_rate': 0.00015944941333698913, 'epoch': 0.96}
 32%|███▏      | 162/507 [44:43<1:34:58, 16.52s/it] 32%|███▏      | 163/507 [44:59<1:34:23, 16.46s/it]                                                   {'loss': 0.0562, 'learning_rate': 0.00015893370820416593, 'epoch': 0.96}
 32%|███▏      | 163/507 [44:59<1:34:23, 16.46s/it] 32%|███▏      | 164/507 [45:16<1:33:59, 16.44s/it]                                                   {'loss': 0.0339, 'learning_rate': 0.00015841559039509896, 'epoch': 0.97}
 32%|███▏      | 164/507 [45:16<1:33:59, 16.44s/it] 33%|███▎      | 165/507 [45:32<1:33:47, 16.45s/it]                                                   {'loss': 0.0284, 'learning_rate': 0.00015789508112091803, 'epoch': 0.97}
 33%|███▎      | 165/507 [45:32<1:33:47, 16.45s/it] 33%|███▎      | 166/507 [45:49<1:33:39, 16.48s/it]                                                   {'loss': 0.0364, 'learning_rate': 0.00015737220169065655, 'epoch': 0.98}
 33%|███▎      | 166/507 [45:49<1:33:39, 16.48s/it] 33%|███▎      | 167/507 [46:05<1:33:15, 16.46s/it]                                                   {'loss': 0.0314, 'learning_rate': 0.00015684697351037936, 'epoch': 0.99}
 33%|███▎      | 167/507 [46:05<1:33:15, 16.46s/it] 33%|███▎      | 168/507 [46:22<1:33:05, 16.48s/it]                                                   {'loss': 0.0139, 'learning_rate': 0.00015631941808230638, 'epoch': 0.99}
 33%|███▎      | 168/507 [46:22<1:33:05, 16.48s/it] 33%|███▎      | 169/507 [46:38<1:32:49, 16.48s/it]                                                   {'loss': 0.0217, 'learning_rate': 0.00015578955700393227, 'epoch': 1.0}
 33%|███▎      | 169/507 [46:38<1:32:49, 16.48s/it] 34%|███▎      | 170/507 [46:56<1:34:00, 16.74s/it]                                                   {'loss': 0.0501, 'learning_rate': 0.0001552574119671423, 'epoch': 1.0}
 34%|███▎      | 170/507 [46:56<1:34:00, 16.74s/it] 34%|███▎      | 171/507 [47:12<1:33:20, 16.67s/it]                                                   {'loss': 0.0293, 'learning_rate': 0.00015472300475732426, 'epoch': 1.01}
 34%|███▎      | 171/507 [47:12<1:33:20, 16.67s/it] 34%|███▍      | 172/507 [47:28<1:32:28, 16.56s/it]                                                   {'loss': 0.0512, 'learning_rate': 0.00015418635725247666, 'epoch': 1.02}
 34%|███▍      | 172/507 [47:28<1:32:28, 16.56s/it] 34%|███▍      | 173/507 [47:45<1:32:05, 16.54s/it]                                                   {'loss': 0.0237, 'learning_rate': 0.00015364749142231303, 'epoch': 1.02}
 34%|███▍      | 173/507 [47:45<1:32:05, 16.54s/it] 34%|███▍      | 174/507 [48:01<1:31:43, 16.53s/it]                                                   {'loss': 0.0402, 'learning_rate': 0.00015310642932736253, 'epoch': 1.03}
 34%|███▍      | 174/507 [48:01<1:31:43, 16.53s/it] 35%|███▍      | 175/507 [48:18<1:31:25, 16.52s/it]                                                   {'loss': 0.0291, 'learning_rate': 0.00015256319311806671, 'epoch': 1.03}
 35%|███▍      | 175/507 [48:18<1:31:25, 16.52s/it] 35%|███▍      | 176/507 [48:34<1:31:08, 16.52s/it]                                                   {'loss': 0.0464, 'learning_rate': 0.0001520178050338729, 'epoch': 1.04}
 35%|███▍      | 176/507 [48:34<1:31:08, 16.52s/it] 35%|███▍      | 177/507 [48:51<1:30:35, 16.47s/it]                                                   {'loss': 0.0218, 'learning_rate': 0.0001514702874023236, 'epoch': 1.05}
 35%|███▍      | 177/507 [48:51<1:30:35, 16.47s/it] 35%|███▌      | 178/507 [49:07<1:30:16, 16.46s/it]                                                   {'loss': 0.0385, 'learning_rate': 0.00015092066263814243, 'epoch': 1.05}
 35%|███▌      | 178/507 [49:07<1:30:16, 16.46s/it] 35%|███▌      | 179/507 [49:24<1:30:03, 16.47s/it]                                                   {'loss': 0.0147, 'learning_rate': 0.0001503689532423166, 'epoch': 1.06}
 35%|███▌      | 179/507 [49:24<1:30:03, 16.47s/it] 36%|███▌      | 180/507 [49:40<1:29:51, 16.49s/it]                                                   {'loss': 0.021, 'learning_rate': 0.00014981518180117557, 'epoch': 1.06}
 36%|███▌      | 180/507 [49:40<1:29:51, 16.49s/it] 36%|███▌      | 181/507 [49:57<1:29:27, 16.47s/it]                                                   {'loss': 0.0437, 'learning_rate': 0.00014925937098546652, 'epoch': 1.07}
 36%|███▌      | 181/507 [49:57<1:29:27, 16.47s/it] 36%|███▌      | 182/507 [50:13<1:29:02, 16.44s/it]                                                   {'loss': 0.0474, 'learning_rate': 0.0001487015435494263, 'epoch': 1.08}
 36%|███▌      | 182/507 [50:13<1:29:02, 16.44s/it] 36%|███▌      | 183/507 [50:29<1:28:53, 16.46s/it]                                                   {'loss': 0.0181, 'learning_rate': 0.00014814172232984968, 'epoch': 1.08}
 36%|███▌      | 183/507 [50:29<1:28:53, 16.46s/it] 36%|███▋      | 184/507 [50:46<1:28:38, 16.47s/it]                                                   {'loss': 0.0086, 'learning_rate': 0.0001475799302451547, 'epoch': 1.09}
 36%|███▋      | 184/507 [50:46<1:28:38, 16.47s/it] 36%|███▋      | 185/507 [51:02<1:28:25, 16.48s/it]                                                   {'loss': 0.0334, 'learning_rate': 0.0001470161902944442, 'epoch': 1.09}
 36%|███▋      | 185/507 [51:02<1:28:25, 16.48s/it] 37%|███▋      | 186/507 [51:19<1:27:56, 16.44s/it]                                                   {'loss': 0.0375, 'learning_rate': 0.00014645052555656431, 'epoch': 1.1}
 37%|███▋      | 186/507 [51:19<1:27:56, 16.44s/it] 37%|███▋      | 187/507 [51:35<1:27:46, 16.46s/it]                                                   {'loss': 0.0165, 'learning_rate': 0.00014588295918915978, 'epoch': 1.1}
 37%|███▋      | 187/507 [51:35<1:27:46, 16.46s/it] 37%|███▋      | 188/507 [51:52<1:27:34, 16.47s/it]                                                   {'loss': 0.0221, 'learning_rate': 0.0001453135144277257, 'epoch': 1.11}
 37%|███▋      | 188/507 [51:52<1:27:34, 16.47s/it] 37%|███▋      | 189/507 [52:08<1:27:16, 16.47s/it]                                                   {'loss': 0.0364, 'learning_rate': 0.0001447422145846565, 'epoch': 1.12}
 37%|███▋      | 189/507 [52:08<1:27:16, 16.47s/it] 37%|███▋      | 190/507 [52:25<1:27:05, 16.48s/it]                                                   {'loss': 0.0218, 'learning_rate': 0.00014416908304829142, 'epoch': 1.12}
 37%|███▋      | 190/507 [52:25<1:27:05, 16.48s/it] 38%|███▊      | 191/507 [52:41<1:26:48, 16.48s/it]                                                   {'loss': 0.0235, 'learning_rate': 0.00014359414328195703, 'epoch': 1.13}
 38%|███▊      | 191/507 [52:41<1:26:48, 16.48s/it] 38%|███▊      | 192/507 [52:58<1:26:33, 16.49s/it]                                                   {'loss': 0.0108, 'learning_rate': 0.00014301741882300672, 'epoch': 1.13}
 38%|███▊      | 192/507 [52:58<1:26:33, 16.49s/it] 38%|███▊      | 193/507 [53:14<1:26:07, 16.46s/it]                                                   {'loss': 0.0321, 'learning_rate': 0.000142438933281857, 'epoch': 1.14}
 38%|███▊      | 193/507 [53:14<1:26:07, 16.46s/it] 38%|███▊      | 194/507 [53:31<1:25:53, 16.47s/it]                                                   {'loss': 0.0123, 'learning_rate': 0.00014185871034102116, 'epoch': 1.15}
 38%|███▊      | 194/507 [53:31<1:25:53, 16.47s/it] 38%|███▊      | 195/507 [53:47<1:25:27, 16.43s/it]                                                   {'loss': 0.0463, 'learning_rate': 0.00014127677375413942, 'epoch': 1.15}
 38%|███▊      | 195/507 [53:47<1:25:27, 16.43s/it] 39%|███▊      | 196/507 [54:04<1:25:17, 16.46s/it]                                                   {'loss': 0.0107, 'learning_rate': 0.00014069314734500675, 'epoch': 1.16}
 39%|███▊      | 196/507 [54:04<1:25:17, 16.46s/it] 39%|███▉      | 197/507 [54:20<1:25:02, 16.46s/it]                                                   {'loss': 0.011, 'learning_rate': 0.00014010785500659736, 'epoch': 1.16}
 39%|███▉      | 197/507 [54:20<1:25:02, 16.46s/it] 39%|███▉      | 198/507 [54:36<1:24:50, 16.47s/it]                                                   {'loss': 0.015, 'learning_rate': 0.0001395209207000867, 'epoch': 1.17}
 39%|███▉      | 198/507 [54:36<1:24:50, 16.47s/it] 39%|███▉      | 199/507 [54:53<1:24:34, 16.48s/it]                                                   {'loss': 0.0292, 'learning_rate': 0.00013893236845387042, 'epoch': 1.18}
 39%|███▉      | 199/507 [54:53<1:24:34, 16.48s/it] 39%|███▉      | 200/507 [55:09<1:24:21, 16.49s/it]                                                   {'loss': 0.0094, 'learning_rate': 0.0001383422223625807, 'epoch': 1.18}
 39%|███▉      | 200/507 [55:09<1:24:21, 16.49s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 40%|███▉      | 201/507 [55:33<1:34:37, 18.55s/it]                                                   {'loss': 0.0179, 'learning_rate': 0.00013775050658609988, 'epoch': 1.19}
 40%|███▉      | 201/507 [55:33<1:34:37, 18.55s/it] 40%|███▉      | 202/507 [55:49<1:30:48, 17.86s/it]                                                   {'loss': 0.0401, 'learning_rate': 0.00013715724534857127, 'epoch': 1.19}
 40%|███▉      | 202/507 [55:49<1:30:48, 17.86s/it] 40%|████      | 203/507 [56:06<1:28:27, 17.46s/it]                                                   {'loss': 0.0575, 'learning_rate': 0.00013656246293740766, 'epoch': 1.2}
 40%|████      | 203/507 [56:06<1:28:27, 17.46s/it] 40%|████      | 204/507 [56:22<1:26:43, 17.17s/it]                                                   {'loss': 0.0318, 'learning_rate': 0.0001359661837022968, 'epoch': 1.21}
 40%|████      | 204/507 [56:22<1:26:43, 17.17s/it] 40%|████      | 205/507 [56:39<1:25:24, 16.97s/it]                                                   {'loss': 0.013, 'learning_rate': 0.0001353684320542046, 'epoch': 1.21}
 40%|████      | 205/507 [56:39<1:25:24, 16.97s/it] 41%|████      | 206/507 [56:55<1:24:25, 16.83s/it]                                                   {'loss': 0.0167, 'learning_rate': 0.0001347692324643759, 'epoch': 1.22}
 41%|████      | 206/507 [56:55<1:24:25, 16.83s/it] 41%|████      | 207/507 [57:12<1:23:38, 16.73s/it]                                                   {'loss': 0.0176, 'learning_rate': 0.00013416860946333255, 'epoch': 1.22}
 41%|████      | 207/507 [57:12<1:23:38, 16.73s/it] 41%|████      | 208/507 [57:28<1:22:47, 16.61s/it]                                                   {'loss': 0.0323, 'learning_rate': 0.00013356658763986917, 'epoch': 1.23}
 41%|████      | 208/507 [57:28<1:22:47, 16.61s/it] 41%|████      | 209/507 [57:44<1:22:19, 16.58s/it]                                                   {'loss': 0.0151, 'learning_rate': 0.00013296319164004644, 'epoch': 1.23}
 41%|████      | 209/507 [57:44<1:22:19, 16.58s/it] 41%|████▏     | 210/507 [58:01<1:21:55, 16.55s/it]                                                   {'loss': 0.0196, 'learning_rate': 0.0001323584461661823, 'epoch': 1.24}
 41%|████▏     | 210/507 [58:01<1:21:55, 16.55s/it] 42%|████▏     | 211/507 [58:17<1:21:28, 16.51s/it]                                                   {'loss': 0.0347, 'learning_rate': 0.00013175237597584045, 'epoch': 1.25}
 42%|████▏     | 211/507 [58:17<1:21:28, 16.51s/it] 42%|████▏     | 212/507 [58:34<1:21:11, 16.51s/it]                                                   {'loss': 0.0064, 'learning_rate': 0.00013114500588081698, 'epoch': 1.25}
 42%|████▏     | 212/507 [58:34<1:21:11, 16.51s/it] 42%|████▏     | 213/507 [58:50<1:20:53, 16.51s/it]                                                   {'loss': 0.0087, 'learning_rate': 0.00013053636074612457, 'epoch': 1.26}
 42%|████▏     | 213/507 [58:50<1:20:53, 16.51s/it] 42%|████▏     | 214/507 [59:07<1:20:26, 16.47s/it]                                                   {'loss': 0.0219, 'learning_rate': 0.00012992646548897442, 'epoch': 1.26}
 42%|████▏     | 214/507 [59:07<1:20:26, 16.47s/it] 42%|████▏     | 215/507 [59:23<1:20:03, 16.45s/it]                                                   {'loss': 0.0409, 'learning_rate': 0.0001293153450777564, 'epoch': 1.27}
 42%|████▏     | 215/507 [59:23<1:20:03, 16.45s/it] 43%|████▎     | 216/507 [59:40<1:19:51, 16.47s/it]                                                   {'loss': 0.0237, 'learning_rate': 0.00012870302453101657, 'epoch': 1.28}
 43%|████▎     | 216/507 [59:40<1:19:51, 16.47s/it] 43%|████▎     | 217/507 [59:56<1:19:30, 16.45s/it]                                                   {'loss': 0.0329, 'learning_rate': 0.00012808952891643326, 'epoch': 1.28}
 43%|████▎     | 217/507 [59:56<1:19:30, 16.45s/it] 43%|████▎     | 218/507 [1:00:13<1:19:18, 16.47s/it]                                                     {'loss': 0.0248, 'learning_rate': 0.00012747488334979062, 'epoch': 1.29}
 43%|████▎     | 218/507 [1:00:13<1:19:18, 16.47s/it] 43%|████▎     | 219/507 [1:00:29<1:19:06, 16.48s/it]                                                     {'loss': 0.0234, 'learning_rate': 0.00012685911299395046, 'epoch': 1.29}
 43%|████▎     | 219/507 [1:00:29<1:19:06, 16.48s/it] 43%|████▎     | 220/507 [1:00:46<1:18:45, 16.47s/it]                                                     {'loss': 0.0148, 'learning_rate': 0.00012624224305782215, 'epoch': 1.3}
 43%|████▎     | 220/507 [1:00:46<1:18:45, 16.47s/it] 44%|████▎     | 221/507 [1:01:02<1:18:26, 16.46s/it]                                                     {'loss': 0.0301, 'learning_rate': 0.0001256242987953306, 'epoch': 1.31}
 44%|████▎     | 221/507 [1:01:02<1:18:26, 16.46s/it] 44%|████▍     | 222/507 [1:01:18<1:18:14, 16.47s/it]                                                     {'loss': 0.0165, 'learning_rate': 0.00012500530550438232, 'epoch': 1.31}
 44%|████▍     | 222/507 [1:01:18<1:18:14, 16.47s/it] 44%|████▍     | 223/507 [1:01:35<1:17:55, 16.46s/it]                                                     {'loss': 0.0544, 'learning_rate': 0.00012438528852582988, 'epoch': 1.32}
 44%|████▍     | 223/507 [1:01:35<1:17:55, 16.46s/it] 44%|████▍     | 224/507 [1:01:51<1:17:36, 16.46s/it]                                                     {'loss': 0.0306, 'learning_rate': 0.00012376427324243432, 'epoch': 1.32}
 44%|████▍     | 224/507 [1:01:51<1:17:36, 16.46s/it] 44%|████▍     | 225/507 [1:02:08<1:17:24, 16.47s/it]                                                     {'loss': 0.014, 'learning_rate': 0.00012314228507782614, 'epoch': 1.33}
 44%|████▍     | 225/507 [1:02:08<1:17:24, 16.47s/it] 45%|████▍     | 226/507 [1:02:24<1:17:11, 16.48s/it]                                                     {'loss': 0.0442, 'learning_rate': 0.00012251934949546447, 'epoch': 1.34}
 45%|████▍     | 226/507 [1:02:24<1:17:11, 16.48s/it] 45%|████▍     | 227/507 [1:02:41<1:16:56, 16.49s/it]                                                     {'loss': 0.0094, 'learning_rate': 0.00012189549199759453, 'epoch': 1.34}
 45%|████▍     | 227/507 [1:02:41<1:16:56, 16.49s/it] 45%|████▍     | 228/507 [1:02:57<1:16:35, 16.47s/it]                                                     {'loss': 0.0554, 'learning_rate': 0.00012127073812420375, 'epoch': 1.35}
 45%|████▍     | 228/507 [1:02:57<1:16:35, 16.47s/it] 45%|████▌     | 229/507 [1:03:14<1:16:13, 16.45s/it]                                                     {'loss': 0.0246, 'learning_rate': 0.00012064511345197607, 'epoch': 1.35}
 45%|████▌     | 229/507 [1:03:14<1:16:13, 16.45s/it] 45%|████▌     | 230/507 [1:03:30<1:16:01, 16.47s/it]                                                     {'loss': 0.0224, 'learning_rate': 0.00012001864359324489, 'epoch': 1.36}
 45%|████▌     | 230/507 [1:03:30<1:16:01, 16.47s/it] 46%|████▌     | 231/507 [1:03:47<1:15:49, 16.48s/it]                                                     {'loss': 0.0351, 'learning_rate': 0.00011939135419494456, 'epoch': 1.36}
 46%|████▌     | 231/507 [1:03:47<1:15:49, 16.48s/it] 46%|████▌     | 232/507 [1:04:03<1:15:30, 16.48s/it]                                                     {'loss': 0.0151, 'learning_rate': 0.00011876327093756047, 'epoch': 1.37}
 46%|████▌     | 232/507 [1:04:03<1:15:30, 16.48s/it] 46%|████▌     | 233/507 [1:04:20<1:15:17, 16.49s/it]                                                     {'loss': 0.0102, 'learning_rate': 0.00011813441953407754, 'epoch': 1.38}
 46%|████▌     | 233/507 [1:04:20<1:15:17, 16.49s/it] 46%|████▌     | 234/507 [1:04:36<1:15:01, 16.49s/it]                                                     {'loss': 0.0131, 'learning_rate': 0.0001175048257289278, 'epoch': 1.38}
 46%|████▌     | 234/507 [1:04:36<1:15:01, 16.49s/it] 46%|████▋     | 235/507 [1:04:53<1:14:33, 16.45s/it]                                                     {'loss': 0.0149, 'learning_rate': 0.00011687451529693624, 'epoch': 1.39}
 46%|████▋     | 235/507 [1:04:53<1:14:33, 16.45s/it] 47%|████▋     | 236/507 [1:05:09<1:14:12, 16.43s/it]                                                     {'loss': 0.0338, 'learning_rate': 0.00011624351404226572, 'epoch': 1.39}
 47%|████▋     | 236/507 [1:05:09<1:14:12, 16.43s/it] 47%|████▋     | 237/507 [1:05:25<1:14:02, 16.46s/it]                                                     {'loss': 0.0131, 'learning_rate': 0.00011561184779736061, 'epoch': 1.4}
 47%|████▋     | 237/507 [1:05:25<1:14:02, 16.46s/it] 47%|████▋     | 238/507 [1:05:42<1:13:48, 16.46s/it]                                                     {'loss': 0.0259, 'learning_rate': 0.00011497954242188913, 'epoch': 1.41}
 47%|████▋     | 238/507 [1:05:42<1:13:48, 16.46s/it] 47%|████▋     | 239/507 [1:05:58<1:13:28, 16.45s/it]                                                     {'loss': 0.0279, 'learning_rate': 0.00011434662380168486, 'epoch': 1.41}
 47%|████▋     | 239/507 [1:05:58<1:13:28, 16.45s/it] 47%|████▋     | 240/507 [1:06:15<1:13:16, 16.47s/it]                                                     {'loss': 0.0202, 'learning_rate': 0.00011371311784768673, 'epoch': 1.42}
 47%|████▋     | 240/507 [1:06:15<1:13:16, 16.47s/it] 48%|████▊     | 241/507 [1:06:31<1:12:57, 16.46s/it]                                                     {'loss': 0.013, 'learning_rate': 0.00011307905049487855, 'epoch': 1.42}
 48%|████▊     | 241/507 [1:06:31<1:12:57, 16.46s/it] 48%|████▊     | 242/507 [1:06:48<1:12:44, 16.47s/it]                                                     {'loss': 0.0231, 'learning_rate': 0.00011244444770122707, 'epoch': 1.43}
 48%|████▊     | 242/507 [1:06:48<1:12:44, 16.47s/it] 48%|████▊     | 243/507 [1:07:04<1:12:29, 16.48s/it]                                                     {'loss': 0.0319, 'learning_rate': 0.00011180933544661936, 'epoch': 1.44}
 48%|████▊     | 243/507 [1:07:04<1:12:29, 16.48s/it] 48%|████▊     | 244/507 [1:07:21<1:12:08, 16.46s/it]                                                     {'loss': 0.0359, 'learning_rate': 0.00011117373973179925, 'epoch': 1.44}
 48%|████▊     | 244/507 [1:07:21<1:12:08, 16.46s/it] 48%|████▊     | 245/507 [1:07:37<1:11:57, 16.48s/it]                                                     {'loss': 0.0313, 'learning_rate': 0.00011053768657730284, 'epoch': 1.45}
 48%|████▊     | 245/507 [1:07:37<1:11:57, 16.48s/it] 49%|████▊     | 246/507 [1:07:54<1:11:43, 16.49s/it]                                                     {'loss': 0.0183, 'learning_rate': 0.00010990120202239324, 'epoch': 1.45}
 49%|████▊     | 246/507 [1:07:54<1:11:43, 16.49s/it] 49%|████▊     | 247/507 [1:08:10<1:11:27, 16.49s/it]                                                     {'loss': 0.021, 'learning_rate': 0.00010926431212399466, 'epoch': 1.46}
 49%|████▊     | 247/507 [1:08:10<1:11:27, 16.49s/it] 49%|████▉     | 248/507 [1:08:27<1:11:13, 16.50s/it]                                                     {'loss': 0.0569, 'learning_rate': 0.0001086270429556255, 'epoch': 1.47}
 49%|████▉     | 248/507 [1:08:27<1:11:13, 16.50s/it] 49%|████▉     | 249/507 [1:08:43<1:10:56, 16.50s/it]                                                     {'loss': 0.0238, 'learning_rate': 0.00010798942060633108, 'epoch': 1.47}
 49%|████▉     | 249/507 [1:08:43<1:10:56, 16.50s/it] 49%|████▉     | 250/507 [1:09:00<1:10:41, 16.50s/it]                                                     {'loss': 0.0097, 'learning_rate': 0.0001073514711796155, 'epoch': 1.48}
 49%|████▉     | 250/507 [1:09:00<1:10:41, 16.50s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 50%|████▉     | 251/507 [1:09:23<1:18:40, 18.44s/it]                                                     {'loss': 0.0132, 'learning_rate': 0.00010671322079237307, 'epoch': 1.48}
 50%|████▉     | 251/507 [1:09:23<1:18:40, 18.44s/it] 50%|████▉     | 252/507 [1:09:39<1:15:53, 17.86s/it]                                                     {'loss': 0.044, 'learning_rate': 0.00010607469557381899, 'epoch': 1.49}
 50%|████▉     | 252/507 [1:09:39<1:15:53, 17.86s/it] 50%|████▉     | 253/507 [1:09:56<1:13:50, 17.44s/it]                                                     {'loss': 0.0085, 'learning_rate': 0.00010543592166441983, 'epoch': 1.49}
 50%|████▉     | 253/507 [1:09:56<1:13:50, 17.44s/it] 50%|█████     | 254/507 [1:10:12<1:12:22, 17.16s/it]                                                     {'loss': 0.0159, 'learning_rate': 0.00010479692521482316, 'epoch': 1.5}
 50%|█████     | 254/507 [1:10:12<1:12:22, 17.16s/it] 50%|█████     | 255/507 [1:10:29<1:11:09, 16.94s/it]                                                     {'loss': 0.032, 'learning_rate': 0.00010415773238478715, 'epoch': 1.51}
 50%|█████     | 255/507 [1:10:29<1:11:09, 16.94s/it] 50%|█████     | 256/507 [1:10:45<1:10:12, 16.78s/it]                                                     {'loss': 0.0297, 'learning_rate': 0.00010351836934210957, 'epoch': 1.51}
 50%|█████     | 256/507 [1:10:45<1:10:12, 16.78s/it] 51%|█████     | 257/507 [1:11:01<1:09:27, 16.67s/it]                                                     {'loss': 0.02, 'learning_rate': 0.00010287886226155641, 'epoch': 1.52}
 51%|█████     | 257/507 [1:11:01<1:09:27, 16.67s/it] 51%|█████     | 258/507 [1:11:18<1:08:58, 16.62s/it]                                                     {'loss': 0.013, 'learning_rate': 0.00010223923732379048, 'epoch': 1.52}
 51%|█████     | 258/507 [1:11:18<1:08:58, 16.62s/it] 51%|█████     | 259/507 [1:11:34<1:08:32, 16.58s/it]                                                     {'loss': 0.0299, 'learning_rate': 0.00010159952071429952, 'epoch': 1.53}
 51%|█████     | 259/507 [1:11:34<1:08:32, 16.58s/it] 51%|█████▏    | 260/507 [1:11:51<1:07:42, 16.45s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.54}
 51%|█████▏    | 260/507 [1:11:51<1:07:42, 16.45s/it] 51%|█████▏    | 261/507 [1:12:07<1:07:04, 16.36s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.54}
 51%|█████▏    | 261/507 [1:12:07<1:07:04, 16.36s/it] 52%|█████▏    | 262/507 [1:12:23<1:06:27, 16.27s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.55}
 52%|█████▏    | 262/507 [1:12:23<1:06:27, 16.27s/it] 52%|█████▏    | 263/507 [1:12:39<1:05:52, 16.20s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.55}
 52%|█████▏    | 263/507 [1:12:39<1:05:52, 16.20s/it] 52%|█████▏    | 264/507 [1:12:55<1:05:33, 16.19s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.56}
 52%|█████▏    | 264/507 [1:12:55<1:05:33, 16.19s/it] 52%|█████▏    | 265/507 [1:13:11<1:05:14, 16.17s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.57}
 52%|█████▏    | 265/507 [1:13:11<1:05:14, 16.17s/it] 52%|█████▏    | 266/507 [1:13:27<1:04:50, 16.14s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.57}
 52%|█████▏    | 266/507 [1:13:27<1:04:50, 16.14s/it] 53%|█████▎    | 267/507 [1:13:43<1:04:34, 16.14s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.58}
 53%|█████▎    | 267/507 [1:13:43<1:04:34, 16.14s/it] 53%|█████▎    | 268/507 [1:13:59<1:04:11, 16.11s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.58}
 53%|█████▎    | 268/507 [1:13:59<1:04:11, 16.11s/it] 53%|█████▎    | 269/507 [1:14:16<1:03:56, 16.12s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.59}
 53%|█████▎    | 269/507 [1:14:16<1:03:56, 16.12s/it] 53%|█████▎    | 270/507 [1:14:32<1:03:39, 16.12s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.6}
 53%|█████▎    | 270/507 [1:14:32<1:03:39, 16.12s/it] 53%|█████▎    | 271/507 [1:14:48<1:03:25, 16.12s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.6}
 53%|█████▎    | 271/507 [1:14:48<1:03:25, 16.12s/it] 54%|█████▎    | 272/507 [1:15:04<1:03:09, 16.13s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.61}
 54%|█████▎    | 272/507 [1:15:04<1:03:09, 16.13s/it] 54%|█████▍    | 273/507 [1:15:20<1:02:56, 16.14s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.61}
 54%|█████▍    | 273/507 [1:15:20<1:02:56, 16.14s/it] 54%|█████▍    | 274/507 [1:15:36<1:02:37, 16.13s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.62}
 54%|█████▍    | 274/507 [1:15:36<1:02:37, 16.13s/it] 54%|█████▍    | 275/507 [1:15:52<1:02:16, 16.11s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.62}
 54%|█████▍    | 275/507 [1:15:52<1:02:16, 16.11s/it] 54%|█████▍    | 276/507 [1:16:08<1:02:03, 16.12s/it]                                                     {'loss': 0.0, 'learning_rate': 0.00010159952071429952, 'epoch': 1.63}
 54%|█████▍    | 276/507 [1:16:08<1:02:03, 16.12s/it]Traceback (most recent call last):
  File "/data3/jisu/LLaVA/llava/train/train_mem.py", line 5, in <module>
Traceback (most recent call last):
  File "/data3/jisu/LLaVA/llava/train/train_mem.py", line 5, in <module>
    train()    
train()
  File "/data3/jisu/LLaVA/llava/train/train.py", line 970, in train
  File "/data3/jisu/LLaVA/llava/train/train.py", line 970, in train
Traceback (most recent call last):
  File "/data3/jisu/LLaVA/llava/train/train_mem.py", line 5, in <module>
    train()
  File "/data3/jisu/LLaVA/llava/train/train.py", line 970, in train
    trainer.train()
    trainer.train()
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
    trainer.train()
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
Traceback (most recent call last):
  File "/data3/jisu/LLaVA/llava/train/train_mem.py", line 5, in <module>
    train()
  File "/data3/jisu/LLaVA/llava/train/train.py", line 970, in train
    trainer.train()
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(    
return inner_training_loop(    
return inner_training_loop( 
                return inner_training_loop(   
              ^   ^   ^ ^ ^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^

  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1869, in _inner_training_loop
^^^^  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1869, in _inner_training_loop
^  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1869, in _inner_training_loop
^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1869, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
    tr_loss_step = self.training_step(model, inputs)
    tr_loss_step = self.training_step(model, inputs)
                            ^  ^  ^  ^  ^  ^^      ^  tr_loss_step = self.training_step(model, inputs)^  
^  ^  ^  ^  ^  ^ ^ ^ ^ ^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^
 ^^^^^  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 2781, in training_step
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 2781, in training_step

^^  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 2781, in training_step
^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 2781, in training_step
    self.accelerator.backward(loss)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py", line 1995, in backward
    self.accelerator.backward(loss)    
self.accelerator.backward(loss)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py", line 1995, in backward
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py", line 1995, in backward
    self.accelerator.backward(loss)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py", line 1995, in backward
        self.deepspeed_engine_wrapped.backward(loss, **kwargs)self.deepspeed_engine_wrapped.backward(loss, **kwargs)

    self.deepspeed_engine_wrapped.backward(loss, **kwargs)      File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/utils/deepspeed.py", line 175, in backward

  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/utils/deepspeed.py", line 175, in backward
self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/utils/deepspeed.py", line 175, in backward
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/utils/deepspeed.py", line 175, in backward
    self.engine.step()        
self.engine.step()self.engine.step()    

self.engine.step()
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2148, in step
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2148, in step
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2148, in step
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2148, in step
            self._take_model_step(lr_kwargs)self._take_model_step(lr_kwargs)self._take_model_step(lr_kwargs)    


self._take_model_step(lr_kwargs)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2054, in _take_model_step
        self.optimizer.step()self.optimizer.step()

        self.optimizer.step()self.optimizer.step()
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1778, in step

  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1778, in step
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1778, in step
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1778, in step
    self._update_scale(self.overflow)        
    self._update_scale(self.overflow)self._update_scale(self.overflow)self._update_scale(self.overflow)


  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2029, in _update_scale
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2029, in _update_scale
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2029, in _update_scale
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2029, in _update_scale
    self.loss_scaler.update_scale(has_overflow)
    self.loss_scaler.update_scale(has_overflow)        
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
self.loss_scaler.update_scale(has_overflow)self.loss_scaler.update_scale(has_overflow)

  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 175, in update_scale
    raise Exception(
Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
    raise Exception(
Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
    raise Exception(
    raise Exception(Exception
: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
 54%|█████▍    | 276/507 [1:16:25<1:03:57, 16.61s/it]
[2026-01-09 20:06:57,043] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 658811
[2026-01-09 20:06:57,155] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 658812
[2026-01-09 20:06:57,233] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 658813
[2026-01-09 20:06:57,344] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 658814
[2026-01-09 20:06:57,345] [ERROR] [launch.py:321:sigkill_handler] ['/data3/jisu/miniconda3/envs/mfm-new/bin/python3.11', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/data3/jisu/LLaVA/visa_llava_instruct.json', '--image_folder', '/data3/jisu/MFM/datasets/ViSA', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bits', '4', '--bf16', 'False', '--fp16', 'True', '--output_dir', './checkpoints/llava-v1.5-7b-mfm-lora', '--num_train_epochs', '3', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.0', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none'] exits with return code = 1
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 20:10:44,723] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:10:56,519] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=3,4,5,6: setting --include=localhost:3,4,5,6
[2026-01-09 20:10:56,579] [INFO] [runner.py:571:main] cmd = /data3/jisu/miniconda3/envs/mfm-new/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMywgNCwgNSwgNl19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero2.json --model_name_or_path liuhaotian/llava-v1.5-7b --version v1 --data_path /data3/jisu/LLaVA/visa_llava_instruct.json --image_folder /data3/jisu/MFM/datasets/ViSA --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bits 4 --bf16 False --fp16 True --output_dir ./checkpoints/llava-v1.5-7b-mfm-lora --resume_from_checkpoint True --num_train_epochs 3 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 False --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to none
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 20:10:58,576] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:10:59,546] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [3, 4, 5, 6]}
[2026-01-09 20:10:59,546] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2026-01-09 20:10:59,546] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2026-01-09 20:10:59,546] [INFO] [launch.py:163:main] dist_world_size=4
[2026-01-09 20:10:59,546] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=3,4,5,6
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 20:11:06,328] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:11:06,334] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:11:06,356] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:11:06,356] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:11:07,288] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 20:11:07,357] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 20:11:07,357] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2026-01-09 20:11:07,409] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 20:11:07,414] [INFO] [comm.py:637:init_distributed] cdb=None
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2026-01-09 20:12:09,560] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 661315
[2026-01-09 20:12:09,560] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 661316
[2026-01-09 20:12:09,994] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 661317
[2026-01-09 20:12:10,268] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 661318
[2026-01-09 20:12:10,541] [ERROR] [launch.py:321:sigkill_handler] ['/data3/jisu/miniconda3/envs/mfm-new/bin/python3.11', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/data3/jisu/LLaVA/visa_llava_instruct.json', '--image_folder', '/data3/jisu/MFM/datasets/ViSA', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bits', '4', '--bf16', 'False', '--fp16', 'True', '--output_dir', './checkpoints/llava-v1.5-7b-mfm-lora', '--resume_from_checkpoint', 'True', '--num_train_epochs', '3', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.0', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none'] exits with return code = -9
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 20:12:16,042] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:12:17,047] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=3,4,5,6: setting --include=localhost:3,4,5,6
[2026-01-09 20:12:17,104] [INFO] [runner.py:571:main] cmd = /data3/jisu/miniconda3/envs/mfm-new/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMywgNCwgNSwgNl19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero2.json --model_name_or_path liuhaotian/llava-v1.5-7b --version v1 --data_path /data3/jisu/LLaVA/visa_llava_instruct.json --image_folder /data3/jisu/MFM/datasets/ViSA --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bits 4 --bf16 False --fp16 True --output_dir ./checkpoints/llava-v1.5-7b-mfm-lora --resume_from_checkpoint True --num_train_epochs 3 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 1e-4 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 False --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to none
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 20:12:19,112] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:12:20,109] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [3, 4, 5, 6]}
[2026-01-09 20:12:20,109] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2026-01-09 20:12:20,109] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2026-01-09 20:12:20,109] [INFO] [launch.py:163:main] dist_world_size=4
[2026-01-09 20:12:20,109] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=3,4,5,6
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 20:12:23,339] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:12:23,385] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:12:23,397] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:12:23,398] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:12:24,089] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 20:12:24,339] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 20:12:24,339] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2026-01-09 20:12:24,367] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 20:12:24,378] [INFO] [comm.py:637:init_distributed] cdb=None
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [01:03<01:03, 63.19s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:03<01:03, 63.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:04<01:04, 64.01s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:04<01:04, 64.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:47<00:00, 51.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:47<00:00, 53.73s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:47<00:00, 52.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:47<00:00, 53.92s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:47<00:00, 52.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:47<00:00, 53.72s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:47<00:00, 51.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:47<00:00, 53.63s/it]
Adding LoRA adapters...
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Formatting inputs...Skip in lazy mode
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Traceback (most recent call last):
  File "/data3/jisu/LLaVA/llava/train/train_mem.py", line 5, in <module>
    train()
  File "/data3/jisu/LLaVA/llava/train/train.py", line 968, in train
    trainer.train(resume_from_checkpoint=True)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1708, in _inner_training_loop
    deepspeed_load_checkpoint(self.model_wrapped, resume_from_checkpoint)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/integrations/deepspeed.py", line 402, in deepspeed_load_checkpoint
    load_path, _ = deepspeed_engine.load_checkpoint(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2724, in load_checkpoint
    load_path, client_states = self._load_checkpoint(load_dir,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2794, in _load_checkpoint
    self.load_module_state_dict(checkpoint=checkpoint,
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2587, in load_module_state_dict
    self.module.load_state_dict(
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2152, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	Missing key(s) in state_dict: "base_model.model.model.embed_tokens.weight", "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.0.input_layernorm.weight", "base_model.model.model.layers.0.post_attention_layernorm.weight", "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.1.input_layernorm.weight", "base_model.model.model.layers.1.post_attention_layernorm.weight", "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.2.input_layernorm.weight", "base_model.model.model.layers.2.post_attention_layernorm.weight", "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.3.input_layernorm.weight", "base_model.model.model.layers.3.post_attention_layernorm.weight", "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.4.input_layernorm.weight", "base_model.model.model.layers.4.post_attention_layernorm.weight", "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.5.input_layernorm.weight", "base_model.model.model.layers.5.post_attention_layernorm.weight", "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.6.input_layernorm.weight", "base_model.model.model.layers.6.post_attention_layernorm.weight", "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.7.input_layernorm.weight", "base_model.model.model.layers.7.post_attention_layernorm.weight", "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.8.input_layernorm.weight", "base_model.model.model.layers.8.post_attention_layernorm.weight", "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.9.input_layernorm.weight", "base_model.model.model.layers.9.post_attention_layernorm.weight", "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.10.input_layernorm.weight", "base_model.model.model.layers.10.post_attention_layernorm.weight", "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.11.input_layernorm.weight", "base_model.model.model.layers.11.post_attention_layernorm.weight", "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.12.input_layernorm.weight", "base_model.model.model.layers.12.post_attention_layernorm.weight", "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.13.input_layernorm.weight", "base_model.model.model.layers.13.post_attention_layernorm.weight", "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.14.input_layernorm.weight", "base_model.model.model.layers.14.post_attention_layernorm.weight", "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.15.input_layernorm.weight", "base_model.model.model.layers.15.post_attention_layernorm.weight", "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.16.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.16.input_layernorm.weight", "base_model.model.model.layers.16.post_attention_layernorm.weight", "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.17.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.17.input_layernorm.weight", "base_model.model.model.layers.17.post_attention_layernorm.weight", "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.18.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.18.input_layernorm.weight", "base_model.model.model.layers.18.post_attention_layernorm.weight", "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.19.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.19.input_layernorm.weight", "base_model.model.model.layers.19.post_attention_layernorm.weight", "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.20.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.20.input_layernorm.weight", "base_model.model.model.layers.20.post_attention_layernorm.weight", "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.21.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.21.input_layernorm.weight", "base_model.model.model.layers.21.post_attention_layernorm.weight", "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.22.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.22.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.22.input_layernorm.weight", "base_model.model.model.layers.22.post_attention_layernorm.weight", "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.23.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.23.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.23.input_layernorm.weight", "base_model.model.model.layers.23.post_attention_layernorm.weight", "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.24.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.24.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.24.input_layernorm.weight", "base_model.model.model.layers.24.post_attention_layernorm.weight", "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.25.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.25.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.25.input_layernorm.weight", "base_model.model.model.layers.25.post_attention_layernorm.weight", "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.26.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.26.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.26.input_layernorm.weight", "base_model.model.model.layers.26.post_attention_layernorm.weight", "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.27.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.27.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.27.input_layernorm.weight", "base_model.model.model.layers.27.post_attention_layernorm.weight", "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.28.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.28.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.28.input_layernorm.weight", "base_model.model.model.layers.28.post_attention_layernorm.weight", "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.29.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.29.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.29.input_layernorm.weight", "base_model.model.model.layers.29.post_attention_layernorm.weight", "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.30.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.30.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.30.input_layernorm.weight", "base_model.model.model.layers.30.post_attention_layernorm.weight", "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.31.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.31.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.31.input_layernorm.weight", "base_model.model.model.layers.31.post_attention_layernorm.weight", "base_model.model.model.norm.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.embeddings.class_embedding", "base_model.model.model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.post_layernorm.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.post_layernorm.bias", "base_model.model.lm_head.weight". 
Traceback (most recent call last):
  File "/data3/jisu/LLaVA/llava/train/train_mem.py", line 5, in <module>
    train()
  File "/data3/jisu/LLaVA/llava/train/train.py", line 968, in train
    trainer.train(resume_from_checkpoint=True)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1708, in _inner_training_loop
    deepspeed_load_checkpoint(self.model_wrapped, resume_from_checkpoint)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/integrations/deepspeed.py", line 402, in deepspeed_load_checkpoint
    load_path, _ = deepspeed_engine.load_checkpoint(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2724, in load_checkpoint
    load_path, client_states = self._load_checkpoint(load_dir,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2794, in _load_checkpoint
    self.load_module_state_dict(checkpoint=checkpoint,
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2587, in load_module_state_dict
    self.module.load_state_dict(
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2152, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	Missing key(s) in state_dict: "base_model.model.model.embed_tokens.weight", "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.0.input_layernorm.weight", "base_model.model.model.layers.0.post_attention_layernorm.weight", "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.1.input_layernorm.weight", "base_model.model.model.layers.1.post_attention_layernorm.weight", "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.2.input_layernorm.weight", "base_model.model.model.layers.2.post_attention_layernorm.weight", "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.3.input_layernorm.weight", "base_model.model.model.layers.3.post_attention_layernorm.weight", "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.4.input_layernorm.weight", "base_model.model.model.layers.4.post_attention_layernorm.weight", "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.5.input_layernorm.weight", "base_model.model.model.layers.5.post_attention_layernorm.weight", "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.6.input_layernorm.weight", "base_model.model.model.layers.6.post_attention_layernorm.weight", "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.7.input_layernorm.weight", "base_model.model.model.layers.7.post_attention_layernorm.weight", "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.8.input_layernorm.weight", "base_model.model.model.layers.8.post_attention_layernorm.weight", "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.9.input_layernorm.weight", "base_model.model.model.layers.9.post_attention_layernorm.weight", "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.10.input_layernorm.weight", "base_model.model.model.layers.10.post_attention_layernorm.weight", "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.11.input_layernorm.weight", "base_model.model.model.layers.11.post_attention_layernorm.weight", "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.12.input_layernorm.weight", "base_model.model.model.layers.12.post_attention_layernorm.weight", "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.13.input_layernorm.weight", "base_model.model.model.layers.13.post_attention_layernorm.weight", "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.14.input_layernorm.weight", "base_model.model.model.layers.14.post_attention_layernorm.weight", "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.15.input_layernorm.weight", "base_model.model.model.layers.15.post_attention_layernorm.weight", "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.16.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.16.input_layernorm.weight", "base_model.model.model.layers.16.post_attention_layernorm.weight", "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.17.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.17.input_layernorm.weight", "base_model.model.model.layers.17.post_attention_layernorm.weight", "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.18.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.18.input_layernorm.weight", "base_model.model.model.layers.18.post_attention_layernorm.weight", "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.19.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.19.input_layernorm.weight", "base_model.model.model.layers.19.post_attention_layernorm.weight", "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.20.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.20.input_layernorm.weight", "base_model.model.model.layers.20.post_attention_layernorm.weight", "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.21.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.21.input_layernorm.weight", "base_model.model.model.layers.21.post_attention_layernorm.weight", "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.22.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.22.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.22.input_layernorm.weight", "base_model.model.model.layers.22.post_attention_layernorm.weight", "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.23.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.23.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.23.input_layernorm.weight", "base_model.model.model.layers.23.post_attention_layernorm.weight", "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.24.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.24.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.24.input_layernorm.weight", "base_model.model.model.layers.24.post_attention_layernorm.weight", "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.25.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.25.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.25.input_layernorm.weight", "base_model.model.model.layers.25.post_attention_layernorm.weight", "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.26.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.26.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.26.input_layernorm.weight", "base_model.model.model.layers.26.post_attention_layernorm.weight", "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.27.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.27.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.27.input_layernorm.weight", "base_model.model.model.layers.27.post_attention_layernorm.weight", "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.28.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.28.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.28.input_layernorm.weight", "base_model.model.model.layers.28.post_attention_layernorm.weight", "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.29.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.29.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.29.input_layernorm.weight", "base_model.model.model.layers.29.post_attention_layernorm.weight", "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.30.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.30.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.30.input_layernorm.weight", "base_model.model.model.layers.30.post_attention_layernorm.weight", "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.31.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.31.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.31.input_layernorm.weight", "base_model.model.model.layers.31.post_attention_layernorm.weight", "base_model.model.model.norm.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.embeddings.class_embedding", "base_model.model.model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.post_layernorm.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.post_layernorm.bias", "base_model.model.lm_head.weight". 
Traceback (most recent call last):
  File "/data3/jisu/LLaVA/llava/train/train_mem.py", line 5, in <module>
    train()
  File "/data3/jisu/LLaVA/llava/train/train.py", line 968, in train
    trainer.train(resume_from_checkpoint=True)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1708, in _inner_training_loop
    deepspeed_load_checkpoint(self.model_wrapped, resume_from_checkpoint)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/integrations/deepspeed.py", line 402, in deepspeed_load_checkpoint
    load_path, _ = deepspeed_engine.load_checkpoint(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2724, in load_checkpoint
    load_path, client_states = self._load_checkpoint(load_dir,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2794, in _load_checkpoint
    self.load_module_state_dict(checkpoint=checkpoint,
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2587, in load_module_state_dict
    self.module.load_state_dict(
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2152, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	Missing key(s) in state_dict: "base_model.model.model.embed_tokens.weight", "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.0.input_layernorm.weight", "base_model.model.model.layers.0.post_attention_layernorm.weight", "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.1.input_layernorm.weight", "base_model.model.model.layers.1.post_attention_layernorm.weight", "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.2.input_layernorm.weight", "base_model.model.model.layers.2.post_attention_layernorm.weight", "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.3.input_layernorm.weight", "base_model.model.model.layers.3.post_attention_layernorm.weight", "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.4.input_layernorm.weight", "base_model.model.model.layers.4.post_attention_layernorm.weight", "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.5.input_layernorm.weight", "base_model.model.model.layers.5.post_attention_layernorm.weight", "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.6.input_layernorm.weight", "base_model.model.model.layers.6.post_attention_layernorm.weight", "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.7.input_layernorm.weight", "base_model.model.model.layers.7.post_attention_layernorm.weight", "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.8.input_layernorm.weight", "base_model.model.model.layers.8.post_attention_layernorm.weight", "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.9.input_layernorm.weight", "base_model.model.model.layers.9.post_attention_layernorm.weight", "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.10.input_layernorm.weight", "base_model.model.model.layers.10.post_attention_layernorm.weight", "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.11.input_layernorm.weight", "base_model.model.model.layers.11.post_attention_layernorm.weight", "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.12.input_layernorm.weight", "base_model.model.model.layers.12.post_attention_layernorm.weight", "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.13.input_layernorm.weight", "base_model.model.model.layers.13.post_attention_layernorm.weight", "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.14.input_layernorm.weight", "base_model.model.model.layers.14.post_attention_layernorm.weight", "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.15.input_layernorm.weight", "base_model.model.model.layers.15.post_attention_layernorm.weight", "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.16.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.16.input_layernorm.weight", "base_model.model.model.layers.16.post_attention_layernorm.weight", "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.17.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.17.input_layernorm.weight", "base_model.model.model.layers.17.post_attention_layernorm.weight", "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.18.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.18.input_layernorm.weight", "base_model.model.model.layers.18.post_attention_layernorm.weight", "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.19.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.19.input_layernorm.weight", "base_model.model.model.layers.19.post_attention_layernorm.weight", "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.20.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.20.input_layernorm.weight", "base_model.model.model.layers.20.post_attention_layernorm.weight", "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.21.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.21.input_layernorm.weight", "base_model.model.model.layers.21.post_attention_layernorm.weight", "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.22.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.22.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.22.input_layernorm.weight", "base_model.model.model.layers.22.post_attention_layernorm.weight", "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.23.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.23.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.23.input_layernorm.weight", "base_model.model.model.layers.23.post_attention_layernorm.weight", "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.24.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.24.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.24.input_layernorm.weight", "base_model.model.model.layers.24.post_attention_layernorm.weight", "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.25.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.25.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.25.input_layernorm.weight", "base_model.model.model.layers.25.post_attention_layernorm.weight", "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.26.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.26.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.26.input_layernorm.weight", "base_model.model.model.layers.26.post_attention_layernorm.weight", "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.27.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.27.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.27.input_layernorm.weight", "base_model.model.model.layers.27.post_attention_layernorm.weight", "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.28.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.28.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.28.input_layernorm.weight", "base_model.model.model.layers.28.post_attention_layernorm.weight", "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.29.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.29.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.29.input_layernorm.weight", "base_model.model.model.layers.29.post_attention_layernorm.weight", "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.30.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.30.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.30.input_layernorm.weight", "base_model.model.model.layers.30.post_attention_layernorm.weight", "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.31.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.31.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.31.input_layernorm.weight", "base_model.model.model.layers.31.post_attention_layernorm.weight", "base_model.model.model.norm.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.embeddings.class_embedding", "base_model.model.model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.post_layernorm.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.post_layernorm.bias", "base_model.model.lm_head.weight". 
Traceback (most recent call last):
  File "/data3/jisu/LLaVA/llava/train/train_mem.py", line 5, in <module>
    train()
  File "/data3/jisu/LLaVA/llava/train/train.py", line 968, in train
    trainer.train(resume_from_checkpoint=True)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/trainer.py", line 1708, in _inner_training_loop
    deepspeed_load_checkpoint(self.model_wrapped, resume_from_checkpoint)
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/transformers/integrations/deepspeed.py", line 402, in deepspeed_load_checkpoint
    load_path, _ = deepspeed_engine.load_checkpoint(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2724, in load_checkpoint
    load_path, client_states = self._load_checkpoint(load_dir,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2794, in _load_checkpoint
    self.load_module_state_dict(checkpoint=checkpoint,
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2587, in load_module_state_dict
    self.module.load_state_dict(
  File "/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2152, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	Missing key(s) in state_dict: "base_model.model.model.embed_tokens.weight", "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.0.input_layernorm.weight", "base_model.model.model.layers.0.post_attention_layernorm.weight", "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.1.input_layernorm.weight", "base_model.model.model.layers.1.post_attention_layernorm.weight", "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.2.input_layernorm.weight", "base_model.model.model.layers.2.post_attention_layernorm.weight", "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.3.input_layernorm.weight", "base_model.model.model.layers.3.post_attention_layernorm.weight", "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.4.input_layernorm.weight", "base_model.model.model.layers.4.post_attention_layernorm.weight", "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.5.input_layernorm.weight", "base_model.model.model.layers.5.post_attention_layernorm.weight", "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.6.input_layernorm.weight", "base_model.model.model.layers.6.post_attention_layernorm.weight", "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.7.input_layernorm.weight", "base_model.model.model.layers.7.post_attention_layernorm.weight", "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.8.input_layernorm.weight", "base_model.model.model.layers.8.post_attention_layernorm.weight", "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.9.input_layernorm.weight", "base_model.model.model.layers.9.post_attention_layernorm.weight", "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.10.input_layernorm.weight", "base_model.model.model.layers.10.post_attention_layernorm.weight", "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.11.input_layernorm.weight", "base_model.model.model.layers.11.post_attention_layernorm.weight", "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.12.input_layernorm.weight", "base_model.model.model.layers.12.post_attention_layernorm.weight", "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.13.input_layernorm.weight", "base_model.model.model.layers.13.post_attention_layernorm.weight", "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.14.input_layernorm.weight", "base_model.model.model.layers.14.post_attention_layernorm.weight", "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.15.input_layernorm.weight", "base_model.model.model.layers.15.post_attention_layernorm.weight", "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.16.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.16.input_layernorm.weight", "base_model.model.model.layers.16.post_attention_layernorm.weight", "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.17.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.17.input_layernorm.weight", "base_model.model.model.layers.17.post_attention_layernorm.weight", "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.18.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.18.input_layernorm.weight", "base_model.model.model.layers.18.post_attention_layernorm.weight", "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.19.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.19.input_layernorm.weight", "base_model.model.model.layers.19.post_attention_layernorm.weight", "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.20.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.20.input_layernorm.weight", "base_model.model.model.layers.20.post_attention_layernorm.weight", "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.21.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.21.input_layernorm.weight", "base_model.model.model.layers.21.post_attention_layernorm.weight", "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.22.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.22.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.22.input_layernorm.weight", "base_model.model.model.layers.22.post_attention_layernorm.weight", "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.23.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.23.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.23.input_layernorm.weight", "base_model.model.model.layers.23.post_attention_layernorm.weight", "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.24.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.24.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.24.input_layernorm.weight", "base_model.model.model.layers.24.post_attention_layernorm.weight", "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.25.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.25.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.25.input_layernorm.weight", "base_model.model.model.layers.25.post_attention_layernorm.weight", "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.26.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.26.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.26.input_layernorm.weight", "base_model.model.model.layers.26.post_attention_layernorm.weight", "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.27.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.27.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.27.input_layernorm.weight", "base_model.model.model.layers.27.post_attention_layernorm.weight", "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.28.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.28.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.28.input_layernorm.weight", "base_model.model.model.layers.28.post_attention_layernorm.weight", "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.29.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.29.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.29.input_layernorm.weight", "base_model.model.model.layers.29.post_attention_layernorm.weight", "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.30.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.30.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.30.input_layernorm.weight", "base_model.model.model.layers.30.post_attention_layernorm.weight", "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight", "base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight", "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight", "base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight", "base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight", "base_model.model.model.layers.31.mlp.up_proj.base_layer.weight", "base_model.model.model.layers.31.mlp.down_proj.base_layer.weight", "base_model.model.model.layers.31.input_layernorm.weight", "base_model.model.model.layers.31.post_attention_layernorm.weight", "base_model.model.model.norm.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.embeddings.class_embedding", "base_model.model.model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias", "base_model.model.model.vision_tower.vision_tower.vision_model.post_layernorm.weight", "base_model.model.model.vision_tower.vision_tower.vision_model.post_layernorm.bias", "base_model.model.lm_head.weight". 
[2026-01-09 20:14:58,138] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 662097
[2026-01-09 20:14:58,157] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 662098
[2026-01-09 20:14:58,158] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 662099
[2026-01-09 20:14:58,168] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 662100
[2026-01-09 20:14:58,176] [ERROR] [launch.py:321:sigkill_handler] ['/data3/jisu/miniconda3/envs/mfm-new/bin/python3.11', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', '/data3/jisu/LLaVA/visa_llava_instruct.json', '--image_folder', '/data3/jisu/MFM/datasets/ViSA', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bits', '4', '--bf16', 'False', '--fp16', 'True', '--output_dir', './checkpoints/llava-v1.5-7b-mfm-lora', '--resume_from_checkpoint', 'True', '--num_train_epochs', '3', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '1e-4', '--weight_decay', '0.0', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none'] exits with return code = 1
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 20:24:14,828] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:24:15,803] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=3,4,5,6: setting --include=localhost:3,4,5,6
[2026-01-09 20:24:15,871] [INFO] [runner.py:571:main] cmd = /data3/jisu/miniconda3/envs/mfm-new/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMywgNCwgNSwgNl19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero2.json --model_name_or_path liuhaotian/llava-v1.5-7b --version v1 --data_path /data3/jisu/LLaVA/visa_llava_instruct.json --image_folder /data3/jisu/MFM/datasets/ViSA --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bits 4 --bf16 False --fp16 True --output_dir ./checkpoints/llava-v1.5-7b-mfm-lora --resume_from_checkpoint True --num_train_epochs 3 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 1e-4 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 False --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to none
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 20:24:17,798] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:24:18,776] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [3, 4, 5, 6]}
[2026-01-09 20:24:18,776] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2026-01-09 20:24:18,776] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2026-01-09 20:24:18,776] [INFO] [launch.py:163:main] dist_world_size=4
[2026-01-09 20:24:18,776] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=3,4,5,6
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-09 20:24:21,998] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:24:22,090] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:24:22,115] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:24:22,191] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-09 20:24:22,734] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 20:24:23,035] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 20:24:23,063] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 20:24:23,080] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-09 20:24:23,080] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.74s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.65s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.86s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.47s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.58s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.50s/it]
Adding LoRA adapters...
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Formatting inputs...Skip in lazy mode
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(

[LoRA Fix] ⚠️ 'PeftModelForCausalLM' 로딩 중 누락된 키 발생! (Frozen Base Model 무시함)
[LoRA Fix] strict=False 모드로 재시도합니다...


[LoRA Fix] ⚠️ 'PeftModelForCausalLM' 로딩 중 누락된 키 발생! (Frozen Base Model 무시함)
[LoRA Fix] strict=False 모드로 재시도합니다...


[LoRA Fix] ⚠️ 'PeftModelForCausalLM' 로딩 중 누락된 키 발생! (Frozen Base Model 무시함)
[LoRA Fix] strict=False 모드로 재시도합니다...


[LoRA Fix] ⚠️ 'PeftModelForCausalLM' 로딩 중 누락된 키 발생! (Frozen Base Model 무시함)
[LoRA Fix] strict=False 모드로 재시도합니다...

  0%|          | 0/507 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
 50%|████▉     | 251/507 [00:17<00:17, 14.64it/s]                                                 {'loss': 0.0267, 'learning_rate': 0.00010671322079237307, 'epoch': 1.48}
 50%|████▉     | 251/507 [00:17<00:17, 14.64it/s] 50%|████▉     | 251/507 [00:29<00:17, 14.64it/s] 50%|████▉     | 252/507 [00:32<00:39,  6.38it/s]                                                 {'loss': 0.0311, 'learning_rate': 0.00010607469557381899, 'epoch': 1.49}
 50%|████▉     | 252/507 [00:32<00:39,  6.38it/s] 50%|████▉     | 253/507 [00:48<01:11,  3.56it/s]                                                 {'loss': 0.0142, 'learning_rate': 0.00010543592166441983, 'epoch': 1.5}
 50%|████▉     | 253/507 [00:48<01:11,  3.56it/s] 50%|█████     | 254/507 [01:04<01:55,  2.19it/s]                                                 {'loss': 0.0157, 'learning_rate': 0.00010479692521482316, 'epoch': 1.5}
 50%|█████     | 254/507 [01:04<01:55,  2.19it/s] 50%|█████     | 255/507 [01:20<02:56,  1.43it/s]                                                 {'loss': 0.0421, 'learning_rate': 0.00010415773238478715, 'epoch': 1.51}
 50%|█████     | 255/507 [01:20<02:56,  1.43it/s] 50%|█████     | 256/507 [01:36<04:21,  1.04s/it]                                                 {'loss': 0.0135, 'learning_rate': 0.00010351836934210957, 'epoch': 1.51}
 50%|█████     | 256/507 [01:36<04:21,  1.04s/it] 51%|█████     | 257/507 [01:52<06:15,  1.50s/it]                                                 {'loss': 0.0202, 'learning_rate': 0.00010287886226155641, 'epoch': 1.52}
 51%|█████     | 257/507 [01:52<06:15,  1.50s/it] 51%|█████     | 258/507 [02:08<08:47,  2.12s/it]                                                 {'loss': 0.0133, 'learning_rate': 0.00010223923732379048, 'epoch': 1.53}
 51%|█████     | 258/507 [02:08<08:47,  2.12s/it] 51%|█████     | 259/507 [02:24<12:03,  2.92s/it]                                                 {'loss': 0.0296, 'learning_rate': 0.00010159952071429952, 'epoch': 1.53}
 51%|█████     | 259/507 [02:24<12:03,  2.92s/it] 51%|█████▏    | 260/507 [02:40<16:08,  3.92s/it]                                                 {'loss': 0.0335, 'learning_rate': 0.0001009597386223241, 'epoch': 1.54}
 51%|█████▏    | 260/507 [02:40<16:08,  3.92s/it] 51%|█████▏    | 261/507 [02:56<20:57,  5.11s/it]                                                 {'loss': 0.031, 'learning_rate': 0.00010031991723978574, 'epoch': 1.54}
 51%|█████▏    | 261/507 [02:56<20:57,  5.11s/it] 52%|█████▏    | 262/507 [03:13<26:26,  6.48s/it]                                                 {'loss': 0.0271, 'learning_rate': 9.96800827602143e-05, 'epoch': 1.55}
 52%|█████▏    | 262/507 [03:13<26:26,  6.48s/it] 52%|█████▏    | 263/507 [03:29<32:13,  7.92s/it]                                                 {'loss': 0.0157, 'learning_rate': 9.90402613776759e-05, 'epoch': 1.56}
 52%|█████▏    | 263/507 [03:29<32:13,  7.92s/it] 52%|█████▏    | 264/507 [03:45<38:04,  9.40s/it]                                                 {'loss': 0.0234, 'learning_rate': 9.84004792857005e-05, 'epoch': 1.56}
 52%|█████▏    | 264/507 [03:45<38:04,  9.40s/it] 52%|█████▏    | 265/507 [04:02<43:32, 10.79s/it]                                                 {'loss': 0.025, 'learning_rate': 9.776076267620955e-05, 'epoch': 1.57}
 52%|█████▏    | 265/507 [04:02<43:32, 10.79s/it] 52%|█████▏    | 266/507 [04:18<48:26, 12.06s/it]                                                 {'loss': 0.0228, 'learning_rate': 9.712113773844361e-05, 'epoch': 1.57}
 52%|█████▏    | 266/507 [04:18<48:26, 12.06s/it] 53%|█████▎    | 267/507 [04:35<52:25, 13.11s/it]                                                 {'loss': 0.0336, 'learning_rate': 9.648163065789045e-05, 'epoch': 1.58}
 53%|█████▎    | 267/507 [04:35<52:25, 13.11s/it] 53%|█████▎    | 268/507 [04:51<55:39, 13.97s/it]                                                 {'loss': 0.0176, 'learning_rate': 9.584226761521285e-05, 'epoch': 1.58}
 53%|█████▎    | 268/507 [04:51<55:39, 13.97s/it] 53%|█████▎    | 269/507 [05:08<58:04, 14.64s/it]                                                 {'loss': 0.0142, 'learning_rate': 9.520307478517686e-05, 'epoch': 1.59}
 53%|█████▎    | 269/507 [05:08<58:04, 14.64s/it] 53%|█████▎    | 270/507 [05:24<59:51, 15.16s/it]                                                 {'loss': 0.0174, 'learning_rate': 9.456407833558018e-05, 'epoch': 1.6}
 53%|█████▎    | 270/507 [05:24<59:51, 15.16s/it] 53%|█████▎    | 271/507 [05:41<1:01:08, 15.54s/it]                                                   {'loss': 0.0146, 'learning_rate': 9.3925304426181e-05, 'epoch': 1.6}
 53%|█████▎    | 271/507 [05:41<1:01:08, 15.54s/it] 54%|█████▎    | 272/507 [05:57<1:01:58, 15.82s/it]                                                   {'loss': 0.0109, 'learning_rate': 9.328677920762697e-05, 'epoch': 1.61}
 54%|█████▎    | 272/507 [05:57<1:01:58, 15.82s/it] 54%|█████▍    | 273/507 [06:14<1:02:29, 16.02s/it]                                                   {'loss': 0.0132, 'learning_rate': 9.264852882038453e-05, 'epoch': 1.61}
 54%|█████▍    | 273/507 [06:14<1:02:29, 16.02s/it] 54%|█████▍    | 274/507 [06:30<1:02:39, 16.13s/it]                                                   {'loss': 0.0215, 'learning_rate': 9.201057939366896e-05, 'epoch': 1.62}
 54%|█████▍    | 274/507 [06:30<1:02:39, 16.13s/it] 54%|█████▍    | 275/507 [06:47<1:02:49, 16.25s/it]                                                   {'loss': 0.0233, 'learning_rate': 9.13729570443745e-05, 'epoch': 1.63}
 54%|█████▍    | 275/507 [06:47<1:02:49, 16.25s/it] 54%|█████▍    | 276/507 [07:03<1:02:51, 16.33s/it]                                                   {'loss': 0.0325, 'learning_rate': 9.073568787600539e-05, 'epoch': 1.63}
 54%|█████▍    | 276/507 [07:03<1:02:51, 16.33s/it] 55%|█████▍    | 277/507 [07:20<1:02:49, 16.39s/it]                                                   {'loss': 0.023, 'learning_rate': 9.009879797760678e-05, 'epoch': 1.64}
 55%|█████▍    | 277/507 [07:20<1:02:49, 16.39s/it] 55%|█████▍    | 278/507 [07:36<1:02:32, 16.38s/it]                                                   {'loss': 0.022, 'learning_rate': 8.946231342269718e-05, 'epoch': 1.64}
 55%|█████▍    | 278/507 [07:36<1:02:32, 16.38s/it] 55%|█████▌    | 279/507 [07:52<1:02:17, 16.39s/it]                                                   {'loss': 0.0493, 'learning_rate': 8.882626026820078e-05, 'epoch': 1.65}
 55%|█████▌    | 279/507 [07:52<1:02:17, 16.39s/it] 55%|█████▌    | 280/507 [08:09<1:02:10, 16.43s/it]                                                   {'loss': 0.02, 'learning_rate': 8.819066455338066e-05, 'epoch': 1.66}
 55%|█████▌    | 280/507 [08:09<1:02:10, 16.43s/it] 55%|█████▌    | 281/507 [08:26<1:02:02, 16.47s/it]                                                   {'loss': 0.0225, 'learning_rate': 8.755555229877294e-05, 'epoch': 1.66}
 55%|█████▌    | 281/507 [08:26<1:02:02, 16.47s/it] 56%|█████▌    | 282/507 [08:42<1:01:40, 16.45s/it]                                                   {'loss': 0.0475, 'learning_rate': 8.692094950512145e-05, 'epoch': 1.67}
 56%|█████▌    | 282/507 [08:42<1:01:40, 16.45s/it] 56%|█████▌    | 283/507 [08:58<1:01:29, 16.47s/it]                                                   {'loss': 0.0097, 'learning_rate': 8.62868821523133e-05, 'epoch': 1.67}
 56%|█████▌    | 283/507 [08:58<1:01:29, 16.47s/it] 56%|█████▌    | 284/507 [09:15<1:01:16, 16.48s/it]                                                   {'loss': 0.0165, 'learning_rate': 8.565337619831516e-05, 'epoch': 1.68}
 56%|█████▌    | 284/507 [09:15<1:01:16, 16.48s/it] 56%|█████▌    | 285/507 [09:31<1:01:02, 16.50s/it]                                                   {'loss': 0.0045, 'learning_rate': 8.502045757811085e-05, 'epoch': 1.69}
 56%|█████▌    | 285/507 [09:31<1:01:02, 16.50s/it] 56%|█████▋    | 286/507 [09:48<1:00:47, 16.51s/it]                                                   {'loss': 0.0226, 'learning_rate': 8.438815220263941e-05, 'epoch': 1.69}
 56%|█████▋    | 286/507 [09:48<1:00:47, 16.51s/it] 57%|█████▋    | 287/507 [10:04<1:00:28, 16.49s/it]                                                   {'loss': 0.0381, 'learning_rate': 8.37564859577343e-05, 'epoch': 1.7}
 57%|█████▋    | 287/507 [10:04<1:00:28, 16.49s/it] 57%|█████▋    | 288/507 [10:21<1:00:16, 16.51s/it]                                                   {'loss': 0.0432, 'learning_rate': 8.312548470306378e-05, 'epoch': 1.7}
 57%|█████▋    | 288/507 [10:21<1:00:16, 16.51s/it] 57%|█████▋    | 289/507 [10:38<1:00:01, 16.52s/it]                                                   {'loss': 0.0415, 'learning_rate': 8.249517427107225e-05, 'epoch': 1.71}
 57%|█████▋    | 289/507 [10:38<1:00:01, 16.52s/it] 57%|█████▋    | 290/507 [10:54<59:43, 16.52s/it]                                                   {'loss': 0.0241, 'learning_rate': 8.186558046592247e-05, 'epoch': 1.71}
 57%|█████▋    | 290/507 [10:54<59:43, 16.52s/it] 57%|█████▋    | 291/507 [11:11<59:27, 16.52s/it]                                                 {'loss': 0.0144, 'learning_rate': 8.123672906243955e-05, 'epoch': 1.72}
 57%|█████▋    | 291/507 [11:11<59:27, 16.52s/it] 58%|█████▊    | 292/507 [11:27<59:13, 16.53s/it]                                                 {'loss': 0.027, 'learning_rate': 8.060864580505543e-05, 'epoch': 1.73}
 58%|█████▊    | 292/507 [11:27<59:13, 16.53s/it] 58%|█████▊    | 293/507 [11:44<58:54, 16.51s/it]                                                 {'loss': 0.0306, 'learning_rate': 7.998135640675513e-05, 'epoch': 1.73}
 58%|█████▊    | 293/507 [11:44<58:54, 16.51s/it] 58%|█████▊    | 294/507 [12:00<58:37, 16.52s/it]                                                 {'loss': 0.0125, 'learning_rate': 7.935488654802394e-05, 'epoch': 1.74}
 58%|█████▊    | 294/507 [12:00<58:37, 16.52s/it] 58%|█████▊    | 295/507 [12:17<58:17, 16.50s/it]                                                 {'loss': 0.0233, 'learning_rate': 7.872926187579626e-05, 'epoch': 1.74}
 58%|█████▊    | 295/507 [12:17<58:17, 16.50s/it] 58%|█████▊    | 296/507 [12:33<57:52, 16.46s/it]                                                 {'loss': 0.0125, 'learning_rate': 7.810450800240549e-05, 'epoch': 1.75}
 58%|█████▊    | 296/507 [12:33<57:52, 16.46s/it] 59%|█████▊    | 297/507 [12:50<57:40, 16.48s/it]                                                 {'loss': 0.0286, 'learning_rate': 7.748065050453557e-05, 'epoch': 1.76}
 59%|█████▊    | 297/507 [12:50<57:40, 16.48s/it] 59%|█████▉    | 298/507 [13:06<57:27, 16.50s/it]                                                 {'loss': 0.0079, 'learning_rate': 7.685771492217386e-05, 'epoch': 1.76}
 59%|█████▉    | 298/507 [13:06<57:27, 16.50s/it] 59%|█████▉    | 299/507 [13:22<57:03, 16.46s/it]                                                 {'loss': 0.0363, 'learning_rate': 7.623572675756569e-05, 'epoch': 1.77}
 59%|█████▉    | 299/507 [13:22<57:03, 16.46s/it] 59%|█████▉    | 300/507 [13:39<56:51, 16.48s/it]                                                 {'loss': 0.0313, 'learning_rate': 7.561471147417016e-05, 'epoch': 1.77}
 59%|█████▉    | 300/507 [13:39<56:51, 16.48s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 59%|█████▉    | 301/507 [14:03<1:04:37, 18.82s/it]                                                   {'loss': 0.038, 'learning_rate': 7.499469449561769e-05, 'epoch': 1.78}
 59%|█████▉    | 301/507 [14:03<1:04:37, 18.82s/it] 60%|█████▉    | 302/507 [14:20<1:01:53, 18.12s/it]                                                   {'loss': 0.0239, 'learning_rate': 7.437570120466942e-05, 'epoch': 1.79}
 60%|█████▉    | 302/507 [14:20<1:01:53, 18.12s/it] 60%|█████▉    | 303/507 [14:36<1:00:02, 17.66s/it]                                                   {'loss': 0.0316, 'learning_rate': 7.375775694217787e-05, 'epoch': 1.79}
 60%|█████▉    | 303/507 [14:36<1:00:02, 17.66s/it] 60%|█████▉    | 304/507 [14:53<58:34, 17.31s/it]                                                   {'loss': 0.0232, 'learning_rate': 7.314088700604958e-05, 'epoch': 1.8}
 60%|█████▉    | 304/507 [14:53<58:34, 17.31s/it] 60%|██████    | 305/507 [15:09<57:29, 17.08s/it]                                                 {'loss': 0.019, 'learning_rate': 7.252511665020939e-05, 'epoch': 1.8}
 60%|██████    | 305/507 [15:09<57:29, 17.08s/it] 60%|██████    | 306/507 [15:26<56:37, 16.90s/it]                                                 {'loss': 0.013, 'learning_rate': 7.191047108356672e-05, 'epoch': 1.81}
 60%|██████    | 306/507 [15:26<56:37, 16.90s/it] 61%|██████    | 307/507 [15:42<55:42, 16.71s/it]                                                 {'loss': 0.0327, 'learning_rate': 7.129697546898344e-05, 'epoch': 1.82}
 61%|██████    | 307/507 [15:42<55:42, 16.71s/it] 61%|██████    | 308/507 [15:59<55:09, 16.63s/it]                                                 {'loss': 0.0126, 'learning_rate': 7.068465492224361e-05, 'epoch': 1.82}
 61%|██████    | 308/507 [15:59<55:09, 16.63s/it] 61%|██████    | 309/507 [16:15<54:46, 16.60s/it]                                                 {'loss': 0.0242, 'learning_rate': 7.007353451102556e-05, 'epoch': 1.83}
 61%|██████    | 309/507 [16:15<54:46, 16.60s/it] 61%|██████    | 310/507 [16:32<54:25, 16.57s/it]                                                 {'loss': 0.0259, 'learning_rate': 6.946363925387546e-05, 'epoch': 1.83}
 61%|██████    | 310/507 [16:32<54:25, 16.57s/it] 61%|██████▏   | 311/507 [16:48<54:05, 16.56s/it]                                                 {'loss': 0.0111, 'learning_rate': 6.885499411918304e-05, 'epoch': 1.84}
 61%|██████▏   | 311/507 [16:48<54:05, 16.56s/it] 62%|██████▏   | 312/507 [17:05<53:45, 16.54s/it]                                                 {'loss': 0.0192, 'learning_rate': 6.824762402415957e-05, 'epoch': 1.84}
 62%|██████▏   | 312/507 [17:05<53:45, 16.54s/it] 62%|██████▏   | 313/507 [17:21<53:22, 16.51s/it]                                                 {'loss': 0.021, 'learning_rate': 6.764155383381771e-05, 'epoch': 1.85}
 62%|██████▏   | 313/507 [17:21<53:22, 16.51s/it] 62%|██████▏   | 314/507 [17:37<53:01, 16.49s/it]                                                 {'loss': 0.0423, 'learning_rate': 6.703680835995359e-05, 'epoch': 1.86}
 62%|██████▏   | 314/507 [17:37<53:01, 16.49s/it] 62%|██████▏   | 315/507 [17:54<52:37, 16.44s/it]                                                 {'loss': 0.0294, 'learning_rate': 6.643341236013086e-05, 'epoch': 1.86}
 62%|██████▏   | 315/507 [17:54<52:37, 16.44s/it] 62%|██████▏   | 316/507 [18:10<52:25, 16.47s/it]                                                 {'loss': 0.0191, 'learning_rate': 6.583139053666745e-05, 'epoch': 1.87}
 62%|██████▏   | 316/507 [18:10<52:25, 16.47s/it] 63%|██████▎   | 317/507 [18:27<52:11, 16.48s/it]                                                 {'loss': 0.0132, 'learning_rate': 6.523076753562411e-05, 'epoch': 1.87}
 63%|██████▎   | 317/507 [18:27<52:11, 16.48s/it] 63%|██████▎   | 318/507 [18:43<51:56, 16.49s/it]                                                 {'loss': 0.0233, 'learning_rate': 6.463156794579544e-05, 'epoch': 1.88}
 63%|██████▎   | 318/507 [18:43<51:56, 16.49s/it] 63%|██████▎   | 319/507 [19:00<51:40, 16.49s/it]                                                 {'loss': 0.0269, 'learning_rate': 6.403381629770325e-05, 'epoch': 1.89}
 63%|██████▎   | 319/507 [19:00<51:40, 16.49s/it] 63%|██████▎   | 320/507 [19:16<51:11, 16.42s/it]                                                 {'loss': 0.0528, 'learning_rate': 6.343753706259239e-05, 'epoch': 1.89}
 63%|██████▎   | 320/507 [19:16<51:11, 16.42s/it] 63%|██████▎   | 321/507 [19:33<50:52, 16.41s/it]                                                 {'loss': 0.0398, 'learning_rate': 6.284275465142874e-05, 'epoch': 1.9}
 63%|██████▎   | 321/507 [19:33<50:52, 16.41s/it] 64%|██████▎   | 322/507 [19:49<50:37, 16.42s/it]                                                 {'loss': 0.0133, 'learning_rate': 6.224949341390016e-05, 'epoch': 1.9}
 64%|██████▎   | 322/507 [19:49<50:37, 16.42s/it] 64%|██████▎   | 323/507 [20:05<50:24, 16.44s/it]                                                 {'loss': 0.0445, 'learning_rate': 6.165777763741931e-05, 'epoch': 1.91}
 64%|██████▎   | 323/507 [20:05<50:24, 16.44s/it] 64%|██████▍   | 324/507 [20:22<50:05, 16.42s/it]                                                 {'loss': 0.0225, 'learning_rate': 6.106763154612962e-05, 'epoch': 1.92}
 64%|██████▍   | 324/507 [20:22<50:05, 16.42s/it] 64%|██████▍   | 325/507 [20:38<49:54, 16.45s/it]                                                 {'loss': 0.0305, 'learning_rate': 6.047907929991333e-05, 'epoch': 1.92}
 64%|██████▍   | 325/507 [20:38<49:54, 16.45s/it] 64%|██████▍   | 326/507 [20:55<49:42, 16.48s/it]                                                 {'loss': 0.0233, 'learning_rate': 5.989214499340267e-05, 'epoch': 1.93}
 64%|██████▍   | 326/507 [20:55<49:42, 16.48s/it] 64%|██████▍   | 327/507 [21:11<49:27, 16.49s/it]                                                 {'loss': 0.0197, 'learning_rate': 5.9306852654993294e-05, 'epoch': 1.93}
 64%|██████▍   | 327/507 [21:11<49:27, 16.49s/it] 65%|██████▍   | 328/507 [21:28<49:04, 16.45s/it]                                                 {'loss': 0.0176, 'learning_rate': 5.872322624586061e-05, 'epoch': 1.94}
 65%|██████▍   | 328/507 [21:28<49:04, 16.45s/it] 65%|██████▍   | 329/507 [21:44<48:51, 16.47s/it]                                                 {'loss': 0.0148, 'learning_rate': 5.814128965897887e-05, 'epoch': 1.95}
 65%|██████▍   | 329/507 [21:44<48:51, 16.47s/it] 65%|██████▌   | 330/507 [22:01<48:36, 16.48s/it]                                                 {'loss': 0.0495, 'learning_rate': 5.756106671814301e-05, 'epoch': 1.95}
 65%|██████▌   | 330/507 [22:01<48:36, 16.48s/it] 65%|██████▌   | 331/507 [22:17<48:22, 16.49s/it]                                                 {'loss': 0.0314, 'learning_rate': 5.6982581176993335e-05, 'epoch': 1.96}
 65%|██████▌   | 331/507 [22:17<48:22, 16.49s/it] 65%|██████▌   | 332/507 [22:34<48:01, 16.47s/it]                                                 {'loss': 0.0257, 'learning_rate': 5.640585671804296e-05, 'epoch': 1.96}
 65%|██████▌   | 332/507 [22:34<48:01, 16.47s/it] 66%|██████▌   | 333/507 [22:50<47:48, 16.49s/it]                                                 {'loss': 0.0155, 'learning_rate': 5.5830916951708565e-05, 'epoch': 1.97}
 66%|██████▌   | 333/507 [22:50<47:48, 16.49s/it] 66%|██████▌   | 334/507 [23:07<47:33, 16.50s/it]                                                 {'loss': 0.0108, 'learning_rate': 5.52577854153435e-05, 'epoch': 1.97}
 66%|██████▌   | 334/507 [23:07<47:33, 16.50s/it] 66%|██████▌   | 335/507 [23:23<47:18, 16.50s/it]                                                 {'loss': 0.0299, 'learning_rate': 5.4686485572274336e-05, 'epoch': 1.98}
 66%|██████▌   | 335/507 [23:23<47:18, 16.50s/it] 66%|██████▋   | 336/507 [23:40<47:03, 16.51s/it]                                                 {'loss': 0.0286, 'learning_rate': 5.4117040810840246e-05, 'epoch': 1.99}
 66%|██████▋   | 336/507 [23:40<47:03, 16.51s/it] 66%|██████▋   | 337/507 [23:56<46:46, 16.51s/it]                                                 {'loss': 0.0127, 'learning_rate': 5.354947444343572e-05, 'epoch': 1.99}
 66%|██████▋   | 337/507 [23:56<46:46, 16.51s/it] 67%|██████▋   | 338/507 [24:13<46:26, 16.49s/it]                                                 {'loss': 0.0198, 'learning_rate': 5.298380970555584e-05, 'epoch': 2.0}
 67%|██████▋   | 338/507 [24:13<46:26, 16.49s/it] 67%|██████▋   | 339/507 [24:30<46:41, 16.68s/it]                                                 {'loss': 0.0182, 'learning_rate': 5.242006975484528e-05, 'epoch': 2.0}
 67%|██████▋   | 339/507 [24:30<46:41, 16.68s/it] 67%|██████▋   | 340/507 [24:46<46:09, 16.58s/it]                                                 {'loss': 0.006, 'learning_rate': 5.1858277670150304e-05, 'epoch': 2.01}
 67%|██████▋   | 340/507 [24:46<46:09, 16.58s/it] 67%|██████▋   | 341/507 [25:03<45:42, 16.52s/it]                                                 {'loss': 0.0344, 'learning_rate': 5.129845645057372e-05, 'epoch': 2.02}
 67%|██████▋   | 341/507 [25:03<45:42, 16.52s/it] 67%|██████▋   | 342/507 [25:19<45:25, 16.52s/it]                                                 {'loss': 0.0261, 'learning_rate': 5.074062901453351e-05, 'epoch': 2.02}
 67%|██████▋   | 342/507 [25:19<45:25, 16.52s/it] 68%|██████▊   | 343/507 [25:36<45:08, 16.52s/it]                                                 {'loss': 0.0091, 'learning_rate': 5.0184818198824454e-05, 'epoch': 2.03}
 68%|██████▊   | 343/507 [25:36<45:08, 16.52s/it] 68%|██████▊   | 344/507 [25:52<44:52, 16.52s/it]                                                 {'loss': 0.0188, 'learning_rate': 4.963104675768345e-05, 'epoch': 2.03}
 68%|██████▊   | 344/507 [25:52<44:52, 16.52s/it] 68%|██████▊   | 345/507 [26:09<44:35, 16.52s/it]                                                 {'loss': 0.0067, 'learning_rate': 4.907933736185757e-05, 'epoch': 2.04}
 68%|██████▊   | 345/507 [26:09<44:35, 16.52s/it] 68%|██████▊   | 346/507 [26:25<44:18, 16.51s/it]                                                 {'loss': 0.0099, 'learning_rate': 4.8529712597676426e-05, 'epoch': 2.05}
 68%|██████▊   | 346/507 [26:25<44:18, 16.51s/it] 68%|██████▊   | 347/507 [26:42<44:00, 16.50s/it]                                                 {'loss': 0.0281, 'learning_rate': 4.79821949661271e-05, 'epoch': 2.05}
 68%|██████▊   | 347/507 [26:42<44:00, 16.50s/it] 69%|██████▊   | 348/507 [26:58<43:43, 16.50s/it]                                                 {'loss': 0.0085, 'learning_rate': 4.74368068819333e-05, 'epoch': 2.06}
 69%|██████▊   | 348/507 [26:58<43:43, 16.50s/it] 69%|██████▉   | 349/507 [27:15<43:25, 16.49s/it]                                                 {'loss': 0.0122, 'learning_rate': 4.689357067263751e-05, 'epoch': 2.06}
 69%|██████▉   | 349/507 [27:15<43:25, 16.49s/it] 69%|██████▉   | 350/507 [27:31<43:09, 16.50s/it]                                                 {'loss': 0.0067, 'learning_rate': 4.635250857768696e-05, 'epoch': 2.07}
 69%|██████▉   | 350/507 [27:31<43:09, 16.50s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 69%|██████▉   | 351/507 [27:55<48:36, 18.69s/it]                                                 {'loss': 0.029, 'learning_rate': 4.581364274752338e-05, 'epoch': 2.08}
 69%|██████▉   | 351/507 [27:55<48:36, 18.69s/it] 69%|██████▉   | 352/507 [28:11<46:33, 18.02s/it]                                                 {'loss': 0.0117, 'learning_rate': 4.527699524267576e-05, 'epoch': 2.08}
 69%|██████▉   | 352/507 [28:11<46:33, 18.02s/it] 70%|██████▉   | 353/507 [28:28<44:57, 17.52s/it]                                                 {'loss': 0.0354, 'learning_rate': 4.474258803285774e-05, 'epoch': 2.09}
 70%|██████▉   | 353/507 [28:28<44:57, 17.52s/it] 70%|██████▉   | 354/507 [28:44<43:54, 17.22s/it]                                                 {'loss': 0.0227, 'learning_rate': 4.4210442996067724e-05, 'epoch': 2.09}
 70%|██████▉   | 354/507 [28:44<43:54, 17.22s/it] 70%|███████   | 355/507 [29:01<43:01, 16.98s/it]                                                 {'loss': 0.0164, 'learning_rate': 4.368058191769363e-05, 'epoch': 2.1}
 70%|███████   | 355/507 [29:01<43:01, 16.98s/it] 70%|███████   | 356/507 [29:17<42:26, 16.86s/it]                                                 {'loss': 0.0102, 'learning_rate': 4.315302648962066e-05, 'epoch': 2.1}
 70%|███████   | 356/507 [29:17<42:26, 16.86s/it] 70%|███████   | 357/507 [29:34<41:53, 16.76s/it]                                                 {'loss': 0.0143, 'learning_rate': 4.262779830934346e-05, 'epoch': 2.11}
 70%|███████   | 357/507 [29:34<41:53, 16.76s/it] 71%|███████   | 358/507 [29:50<41:25, 16.68s/it]                                                 {'loss': 0.0189, 'learning_rate': 4.210491887908201e-05, 'epoch': 2.12}
 71%|███████   | 358/507 [29:50<41:25, 16.68s/it] 71%|███████   | 359/507 [30:07<41:01, 16.63s/it]                                                 {'loss': 0.0245, 'learning_rate': 4.158440960490103e-05, 'epoch': 2.12}
 71%|███████   | 359/507 [30:07<41:01, 16.63s/it] 71%|███████   | 360/507 [30:23<40:39, 16.60s/it]                                                 {'loss': 0.0255, 'learning_rate': 4.1066291795834114e-05, 'epoch': 2.13}
 71%|███████   | 360/507 [30:23<40:39, 16.60s/it] 71%|███████   | 361/507 [30:40<40:20, 16.58s/it]                                                 {'loss': 0.0049, 'learning_rate': 4.055058666301087e-05, 'epoch': 2.13}
 71%|███████   | 361/507 [30:40<40:20, 16.58s/it] 71%|███████▏  | 362/507 [30:56<39:58, 16.54s/it]                                                 {'loss': 0.0117, 'learning_rate': 4.003731531878899e-05, 'epoch': 2.14}
 71%|███████▏  | 362/507 [30:56<39:58, 16.54s/it] 72%|███████▏  | 363/507 [31:13<39:40, 16.53s/it]                                                 {'loss': 0.0231, 'learning_rate': 3.952649877588964e-05, 'epoch': 2.15}
 72%|███████▏  | 363/507 [31:13<39:40, 16.53s/it] 72%|███████▏  | 364/507 [31:29<39:24, 16.53s/it]                                                 {'loss': 0.0239, 'learning_rate': 3.901815794653729e-05, 'epoch': 2.15}
 72%|███████▏  | 364/507 [31:29<39:24, 16.53s/it] 72%|███████▏  | 365/507 [31:46<39:07, 16.53s/it]                                                 {'loss': 0.0217, 'learning_rate': 3.851231364160379e-05, 'epoch': 2.16}
 72%|███████▏  | 365/507 [31:46<39:07, 16.53s/it] 72%|███████▏  | 366/507 [32:02<38:48, 16.51s/it]                                                 {'loss': 0.0165, 'learning_rate': 3.800898656975599e-05, 'epoch': 2.16}
 72%|███████▏  | 366/507 [32:02<38:48, 16.51s/it] 72%|███████▏  | 367/507 [32:19<38:32, 16.52s/it]                                                 {'loss': 0.0145, 'learning_rate': 3.750819733660844e-05, 'epoch': 2.17}
 72%|███████▏  | 367/507 [32:19<38:32, 16.52s/it] 73%|███████▎  | 368/507 [32:35<38:15, 16.51s/it]                                                 {'loss': 0.0099, 'learning_rate': 3.700996644387944e-05, 'epoch': 2.18}
 73%|███████▎  | 368/507 [32:35<38:15, 16.51s/it] 73%|███████▎  | 369/507 [32:52<37:59, 16.52s/it]                                                 {'loss': 0.0092, 'learning_rate': 3.651431428855188e-05, 'epoch': 2.18}
 73%|███████▎  | 369/507 [32:52<37:59, 16.52s/it] 73%|███████▎  | 370/507 [33:08<37:42, 16.51s/it]                                                 {'loss': 0.0041, 'learning_rate': 3.602126116203819e-05, 'epoch': 2.19}
 73%|███████▎  | 370/507 [33:08<37:42, 16.51s/it] 73%|███████▎  | 371/507 [33:25<37:26, 16.52s/it]                                                 {'loss': 0.0039, 'learning_rate': 3.553082724934973e-05, 'epoch': 2.19}
 73%|███████▎  | 371/507 [33:25<37:26, 16.52s/it] 73%|███████▎  | 372/507 [33:41<37:07, 16.50s/it]                                                 {'loss': 0.0057, 'learning_rate': 3.504303262827022e-05, 'epoch': 2.2}
 73%|███████▎  | 372/507 [33:41<37:07, 16.50s/it] 74%|███████▎  | 373/507 [33:58<36:49, 16.49s/it]                                                 {'loss': 0.0231, 'learning_rate': 3.4557897268533935e-05, 'epoch': 2.21}
 74%|███████▎  | 373/507 [33:58<36:49, 16.49s/it] 74%|███████▍  | 374/507 [34:14<36:30, 16.47s/it]                                                 {'loss': 0.0227, 'learning_rate': 3.407544103100824e-05, 'epoch': 2.21}
 74%|███████▍  | 374/507 [34:14<36:30, 16.47s/it] 74%|███████▍  | 375/507 [34:31<36:16, 16.49s/it]                                                 {'loss': 0.0289, 'learning_rate': 3.359568366688028e-05, 'epoch': 2.22}
 74%|███████▍  | 375/507 [34:31<36:16, 16.49s/it] 74%|███████▍  | 376/507 [34:47<36:00, 16.50s/it]                                                 {'loss': 0.012, 'learning_rate': 3.3118644816848574e-05, 'epoch': 2.22}
 74%|███████▍  | 376/507 [34:47<36:00, 16.50s/it] 74%|███████▍  | 377/507 [35:04<35:45, 16.50s/it]                                                 {'loss': 0.0039, 'learning_rate': 3.264434401031887e-05, 'epoch': 2.23}
 74%|███████▍  | 377/507 [35:04<35:45, 16.50s/it] 75%|███████▍  | 378/507 [35:20<35:27, 16.49s/it]                                                 {'loss': 0.0261, 'learning_rate': 3.217280066460472e-05, 'epoch': 2.23}
 75%|███████▍  | 378/507 [35:20<35:27, 16.49s/it] 75%|███████▍  | 379/507 [35:37<35:12, 16.51s/it]                                                 {'loss': 0.0196, 'learning_rate': 3.170403408413243e-05, 'epoch': 2.24}
 75%|███████▍  | 379/507 [35:37<35:12, 16.51s/it] 75%|███████▍  | 380/507 [35:53<34:56, 16.51s/it]                                                 {'loss': 0.0063, 'learning_rate': 3.1238063459650805e-05, 'epoch': 2.25}
 75%|███████▍  | 380/507 [35:53<34:56, 16.51s/it] 75%|███████▌  | 381/507 [36:10<34:40, 16.51s/it]                                                 {'loss': 0.0087, 'learning_rate': 3.077490786744562e-05, 'epoch': 2.25}
 75%|███████▌  | 381/507 [36:10<34:40, 16.51s/it] 75%|███████▌  | 382/507 [36:26<34:21, 16.49s/it]                                                 {'loss': 0.0253, 'learning_rate': 3.031458626855849e-05, 'epoch': 2.26}
 75%|███████▌  | 382/507 [36:26<34:21, 16.49s/it] 76%|███████▌  | 383/507 [36:43<34:05, 16.49s/it]                                                 {'loss': 0.0102, 'learning_rate': 2.985711750801068e-05, 'epoch': 2.26}
 76%|███████▌  | 383/507 [36:43<34:05, 16.49s/it] 76%|███████▌  | 384/507 [36:59<33:49, 16.50s/it]                                                 {'loss': 0.0273, 'learning_rate': 2.9402520314031644e-05, 'epoch': 2.27}
 76%|███████▌  | 384/507 [36:59<33:49, 16.50s/it] 76%|███████▌  | 385/507 [37:16<33:30, 16.48s/it]                                                 {'loss': 0.0137, 'learning_rate': 2.895081329729239e-05, 'epoch': 2.28}
 76%|███████▌  | 385/507 [37:16<33:30, 16.48s/it] 76%|███████▌  | 386/507 [37:32<33:10, 16.45s/it]                                                 {'loss': 0.0151, 'learning_rate': 2.8502014950143373e-05, 'epoch': 2.28}
 76%|███████▌  | 386/507 [37:32<33:10, 16.45s/it] 76%|███████▋  | 387/507 [37:49<32:56, 16.47s/it]                                                 {'loss': 0.0126, 'learning_rate': 2.805614364585758e-05, 'epoch': 2.29}
 76%|███████▋  | 387/507 [37:49<32:56, 16.47s/it] 77%|███████▋  | 388/507 [38:05<32:42, 16.49s/it]                                                 {'loss': 0.01, 'learning_rate': 2.7613217637878407e-05, 'epoch': 2.29}
 77%|███████▋  | 388/507 [38:05<32:42, 16.49s/it] 77%|███████▋  | 389/507 [38:22<32:26, 16.49s/it]                                                 {'loss': 0.0147, 'learning_rate': 2.7173255059072233e-05, 'epoch': 2.3}
 77%|███████▋  | 389/507 [38:22<32:26, 16.49s/it] 77%|███████▋  | 390/507 [38:38<32:11, 16.51s/it]                                                 {'loss': 0.0231, 'learning_rate': 2.6736273920986167e-05, 'epoch': 2.31}
 77%|███████▋  | 390/507 [38:38<32:11, 16.51s/it] 77%|███████▋  | 391/507 [38:55<31:55, 16.51s/it]                                                 {'loss': 0.0112, 'learning_rate': 2.6302292113110637e-05, 'epoch': 2.31}
 77%|███████▋  | 391/507 [38:55<31:55, 16.51s/it] 77%|███████▋  | 392/507 [39:11<31:36, 16.49s/it]                                                 {'loss': 0.0191, 'learning_rate': 2.5871327402147172e-05, 'epoch': 2.32}
 77%|███████▋  | 392/507 [39:11<31:36, 16.49s/it] 78%|███████▊  | 393/507 [39:28<31:21, 16.50s/it]                                                 {'loss': 0.0258, 'learning_rate': 2.5443397431280702e-05, 'epoch': 2.32}
 78%|███████▊  | 393/507 [39:28<31:21, 16.50s/it] 78%|███████▊  | 394/507 [39:44<31:04, 16.50s/it]                                                 {'loss': 0.0036, 'learning_rate': 2.5018519719457723e-05, 'epoch': 2.33}
 78%|███████▊  | 394/507 [39:44<31:04, 16.50s/it] 78%|███████▊  | 395/507 [40:01<30:45, 16.48s/it]                                                 {'loss': 0.0154, 'learning_rate': 2.4596711660668692e-05, 'epoch': 2.34}
 78%|███████▊  | 395/507 [40:01<30:45, 16.48s/it] 78%|███████▊  | 396/507 [40:17<30:29, 16.49s/it]                                                 {'loss': 0.0031, 'learning_rate': 2.4177990523236216e-05, 'epoch': 2.34}
 78%|███████▊  | 396/507 [40:17<30:29, 16.49s/it] 78%|███████▊  | 397/507 [40:34<30:14, 16.50s/it]                                                 {'loss': 0.004, 'learning_rate': 2.3762373449107932e-05, 'epoch': 2.35}
 78%|███████▊  | 397/507 [40:34<30:14, 16.50s/it] 79%|███████▊  | 398/507 [40:50<29:55, 16.47s/it]                                                 {'loss': 0.0062, 'learning_rate': 2.334987745315478e-05, 'epoch': 2.35}
 79%|███████▊  | 398/507 [40:50<29:55, 16.47s/it] 79%|███████▊  | 399/507 [41:07<29:40, 16.48s/it]                                                 {'loss': 0.0131, 'learning_rate': 2.2940519422474573e-05, 'epoch': 2.36}
 79%|███████▊  | 399/507 [41:07<29:40, 16.48s/it] 79%|███████▉  | 400/507 [41:23<29:24, 16.49s/it]                                                 {'loss': 0.011, 'learning_rate': 2.253431611570035e-05, 'epoch': 2.36}
 79%|███████▉  | 400/507 [41:23<29:24, 16.49s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 79%|███████▉  | 401/507 [41:46<32:26, 18.36s/it]                                                 {'loss': 0.0032, 'learning_rate': 2.213128416231468e-05, 'epoch': 2.37}
 79%|███████▉  | 401/507 [41:46<32:26, 18.36s/it] 79%|███████▉  | 402/507 [42:02<31:09, 17.81s/it]                                                 {'loss': 0.0079, 'learning_rate': 2.1731440061968533e-05, 'epoch': 2.38}
 79%|███████▉  | 402/507 [42:02<31:09, 17.81s/it] 79%|███████▉  | 403/507 [42:19<30:10, 17.41s/it]                                                 {'loss': 0.0092, 'learning_rate': 2.133480018380608e-05, 'epoch': 2.38}
 79%|███████▉  | 403/507 [42:19<30:10, 17.41s/it] 80%|███████▉  | 404/507 [42:35<29:25, 17.14s/it]                                                 {'loss': 0.0129, 'learning_rate': 2.0941380765794327e-05, 'epoch': 2.39}
 80%|███████▉  | 404/507 [42:35<29:25, 17.14s/it] 80%|███████▉  | 405/507 [42:52<28:48, 16.95s/it]                                                 {'loss': 0.0105, 'learning_rate': 2.0551197914058464e-05, 'epoch': 2.39}
 80%|███████▉  | 405/507 [42:52<28:48, 16.95s/it] 80%|████████  | 406/507 [43:08<28:18, 16.82s/it]                                                 {'loss': 0.0089, 'learning_rate': 2.0164267602222586e-05, 'epoch': 2.4}
 80%|████████  | 406/507 [43:08<28:18, 16.82s/it] 80%|████████  | 407/507 [43:25<27:53, 16.73s/it]                                                 {'loss': 0.0044, 'learning_rate': 1.978060567075547e-05, 'epoch': 2.41}
 80%|████████  | 407/507 [43:25<27:53, 16.73s/it] 80%|████████  | 408/507 [43:41<27:29, 16.66s/it]                                                 {'loss': 0.02, 'learning_rate': 1.940022782632248e-05, 'epoch': 2.41}
 80%|████████  | 408/507 [43:41<27:29, 16.66s/it] 81%|████████  | 409/507 [43:58<27:03, 16.57s/it]                                                 {'loss': 0.0192, 'learning_rate': 1.902314964114219e-05, 'epoch': 2.42}
 81%|████████  | 409/507 [43:58<27:03, 16.57s/it] 81%|████████  | 410/507 [44:14<26:39, 16.49s/it]                                                 {'loss': 0.0221, 'learning_rate': 1.8649386552349134e-05, 'epoch': 2.42}
 81%|████████  | 410/507 [44:14<26:39, 16.49s/it] 81%|████████  | 411/507 [44:31<26:22, 16.49s/it]                                                 {'loss': 0.0324, 'learning_rate': 1.827895386136166e-05, 'epoch': 2.43}
 81%|████████  | 411/507 [44:31<26:22, 16.49s/it] 81%|████████▏ | 412/507 [44:47<26:07, 16.50s/it]                                                 {'loss': 0.0074, 'learning_rate': 1.7911866733255556e-05, 'epoch': 2.44}
 81%|████████▏ | 412/507 [44:47<26:07, 16.50s/it] 81%|████████▏ | 413/507 [45:04<25:52, 16.51s/it]                                                 {'loss': 0.0032, 'learning_rate': 1.7548140196143335e-05, 'epoch': 2.44}
 81%|████████▏ | 413/507 [45:04<25:52, 16.51s/it] 82%|████████▏ | 414/507 [45:20<25:33, 16.49s/it]                                                 {'loss': 0.0436, 'learning_rate': 1.718778914055873e-05, 'epoch': 2.45}
 82%|████████▏ | 414/507 [45:20<25:33, 16.49s/it] 82%|████████▏ | 415/507 [45:37<25:18, 16.50s/it]                                                 {'loss': 0.0198, 'learning_rate': 1.6830828318847414e-05, 'epoch': 2.45}
 82%|████████▏ | 415/507 [45:37<25:18, 16.50s/it] 82%|████████▏ | 416/507 [45:53<25:02, 16.51s/it]                                                 {'loss': 0.0092, 'learning_rate': 1.647727234456279e-05, 'epoch': 2.46}
 82%|████████▏ | 416/507 [45:53<25:02, 16.51s/it] 82%|████████▏ | 417/507 [46:10<24:42, 16.48s/it]                                                 {'loss': 0.0076, 'learning_rate': 1.6127135691867945e-05, 'epoch': 2.47}
 82%|████████▏ | 417/507 [46:10<24:42, 16.48s/it] 82%|████████▏ | 418/507 [46:26<24:27, 16.49s/it]                                                 {'loss': 0.0241, 'learning_rate': 1.5780432694942815e-05, 'epoch': 2.47}
 82%|████████▏ | 418/507 [46:26<24:27, 16.49s/it] 83%|████████▎ | 419/507 [46:43<24:12, 16.51s/it]                                                 {'loss': 0.017, 'learning_rate': 1.543717754739774e-05, 'epoch': 2.48}
 83%|████████▎ | 419/507 [46:43<24:12, 16.51s/it] 83%|████████▎ | 420/507 [46:59<23:54, 16.49s/it]                                                 {'loss': 0.0147, 'learning_rate': 1.5097384301692041e-05, 'epoch': 2.48}
 83%|████████▎ | 420/507 [46:59<23:54, 16.49s/it] 83%|████████▎ | 421/507 [47:16<23:38, 16.50s/it]                                                 {'loss': 0.0074, 'learning_rate': 1.4761066868558914e-05, 'epoch': 2.49}
 83%|████████▎ | 421/507 [47:16<23:38, 16.50s/it] 83%|████████▎ | 422/507 [47:32<23:23, 16.51s/it]                                                 {'loss': 0.0059, 'learning_rate': 1.4428239016435951e-05, 'epoch': 2.49}
 83%|████████▎ | 422/507 [47:32<23:23, 16.51s/it] 83%|████████▎ | 423/507 [47:48<23:03, 16.47s/it]                                                 {'loss': 0.0152, 'learning_rate': 1.4098914370901384e-05, 'epoch': 2.5}
 83%|████████▎ | 423/507 [47:48<23:03, 16.47s/it] 84%|████████▎ | 424/507 [48:05<22:46, 16.46s/it]                                                 {'loss': 0.0218, 'learning_rate': 1.3773106414116299e-05, 'epoch': 2.51}
 84%|████████▎ | 424/507 [48:05<22:46, 16.46s/it] 84%|████████▍ | 425/507 [48:21<22:31, 16.48s/it]                                                 {'loss': 0.019, 'learning_rate': 1.3450828484272726e-05, 'epoch': 2.51}
 84%|████████▍ | 425/507 [48:21<22:31, 16.48s/it] 84%|████████▍ | 426/507 [48:38<22:14, 16.47s/it]                                                 {'loss': 0.0094, 'learning_rate': 1.3132093775047615e-05, 'epoch': 2.52}
 84%|████████▍ | 426/507 [48:38<22:14, 16.47s/it] 84%|████████▍ | 427/507 [48:54<21:56, 16.45s/it]                                                 {'loss': 0.013, 'learning_rate': 1.2816915335062595e-05, 'epoch': 2.52}
 84%|████████▍ | 427/507 [48:54<21:56, 16.45s/it] 84%|████████▍ | 428/507 [49:11<21:39, 16.45s/it]                                                 {'loss': 0.0094, 'learning_rate': 1.2505306067349853e-05, 'epoch': 2.53}
 84%|████████▍ | 428/507 [49:11<21:39, 16.45s/it] 85%|████████▍ | 429/507 [49:27<21:24, 16.47s/it]                                                 {'loss': 0.0134, 'learning_rate': 1.2197278728823947e-05, 'epoch': 2.54}
 85%|████████▍ | 429/507 [49:27<21:24, 16.47s/it] 85%|████████▍ | 430/507 [49:44<21:09, 16.49s/it]                                                 {'loss': 0.012, 'learning_rate': 1.1892845929759412e-05, 'epoch': 2.54}
 85%|████████▍ | 430/507 [49:44<21:09, 16.49s/it] 85%|████████▌ | 431/507 [50:00<20:52, 16.48s/it]                                                 {'loss': 0.0091, 'learning_rate': 1.1592020133274639e-05, 'epoch': 2.55}
 85%|████████▌ | 431/507 [50:00<20:52, 16.48s/it] 85%|████████▌ | 432/507 [50:17<20:34, 16.46s/it]                                                 {'loss': 0.0033, 'learning_rate': 1.129481365482159e-05, 'epoch': 2.55}
 85%|████████▌ | 432/507 [50:17<20:34, 16.46s/it] 85%|████████▌ | 433/507 [50:33<20:19, 16.49s/it]                                                 {'loss': 0.0029, 'learning_rate': 1.1001238661681657e-05, 'epoch': 2.56}
 85%|████████▌ | 433/507 [50:33<20:19, 16.49s/it] 86%|████████▌ | 434/507 [50:50<20:04, 16.50s/it]                                                 {'loss': 0.023, 'learning_rate': 1.07113071724675e-05, 'epoch': 2.57}
 86%|████████▌ | 434/507 [50:50<20:04, 16.50s/it] 86%|████████▌ | 435/507 [51:06<19:42, 16.43s/it]                                                 {'loss': 0.023, 'learning_rate': 1.0425031056631007e-05, 'epoch': 2.57}
 86%|████████▌ | 435/507 [51:06<19:42, 16.43s/it] 86%|████████▌ | 436/507 [51:23<19:28, 16.46s/it]                                                 {'loss': 0.0226, 'learning_rate': 1.0142422033977505e-05, 'epoch': 2.58}
 86%|████████▌ | 436/507 [51:23<19:28, 16.46s/it] 86%|████████▌ | 437/507 [51:39<19:11, 16.46s/it]                                                 {'loss': 0.0032, 'learning_rate': 9.863491674185776e-06, 'epoch': 2.58}
 86%|████████▌ | 437/507 [51:39<19:11, 16.46s/it] 86%|████████▋ | 438/507 [51:55<18:56, 16.47s/it]                                                 {'loss': 0.0115, 'learning_rate': 9.588251396334524e-06, 'epoch': 2.59}
 86%|████████▋ | 438/507 [51:55<18:56, 16.47s/it] 87%|████████▋ | 439/507 [52:12<18:40, 16.48s/it]                                                 {'loss': 0.0042, 'learning_rate': 9.316712468434874e-06, 'epoch': 2.6}
 87%|████████▋ | 439/507 [52:12<18:40, 16.48s/it] 87%|████████▋ | 440/507 [52:29<18:25, 16.49s/it]                                                 {'loss': 0.0064, 'learning_rate': 9.048886006969093e-06, 'epoch': 2.6}
 87%|████████▋ | 440/507 [52:29<18:25, 16.49s/it] 87%|████████▋ | 441/507 [52:45<18:07, 16.47s/it]                                                 {'loss': 0.0105, 'learning_rate': 8.784782976435424e-06, 'epoch': 2.61}
 87%|████████▋ | 441/507 [52:45<18:07, 16.47s/it] 87%|████████▋ | 442/507 [53:01<17:51, 16.49s/it]                                                 {'loss': 0.0108, 'learning_rate': 8.524414188899266e-06, 'epoch': 2.61}
 87%|████████▋ | 442/507 [53:01<17:51, 16.49s/it] 87%|████████▋ | 443/507 [53:18<17:36, 16.51s/it]                                                 {'loss': 0.0195, 'learning_rate': 8.267790303550526e-06, 'epoch': 2.62}
 87%|████████▋ | 443/507 [53:18<17:36, 16.51s/it] 88%|████████▊ | 444/507 [53:35<17:19, 16.51s/it]                                                 {'loss': 0.0024, 'learning_rate': 8.014921826267285e-06, 'epoch': 2.62}
 88%|████████▊ | 444/507 [53:35<17:19, 16.51s/it] 88%|████████▊ | 445/507 [53:51<17:03, 16.51s/it]                                                 {'loss': 0.0125, 'learning_rate': 7.765819109185635e-06, 'epoch': 2.63}
 88%|████████▊ | 445/507 [53:51<17:03, 16.51s/it] 88%|████████▊ | 446/507 [54:07<16:45, 16.49s/it]                                                 {'loss': 0.0216, 'learning_rate': 7.520492350275876e-06, 'epoch': 2.64}
 88%|████████▊ | 446/507 [54:07<16:45, 16.49s/it] 88%|████████▊ | 447/507 [54:24<16:29, 16.50s/it]                                                 {'loss': 0.0165, 'learning_rate': 7.278951592925154e-06, 'epoch': 2.64}
 88%|████████▊ | 447/507 [54:24<16:29, 16.50s/it] 88%|████████▊ | 448/507 [54:41<16:13, 16.50s/it]                                                 {'loss': 0.012, 'learning_rate': 7.041206725526028e-06, 'epoch': 2.65}
 88%|████████▊ | 448/507 [54:41<16:13, 16.50s/it] 89%|████████▊ | 449/507 [54:57<15:57, 16.50s/it]                                                 {'loss': 0.0149, 'learning_rate': 6.807267481071966e-06, 'epoch': 2.65}
 89%|████████▊ | 449/507 [54:57<15:57, 16.50s/it] 89%|████████▉ | 450/507 [55:14<15:40, 16.51s/it]                                                 {'loss': 0.0166, 'learning_rate': 6.577143436758659e-06, 'epoch': 2.66}
 89%|████████▉ | 450/507 [55:14<15:40, 16.51s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 89%|████████▉ | 451/507 [55:37<17:22, 18.62s/it]                                                 {'loss': 0.0161, 'learning_rate': 6.350844013592061e-06, 'epoch': 2.67}
 89%|████████▉ | 451/507 [55:37<17:22, 18.62s/it] 89%|████████▉ | 452/507 [55:53<16:25, 17.93s/it]                                                 {'loss': 0.0254, 'learning_rate': 6.1283784760026494e-06, 'epoch': 2.67}
 89%|████████▉ | 452/507 [55:53<16:25, 17.93s/it] 89%|████████▉ | 453/507 [56:10<15:45, 17.50s/it]                                                 {'loss': 0.0323, 'learning_rate': 5.9097559314661214e-06, 'epoch': 2.68}
 89%|████████▉ | 453/507 [56:10<15:45, 17.50s/it] 90%|████████▉ | 454/507 [56:26<15:10, 17.18s/it]                                                 {'loss': 0.008, 'learning_rate': 5.694985330130698e-06, 'epoch': 2.68}
 90%|████████▉ | 454/507 [56:26<15:10, 17.18s/it] 90%|████████▉ | 455/507 [56:43<14:42, 16.98s/it]                                                 {'loss': 0.0071, 'learning_rate': 5.484075464450455e-06, 'epoch': 2.69}
 90%|████████▉ | 455/507 [56:43<14:42, 16.98s/it] 90%|████████▉ | 456/507 [56:59<14:18, 16.84s/it]                                                 {'loss': 0.0022, 'learning_rate': 5.277034968825667e-06, 'epoch': 2.7}
 90%|████████▉ | 456/507 [56:59<14:18, 16.84s/it] 90%|█████████ | 457/507 [57:16<13:56, 16.73s/it]                                                 {'loss': 0.0247, 'learning_rate': 5.073872319249073e-06, 'epoch': 2.7}
 90%|█████████ | 457/507 [57:16<13:56, 16.73s/it] 90%|█████████ | 458/507 [57:32<13:36, 16.66s/it]                                                 {'loss': 0.0148, 'learning_rate': 4.8745958329590615e-06, 'epoch': 2.71}
 90%|█████████ | 458/507 [57:32<13:36, 16.66s/it] 91%|█████████ | 459/507 [57:49<13:17, 16.61s/it]                                                 {'loss': 0.0164, 'learning_rate': 4.679213668099036e-06, 'epoch': 2.71}
 91%|█████████ | 459/507 [57:49<13:17, 16.61s/it] 91%|█████████ | 460/507 [58:05<12:59, 16.59s/it]                                                 {'loss': 0.0304, 'learning_rate': 4.487733823383522e-06, 'epoch': 2.72}
 91%|█████████ | 460/507 [58:05<12:59, 16.59s/it] 91%|█████████ | 461/507 [58:22<12:42, 16.57s/it]                                                 {'loss': 0.0201, 'learning_rate': 4.3001641377707125e-06, 'epoch': 2.73}
 91%|█████████ | 461/507 [58:22<12:42, 16.57s/it] 91%|█████████ | 462/507 [58:38<12:24, 16.55s/it]                                                 {'loss': 0.0107, 'learning_rate': 4.116512290141405e-06, 'epoch': 2.73}
 91%|█████████ | 462/507 [58:38<12:24, 16.55s/it] 91%|█████████▏| 463/507 [58:55<12:07, 16.54s/it]                                                 {'loss': 0.0166, 'learning_rate': 3.936785798984877e-06, 'epoch': 2.74}
 91%|█████████▏| 463/507 [58:55<12:07, 16.54s/it] 92%|█████████▏| 464/507 [59:11<11:49, 16.49s/it]                                                 {'loss': 0.0216, 'learning_rate': 3.7609920220908813e-06, 'epoch': 2.74}
 92%|█████████▏| 464/507 [59:11<11:49, 16.49s/it] 92%|█████████▏| 465/507 [59:28<11:32, 16.50s/it]                                                 {'loss': 0.0059, 'learning_rate': 3.5891381562485504e-06, 'epoch': 2.75}
 92%|█████████▏| 465/507 [59:28<11:32, 16.50s/it] 92%|█████████▏| 466/507 [59:44<11:14, 16.46s/it]                                                 {'loss': 0.0132, 'learning_rate': 3.4212312369516497e-06, 'epoch': 2.75}
 92%|█████████▏| 466/507 [59:44<11:14, 16.46s/it] 92%|█████████▏| 467/507 [1:00:01<10:59, 16.48s/it]                                                   {'loss': 0.0204, 'learning_rate': 3.2572781381107197e-06, 'epoch': 2.76}
 92%|█████████▏| 467/507 [1:00:01<10:59, 16.48s/it] 92%|█████████▏| 468/507 [1:00:17<10:43, 16.49s/it]                                                   {'loss': 0.0065, 'learning_rate': 3.0972855717715134e-06, 'epoch': 2.77}
 92%|█████████▏| 468/507 [1:00:17<10:43, 16.49s/it] 93%|█████████▎| 469/507 [1:00:34<10:24, 16.45s/it]                                                   {'loss': 0.0227, 'learning_rate': 2.9412600878402697e-06, 'epoch': 2.77}
 93%|█████████▎| 469/507 [1:00:34<10:24, 16.45s/it] 93%|█████████▎| 470/507 [1:00:50<10:09, 16.47s/it]                                                   {'loss': 0.0282, 'learning_rate': 2.789208073815608e-06, 'epoch': 2.78}
 93%|█████████▎| 470/507 [1:00:50<10:09, 16.47s/it] 93%|█████████▎| 471/507 [1:01:07<09:53, 16.48s/it]                                                   {'loss': 0.0083, 'learning_rate': 2.6411357545269577e-06, 'epoch': 2.78}
 93%|█████████▎| 471/507 [1:01:07<09:53, 16.48s/it] 93%|█████████▎| 472/507 [1:01:23<09:37, 16.49s/it]                                                   {'loss': 0.0134, 'learning_rate': 2.4970491918797854e-06, 'epoch': 2.79}
 93%|█████████▎| 472/507 [1:01:23<09:37, 16.49s/it] 93%|█████████▎| 473/507 [1:01:40<09:20, 16.50s/it]                                                   {'loss': 0.0081, 'learning_rate': 2.35695428460736e-06, 'epoch': 2.8}
 93%|█████████▎| 473/507 [1:01:40<09:20, 16.50s/it] 93%|█████████▎| 474/507 [1:01:56<09:04, 16.50s/it]                                                   {'loss': 0.0177, 'learning_rate': 2.2208567680293667e-06, 'epoch': 2.8}
 93%|█████████▎| 474/507 [1:01:56<09:04, 16.50s/it] 94%|█████████▎| 475/507 [1:02:13<08:48, 16.50s/it]                                                   {'loss': 0.0143, 'learning_rate': 2.088762213816986e-06, 'epoch': 2.81}
 94%|█████████▎| 475/507 [1:02:13<08:48, 16.50s/it] 94%|█████████▍| 476/507 [1:02:29<08:31, 16.51s/it]                                                   {'loss': 0.0123, 'learning_rate': 1.960676029764874e-06, 'epoch': 2.81}
 94%|█████████▍| 476/507 [1:02:29<08:31, 16.51s/it] 94%|█████████▍| 477/507 [1:02:45<08:13, 16.46s/it]                                                   {'loss': 0.0196, 'learning_rate': 1.8366034595698078e-06, 'epoch': 2.82}
 94%|█████████▍| 477/507 [1:02:45<08:13, 16.46s/it] 94%|█████████▍| 478/507 [1:03:02<07:56, 16.44s/it]                                                   {'loss': 0.0208, 'learning_rate': 1.7165495826158896e-06, 'epoch': 2.83}
 94%|█████████▍| 478/507 [1:03:02<07:56, 16.44s/it] 94%|█████████▍| 479/507 [1:03:18<07:40, 16.46s/it]                                                   {'loss': 0.0034, 'learning_rate': 1.600519313766724e-06, 'epoch': 2.83}
 94%|█████████▍| 479/507 [1:03:18<07:40, 16.46s/it] 95%|█████████▍| 480/507 [1:03:35<07:25, 16.48s/it]                                                   {'loss': 0.006, 'learning_rate': 1.4885174031641469e-06, 'epoch': 2.84}
 95%|█████████▍| 480/507 [1:03:35<07:25, 16.48s/it] 95%|█████████▍| 481/507 [1:03:51<07:08, 16.50s/it]                                                   {'loss': 0.0083, 'learning_rate': 1.3805484360337906e-06, 'epoch': 2.84}
 95%|█████████▍| 481/507 [1:03:51<07:08, 16.50s/it] 95%|█████████▌| 482/507 [1:04:08<06:52, 16.50s/it]                                                   {'loss': 0.0129, 'learning_rate': 1.276616832497346e-06, 'epoch': 2.85}
 95%|█████████▌| 482/507 [1:04:08<06:52, 16.50s/it] 95%|█████████▌| 483/507 [1:04:24<06:35, 16.49s/it]                                                   {'loss': 0.0097, 'learning_rate': 1.1767268473916182e-06, 'epoch': 2.86}
 95%|█████████▌| 483/507 [1:04:24<06:35, 16.49s/it] 95%|█████████▌| 484/507 [1:04:41<06:18, 16.47s/it]                                                   {'loss': 0.0172, 'learning_rate': 1.0808825700943438e-06, 'epoch': 2.86}
 95%|█████████▌| 484/507 [1:04:41<06:18, 16.47s/it] 96%|█████████▌| 485/507 [1:04:57<06:02, 16.49s/it]                                                   {'loss': 0.0163, 'learning_rate': 9.890879243567686e-07, 'epoch': 2.87}
 96%|█████████▌| 485/507 [1:04:57<06:02, 16.49s/it] 96%|█████████▌| 486/507 [1:05:14<05:46, 16.49s/it]                                                   {'loss': 0.005, 'learning_rate': 9.013466681429994e-07, 'epoch': 2.87}
 96%|█████████▌| 486/507 [1:05:14<05:46, 16.49s/it] 96%|█████████▌| 487/507 [1:05:30<05:29, 16.50s/it]                                                   {'loss': 0.0161, 'learning_rate': 8.17662393476204e-07, 'epoch': 2.88}
 96%|█████████▌| 487/507 [1:05:30<05:29, 16.50s/it] 96%|█████████▋| 488/507 [1:05:47<05:13, 16.50s/it]                                                   {'loss': 0.0194, 'learning_rate': 7.380385262915179e-07, 'epoch': 2.88}
 96%|█████████▋| 488/507 [1:05:47<05:13, 16.50s/it] 96%|█████████▋| 489/507 [1:06:03<04:57, 16.51s/it]                                                   {'loss': 0.0199, 'learning_rate': 6.624783262958012e-07, 'epoch': 2.89}
 96%|█████████▋| 489/507 [1:06:03<04:57, 16.51s/it] 97%|█████████▋| 490/507 [1:06:20<04:40, 16.51s/it]                                                   {'loss': 0.0163, 'learning_rate': 5.909848868341783e-07, 'epoch': 2.9}
 97%|█████████▋| 490/507 [1:06:20<04:40, 16.51s/it] 97%|█████████▋| 491/507 [1:06:36<04:24, 16.51s/it]                                                   {'loss': 0.0133, 'learning_rate': 5.235611347634172e-07, 'epoch': 2.9}
 97%|█████████▋| 491/507 [1:06:36<04:24, 16.51s/it] 97%|█████████▋| 492/507 [1:06:53<04:07, 16.51s/it]                                                   {'loss': 0.0094, 'learning_rate': 4.602098303321256e-07, 'epoch': 2.91}
 97%|█████████▋| 492/507 [1:06:53<04:07, 16.51s/it] 97%|█████████▋| 493/507 [1:07:09<03:51, 16.52s/it]                                                   {'loss': 0.0024, 'learning_rate': 4.00933567067685e-07, 'epoch': 2.91}
 97%|█████████▋| 493/507 [1:07:09<03:51, 16.52s/it] 97%|█████████▋| 494/507 [1:07:26<03:34, 16.52s/it]                                                   {'loss': 0.0211, 'learning_rate': 3.4573477167015866e-07, 'epoch': 2.92}
 97%|█████████▋| 494/507 [1:07:26<03:34, 16.52s/it] 98%|█████████▊| 495/507 [1:07:43<03:18, 16.52s/it]                                                   {'loss': 0.0287, 'learning_rate': 2.9461570391287055e-07, 'epoch': 2.93}
 98%|█████████▊| 495/507 [1:07:43<03:18, 16.52s/it] 98%|█████████▊| 496/507 [1:07:59<03:01, 16.52s/it]                                                   {'loss': 0.0084, 'learning_rate': 2.4757845654992397e-07, 'epoch': 2.93}
 98%|█████████▊| 496/507 [1:07:59<03:01, 16.52s/it] 98%|█████████▊| 497/507 [1:08:16<02:45, 16.51s/it]                                                   {'loss': 0.0096, 'learning_rate': 2.0462495523057011e-07, 'epoch': 2.94}
 98%|█████████▊| 497/507 [1:08:16<02:45, 16.51s/it] 98%|█████████▊| 498/507 [1:08:32<02:28, 16.51s/it]                                                   {'loss': 0.0021, 'learning_rate': 1.6575695842027117e-07, 'epoch': 2.94}
 98%|█████████▊| 498/507 [1:08:32<02:28, 16.51s/it] 98%|█████████▊| 499/507 [1:08:49<02:12, 16.51s/it]                                                   {'loss': 0.0112, 'learning_rate': 1.3097605732882435e-07, 'epoch': 2.95}
 98%|█████████▊| 499/507 [1:08:49<02:12, 16.51s/it] 99%|█████████▊| 500/507 [1:09:05<01:55, 16.47s/it]                                                   {'loss': 0.0109, 'learning_rate': 1.0028367584512532e-07, 'epoch': 2.96}
 99%|█████████▊| 500/507 [1:09:05<01:55, 16.47s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 99%|█████████▉| 501/507 [1:09:27<01:49, 18.24s/it]                                                   {'loss': 0.0052, 'learning_rate': 7.368107047894812e-08, 'epoch': 2.96}
 99%|█████████▉| 501/507 [1:09:27<01:49, 18.24s/it] 99%|█████████▉| 502/507 [1:09:44<01:28, 17.66s/it]                                                   {'loss': 0.0163, 'learning_rate': 5.116933030946402e-08, 'epoch': 2.97}
 99%|█████████▉| 502/507 [1:09:44<01:28, 17.66s/it] 99%|█████████▉| 503/507 [1:10:00<01:09, 17.28s/it]                                                   {'loss': 0.009, 'learning_rate': 3.2749376940655054e-08, 'epoch': 2.97}
 99%|█████████▉| 503/507 [1:10:00<01:09, 17.28s/it] 99%|█████████▉| 504/507 [1:10:17<00:51, 17.05s/it]                                                   {'loss': 0.0148, 'learning_rate': 1.8421964463610774e-08, 'epoch': 2.98}
 99%|█████████▉| 504/507 [1:10:17<00:51, 17.05s/it]100%|█████████▉| 505/507 [1:10:33<00:33, 16.89s/it]                                                   {'loss': 0.0106, 'learning_rate': 8.187679425630812e-09, 'epoch': 2.99}
100%|█████████▉| 505/507 [1:10:33<00:33, 16.89s/it]100%|█████████▉| 506/507 [1:10:49<00:16, 16.76s/it]                                                   {'loss': 0.0144, 'learning_rate': 2.046940806244013e-09, 'epoch': 2.99}
100%|█████████▉| 506/507 [1:10:49<00:16, 16.76s/it]100%|██████████| 507/507 [1:11:06<00:00, 16.69s/it]                                                   {'loss': 0.02, 'learning_rate': 0.0, 'epoch': 3.0}
100%|██████████| 507/507 [1:11:06<00:00, 16.69s/it]                                                   {'train_runtime': 4266.6526, 'train_samples_per_second': 7.609, 'train_steps_per_second': 0.119, 'train_loss': 0.009051857632814487, 'epoch': 3.0}
100%|██████████| 507/507 [1:11:06<00:00, 16.69s/it]100%|██████████| 507/507 [1:11:06<00:00,  8.42s/it]
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[2026-01-09 21:36:02,440] [INFO] [launch.py:347:main] Process 665012 exits successfully.
[2026-01-09 21:36:02,440] [INFO] [launch.py:347:main] Process 665013 exits successfully.
[2026-01-09 21:36:03,441] [INFO] [launch.py:347:main] Process 665014 exits successfully.
[2026-01-09 21:36:03,441] [INFO] [launch.py:347:main] Process 665011 exits successfully.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` attribute will be overwritten with the one you passed to `from_pretrained`.
Loading LLaVA from base model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  2.00s/it]
Loading additional LLaVA weights...
⚠️ Warning: Size mismatch detected in non-LoRA trainables loading. Skipping... 
Error: Error(s) in loading state_dict for LlavaLlamaForCausalLM:
	size mismatch for model.mm_projector.0.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([2097152, 1]).
	size mismatch for model.mm_projector.2.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([8388608, 1]).
These weights (e.g., projector) will likely be loaded correctly via LoRA adapter later.
Loading LoRA weights...
Model is loaded...
[auto] 'visa' not found at /data3/jisu/MFM/datasets/ViSA. Start downloading...
[extract] /data3/jisu/MFM/datasets/VisA_20220922.tar -> /data3/jisu/MFM/datasets/ViSA
[warn] auto-download failed for visa: [Errno 2] No such file or directory: '/data3/jisu/MFM/datasets/VisA_20220922.tar'
→ 수동으로 내려받아 /data3/jisu/MFM/datasets/ViSA에 두고 다시 시도하세요.
Start Evaluation...
  0%|          | 0/4328 [00:00<?, ?it/s]  0%|          | 1/4328 [00:01<2:02:26,  1.70s/it]  0%|          | 2/4328 [00:02<1:27:13,  1.21s/it]  0%|          | 3/4328 [00:03<1:15:48,  1.05s/it]  0%|          | 4/4328 [00:04<1:10:17,  1.03it/s]  0%|          | 5/4328 [00:05<1:07:16,  1.07it/s]  0%|          | 6/4328 [00:06<1:05:40,  1.10it/s]  0%|          | 7/4328 [00:06<1:04:47,  1.11it/s]  0%|          | 8/4328 [00:07<1:03:50,  1.13it/s]  0%|          | 9/4328 [00:08<1:03:10,  1.14it/s]  0%|          | 10/4328 [00:09<1:03:05,  1.14it/s]  0%|          | 11/4328 [00:10<1:02:46,  1.15it/s]  0%|          | 12/4328 [00:11<1:02:27,  1.15it/s]  0%|          | 13/4328 [00:12<1:02:34,  1.15it/s]  0%|          | 14/4328 [00:12<1:02:31,  1.15it/s]  0%|          | 15/4328 [00:13<1:02:14,  1.15it/s]  0%|          | 16/4328 [00:14<1:02:04,  1.16it/s]  0%|          | 17/4328 [00:15<1:02:44,  1.15it/s]  0%|          | 18/4328 [00:16<1:02:27,  1.15it/s]  0%|          | 19/4328 [00:17<1:02:15,  1.15it/s]  0%|          | 20/4328 [00:18<1:02:27,  1.15it/s]  0%|          | 21/4328 [00:19<1:02:13,  1.15it/s]  1%|          | 22/4328 [00:19<1:02:05,  1.16it/s]  1%|          | 23/4328 [00:20<1:02:23,  1.15it/s]  1%|          | 24/4328 [00:21<1:02:11,  1.15it/s]  1%|          | 25/4328 [00:22<1:02:02,  1.16it/s]  1%|          | 26/4328 [00:23<1:02:07,  1.15it/s]  1%|          | 27/4328 [00:24<1:02:03,  1.16it/s]  1%|          | 28/4328 [00:25<1:01:56,  1.16it/s]  1%|          | 29/4328 [00:25<1:02:09,  1.15it/s]  1%|          | 30/4328 [00:26<1:02:08,  1.15it/s]  1%|          | 31/4328 [00:27<1:01:58,  1.16it/s]  1%|          | 32/4328 [00:28<1:01:51,  1.16it/s]  1%|          | 33/4328 [00:29<1:02:00,  1.15it/s]  1%|          | 34/4328 [00:30<1:01:54,  1.16it/s]  1%|          | 35/4328 [00:31<1:01:49,  1.16it/s]  1%|          | 36/4328 [00:32<1:02:08,  1.15it/s]  1%|          | 37/4328 [00:32<1:02:00,  1.15it/s]  1%|          | 38/4328 [00:33<1:01:53,  1.16it/s]  1%|          | 39/4328 [00:34<1:02:13,  1.15it/s]  1%|          | 40/4328 [00:35<1:02:03,  1.15it/s]  1%|          | 41/4328 [00:36<1:01:56,  1.15it/s]  1%|          | 42/4328 [00:37<1:02:02,  1.15it/s]  1%|          | 43/4328 [00:38<1:01:55,  1.15it/s]  1%|          | 44/4328 [00:38<1:01:48,  1.16it/s]  1%|          | 45/4328 [00:39<1:02:02,  1.15it/s]  1%|          | 46/4328 [00:40<1:01:57,  1.15it/s]  1%|          | 47/4328 [00:41<1:01:49,  1.15it/s]  1%|          | 48/4328 [00:42<1:02:01,  1.15it/s]  1%|          | 49/4328 [00:43<1:01:57,  1.15it/s]  1%|          | 50/4328 [00:44<1:01:49,  1.15it/s]  1%|          | 51/4328 [00:45<1:02:03,  1.15it/s]  1%|          | 52/4328 [00:45<1:02:00,  1.15it/s]  1%|          | 53/4328 [00:46<1:01:51,  1.15it/s]  1%|          | 54/4328 [00:47<1:01:43,  1.15it/s]  1%|▏         | 55/4328 [00:48<1:01:50,  1.15it/s]  1%|▏         | 56/4328 [00:49<1:01:44,  1.15it/s]  1%|▏         | 57/4328 [00:50<1:01:39,  1.15it/s]  1%|▏         | 58/4328 [00:51<1:01:58,  1.15it/s]  1%|▏         | 59/4328 [00:51<1:01:50,  1.15it/s]  1%|▏         | 60/4328 [00:52<1:01:47,  1.15it/s]  1%|▏         | 61/4328 [00:53<1:02:03,  1.15it/s]  1%|▏         | 62/4328 [00:54<1:01:55,  1.15it/s]  1%|▏         | 63/4328 [00:55<1:01:49,  1.15it/s]  1%|▏         | 64/4328 [00:56<1:02:04,  1.14it/s]  2%|▏         | 65/4328 [00:57<1:01:54,  1.15it/s]  2%|▏         | 66/4328 [00:58<1:01:47,  1.15it/s]  2%|▏         | 67/4328 [00:58<1:02:04,  1.14it/s]  2%|▏         | 68/4328 [00:59<1:01:53,  1.15it/s]  2%|▏         | 69/4328 [01:00<1:01:45,  1.15it/s]  2%|▏         | 70/4328 [01:01<1:01:55,  1.15it/s]  2%|▏         | 71/4328 [01:02<1:01:45,  1.15it/s]  2%|▏         | 72/4328 [01:03<1:01:39,  1.15it/s]  2%|▏         | 73/4328 [01:04<1:01:55,  1.15it/s]  2%|▏         | 74/4328 [01:05<1:01:49,  1.15it/s]  2%|▏         | 75/4328 [01:05<1:01:42,  1.15it/s]  2%|▏         | 76/4328 [01:06<1:01:50,  1.15it/s]  2%|▏         | 77/4328 [01:07<1:01:42,  1.15it/s]  2%|▏         | 78/4328 [01:08<1:01:35,  1.15it/s]  2%|▏         | 79/4328 [01:09<1:01:45,  1.15it/s]  2%|▏         | 80/4328 [01:10<1:01:36,  1.15it/s]  2%|▏         | 81/4328 [01:11<1:01:30,  1.15it/s]  2%|▏         | 82/4328 [01:12<1:01:41,  1.15it/s]  2%|▏         | 83/4328 [01:12<1:01:33,  1.15it/s]  2%|▏         | 84/4328 [01:13<1:01:28,  1.15it/s]  2%|▏         | 85/4328 [01:14<1:01:39,  1.15it/s]  2%|▏         | 86/4328 [01:15<1:01:31,  1.15it/s]  2%|▏         | 87/4328 [01:16<1:01:26,  1.15it/s]  2%|▏         | 88/4328 [01:17<1:01:37,  1.15it/s]  2%|▏         | 89/4328 [01:18<1:01:29,  1.15it/s]  2%|▏         | 90/4328 [01:18<1:01:24,  1.15it/s]  2%|▏         | 91/4328 [01:19<1:01:36,  1.15it/s]  2%|▏         | 92/4328 [01:20<1:01:27,  1.15it/s]  2%|▏         | 93/4328 [01:21<1:01:21,  1.15it/s]  2%|▏         | 94/4328 [01:22<1:01:29,  1.15it/s]  2%|▏         | 95/4328 [01:23<1:01:22,  1.15it/s]  2%|▏         | 96/4328 [01:24<1:01:17,  1.15it/s]  2%|▏         | 97/4328 [01:25<1:01:28,  1.15it/s]  2%|▏         | 98/4328 [01:25<1:01:21,  1.15it/s]  2%|▏         | 99/4328 [01:26<1:01:15,  1.15it/s]  2%|▏         | 100/4328 [01:27<1:01:25,  1.15it/s]  2%|▏         | 101/4328 [01:28<1:01:19,  1.15it/s]  2%|▏         | 102/4328 [01:29<1:01:14,  1.15it/s]  2%|▏         | 103/4328 [01:30<1:01:24,  1.15it/s]  2%|▏         | 104/4328 [01:31<1:01:17,  1.15it/s]  2%|▏         | 105/4328 [01:32<1:01:12,  1.15it/s]  2%|▏         | 106/4328 [01:32<1:01:26,  1.15it/s]  2%|▏         | 107/4328 [01:33<1:01:20,  1.15it/s]  2%|▏         | 108/4328 [01:34<1:01:14,  1.15it/s]  3%|▎         | 109/4328 [01:35<1:01:23,  1.15it/s]  3%|▎         | 110/4328 [01:36<1:01:20,  1.15it/s]  3%|▎         | 111/4328 [01:37<1:01:31,  1.14it/s]  3%|▎         | 112/4328 [01:38<1:01:30,  1.14it/s]  3%|▎         | 113/4328 [01:39<1:01:19,  1.15it/s]  3%|▎         | 114/4328 [01:39<1:01:12,  1.15it/s]  3%|▎         | 115/4328 [01:40<1:01:05,  1.15it/s]  3%|▎         | 116/4328 [01:41<1:00:59,  1.15it/s]  3%|▎         | 117/4328 [01:42<1:01:29,  1.14it/s]  3%|▎         | 118/4328 [01:43<1:01:20,  1.14it/s]  3%|▎         | 119/4328 [01:44<1:01:13,  1.15it/s]  3%|▎         | 120/4328 [01:45<1:01:22,  1.14it/s]  3%|▎         | 121/4328 [01:46<1:01:16,  1.14it/s]  3%|▎         | 122/4328 [01:46<1:01:09,  1.15it/s]  3%|▎         | 123/4328 [01:47<1:01:13,  1.14it/s]  3%|▎         | 124/4328 [01:48<1:01:06,  1.15it/s]  3%|▎         | 125/4328 [01:49<1:00:59,  1.15it/s]  3%|▎         | 126/4328 [01:50<1:01:06,  1.15it/s]  3%|▎         | 127/4328 [01:51<1:01:04,  1.15it/s]  3%|▎         | 128/4328 [01:52<1:00:57,  1.15it/s]  3%|▎         | 129/4328 [01:52<1:01:03,  1.15it/s]  3%|▎         | 130/4328 [01:53<1:00:57,  1.15it/s]  3%|▎         | 131/4328 [01:54<1:00:51,  1.15it/s]  3%|▎         | 132/4328 [01:55<1:00:55,  1.15it/s]  3%|▎         | 133/4328 [01:56<1:00:52,  1.15it/s]  3%|▎         | 134/4328 [01:57<1:00:47,  1.15it/s]  3%|▎         | 135/4328 [01:58<1:00:52,  1.15it/s]  3%|▎         | 136/4328 [01:59<1:00:48,  1.15it/s]  3%|▎         | 137/4328 [01:59<1:00:43,  1.15it/s]  3%|▎         | 138/4328 [02:00<1:00:53,  1.15it/s]  3%|▎         | 139/4328 [02:01<1:00:50,  1.15it/s]  3%|▎         | 140/4328 [02:02<1:00:46,  1.15it/s]  3%|▎         | 141/4328 [02:03<1:00:52,  1.15it/s]  3%|▎         | 142/4328 [02:04<1:00:49,  1.15it/s]  3%|▎         | 143/4328 [02:05<1:00:47,  1.15it/s]  3%|▎         | 144/4328 [02:06<1:00:42,  1.15it/s]  3%|▎         | 145/4328 [02:06<1:00:37,  1.15it/s]  3%|▎         | 146/4328 [02:07<1:00:51,  1.15it/s]  3%|▎         | 147/4328 [02:08<1:01:06,  1.14it/s]  3%|▎         | 148/4328 [02:09<1:00:57,  1.14it/s]  3%|▎         | 149/4328 [02:10<1:00:49,  1.14it/s]  3%|▎         | 150/4328 [02:11<1:00:57,  1.14it/s]  3%|▎         | 151/4328 [02:12<1:00:49,  1.14it/s]  4%|▎         | 152/4328 [02:13<1:00:43,  1.15it/s]  4%|▎         | 153/4328 [02:13<1:00:49,  1.14it/s]  4%|▎         | 154/4328 [02:14<1:00:40,  1.15it/s]  4%|▎         | 155/4328 [02:15<1:00:33,  1.15it/s]  4%|▎         | 156/4328 [02:16<1:00:39,  1.15it/s]  4%|▎         | 157/4328 [02:17<1:00:33,  1.15it/s]  4%|▎         | 158/4328 [02:18<1:00:29,  1.15it/s]  4%|▎         | 159/4328 [02:19<1:00:36,  1.15it/s]  4%|▎         | 160/4328 [02:20<1:00:30,  1.15it/s]  4%|▎         | 161/4328 [02:20<1:00:27,  1.15it/s]  4%|▎         | 162/4328 [02:21<1:00:35,  1.15it/s]  4%|▍         | 163/4328 [02:22<1:00:30,  1.15it/s]  4%|▍         | 164/4328 [02:23<1:00:27,  1.15it/s]  4%|▍         | 165/4328 [02:24<1:00:31,  1.15it/s]  4%|▍         | 166/4328 [02:25<1:00:26,  1.15it/s]  4%|▍         | 167/4328 [02:26<1:00:21,  1.15it/s]  4%|▍         | 168/4328 [02:26<1:00:26,  1.15it/s]  4%|▍         | 169/4328 [02:27<1:00:21,  1.15it/s]  4%|▍         | 170/4328 [02:28<1:00:17,  1.15it/s]  4%|▍         | 171/4328 [02:29<1:00:22,  1.15it/s]  4%|▍         | 172/4328 [02:30<1:00:18,  1.15it/s]  4%|▍         | 173/4328 [02:31<1:00:14,  1.15it/s]  4%|▍         | 174/4328 [02:32<1:00:21,  1.15it/s]  4%|▍         | 175/4328 [02:33<1:00:18,  1.15it/s]  4%|▍         | 176/4328 [02:33<1:00:14,  1.15it/s]  4%|▍         | 177/4328 [02:34<1:00:18,  1.15it/s]  4%|▍         | 178/4328 [02:35<1:00:16,  1.15it/s]  4%|▍         | 179/4328 [02:36<1:00:31,  1.14it/s]  4%|▍         | 180/4328 [02:37<1:00:28,  1.14it/s]  4%|▍         | 181/4328 [02:38<1:00:22,  1.14it/s]  4%|▍         | 182/4328 [02:39<1:00:23,  1.14it/s]  4%|▍         | 183/4328 [02:40<1:00:20,  1.14it/s]  4%|▍         | 184/4328 [02:40<1:00:14,  1.15it/s]  4%|▍         | 185/4328 [02:41<1:00:18,  1.14it/s]  4%|▍         | 186/4328 [02:42<1:00:14,  1.15it/s]  4%|▍         | 187/4328 [02:43<1:00:10,  1.15it/s]  4%|▍         | 188/4328 [02:44<1:00:13,  1.15it/s]  4%|▍         | 189/4328 [02:45<1:00:09,  1.15it/s]  4%|▍         | 190/4328 [02:46<1:00:06,  1.15it/s]  4%|▍         | 191/4328 [02:47<1:00:23,  1.14it/s]  4%|▍         | 192/4328 [02:47<1:00:16,  1.14it/s]  4%|▍         | 193/4328 [02:48<1:00:10,  1.15it/s]  4%|▍         | 194/4328 [02:49<1:00:19,  1.14it/s]  5%|▍         | 195/4328 [02:50<1:00:12,  1.14it/s]  5%|▍         | 196/4328 [02:51<1:00:04,  1.15it/s]  5%|▍         | 197/4328 [02:52<1:00:09,  1.14it/s]  5%|▍         | 198/4328 [02:53<1:00:03,  1.15it/s]  5%|▍         | 199/4328 [02:54<59:59,  1.15it/s]    5%|▍         | 200/4328 [02:54<1:00:03,  1.15it/s]  5%|▍         | 201/4328 [02:55<59:57,  1.15it/s]    5%|▍         | 202/4328 [02:56<59:51,  1.15it/s]  5%|▍         | 203/4328 [02:57<59:58,  1.15it/s]  5%|▍         | 204/4328 [02:58<59:53,  1.15it/s]  5%|▍         | 205/4328 [02:59<59:49,  1.15it/s]  5%|▍         | 206/4328 [03:00<59:52,  1.15it/s]  5%|▍         | 207/4328 [03:01<59:47,  1.15it/s]  5%|▍         | 208/4328 [03:01<59:44,  1.15it/s]  5%|▍         | 209/4328 [03:02<59:50,  1.15it/s]  5%|▍         | 210/4328 [03:03<59:46,  1.15it/s]  5%|▍         | 211/4328 [03:04<59:42,  1.15it/s]  5%|▍         | 212/4328 [03:05<59:51,  1.15it/s]  5%|▍         | 213/4328 [03:06<59:45,  1.15it/s]  5%|▍         | 214/4328 [03:07<59:40,  1.15it/s]  5%|▍         | 215/4328 [03:07<59:44,  1.15it/s]  5%|▍         | 216/4328 [03:08<59:39,  1.15it/s]  5%|▌         | 217/4328 [03:09<59:36,  1.15it/s]  5%|▌         | 218/4328 [03:10<59:43,  1.15it/s]  5%|▌         | 219/4328 [03:11<59:39,  1.15it/s]  5%|▌         | 220/4328 [03:12<59:35,  1.15it/s]  5%|▌         | 221/4328 [03:13<59:39,  1.15it/s]  5%|▌         | 222/4328 [03:14<59:30,  1.15it/s]  5%|▌         | 223/4328 [03:14<59:31,  1.15it/s]  5%|▌         | 224/4328 [03:15<59:38,  1.15it/s]  5%|▌         | 225/4328 [03:16<59:32,  1.15it/s]  5%|▌         | 226/4328 [03:17<59:29,  1.15it/s]  5%|▌         | 227/4328 [03:18<59:34,  1.15it/s]  5%|▌         | 228/4328 [03:19<59:29,  1.15it/s]  5%|▌         | 229/4328 [03:20<59:26,  1.15it/s]  5%|▌         | 230/4328 [03:21<59:31,  1.15it/s]  5%|▌         | 231/4328 [03:21<59:26,  1.15it/s]  5%|▌         | 232/4328 [03:22<59:23,  1.15it/s]  5%|▌         | 233/4328 [03:23<59:29,  1.15it/s]  5%|▌         | 234/4328 [03:24<59:24,  1.15it/s]  5%|▌         | 235/4328 [03:25<59:22,  1.15it/s]  5%|▌         | 236/4328 [03:26<59:27,  1.15it/s]  5%|▌         | 237/4328 [03:27<59:28,  1.15it/s]  5%|▌         | 238/4328 [03:28<59:37,  1.14it/s]  6%|▌         | 239/4328 [03:28<59:35,  1.14it/s]  6%|▌         | 240/4328 [03:29<59:28,  1.15it/s]  6%|▌         | 241/4328 [03:30<59:33,  1.14it/s]  6%|▌         | 242/4328 [03:31<59:31,  1.14it/s]  6%|▌         | 243/4328 [03:32<59:25,  1.15it/s]  6%|▌         | 244/4328 [03:33<59:26,  1.15it/s]  6%|▌         | 245/4328 [03:34<59:25,  1.15it/s]  6%|▌         | 246/4328 [03:35<59:20,  1.15it/s]  6%|▌         | 247/4328 [03:35<59:15,  1.15it/s]  6%|▌         | 248/4328 [03:36<59:11,  1.15it/s]  6%|▌         | 249/4328 [03:37<59:25,  1.14it/s]  6%|▌         | 250/4328 [03:38<59:41,  1.14it/s]  6%|▌         | 251/4328 [03:39<59:30,  1.14it/s]  6%|▌         | 252/4328 [03:40<59:22,  1.14it/s]  6%|▌         | 253/4328 [03:41<59:27,  1.14it/s]  6%|▌         | 254/4328 [03:41<59:20,  1.14it/s]  6%|▌         | 255/4328 [03:42<59:14,  1.15it/s]  6%|▌         | 256/4328 [03:43<59:18,  1.14it/s]  6%|▌         | 257/4328 [03:44<59:17,  1.14it/s]  6%|▌         | 258/4328 [03:45<59:27,  1.14it/s]  6%|▌         | 259/4328 [03:46<59:23,  1.14it/s]  6%|▌         | 260/4328 [03:47<59:17,  1.14it/s]  6%|▌         | 261/4328 [03:48<59:23,  1.14it/s]  6%|▌         | 262/4328 [03:48<59:17,  1.14it/s]  6%|▌         | 263/4328 [03:49<59:10,  1.14it/s]  6%|▌         | 264/4328 [03:50<59:13,  1.14it/s]  6%|▌         | 265/4328 [03:51<59:09,  1.14it/s]  6%|▌         | 266/4328 [03:52<59:04,  1.15it/s]  6%|▌         | 267/4328 [03:53<58:57,  1.15it/s]  6%|▌         | 268/4328 [03:54<58:53,  1.15it/s]  6%|▌         | 269/4328 [03:55<59:07,  1.14it/s]  6%|▌         | 270/4328 [03:55<59:22,  1.14it/s]  6%|▋         | 271/4328 [03:56<59:12,  1.14it/s]  6%|▋         | 272/4328 [03:57<59:08,  1.14it/s]  6%|▋         | 273/4328 [03:58<59:10,  1.14it/s]  6%|▋         | 274/4328 [03:59<59:02,  1.14it/s]  6%|▋         | 275/4328 [04:00<58:58,  1.15it/s]  6%|▋         | 276/4328 [04:01<59:00,  1.14it/s]  6%|▋         | 277/4328 [04:02<58:58,  1.14it/s]  6%|▋         | 278/4328 [04:02<59:11,  1.14it/s]  6%|▋         | 279/4328 [04:03<59:08,  1.14it/s]  6%|▋         | 280/4328 [04:04<59:02,  1.14it/s]  6%|▋         | 281/4328 [04:05<59:02,  1.14it/s]  7%|▋         | 282/4328 [04:06<58:57,  1.14it/s]  7%|▋         | 283/4328 [04:07<58:51,  1.15it/s]  7%|▋         | 284/4328 [04:08<58:51,  1.15it/s]  7%|▋         | 285/4328 [04:09<58:47,  1.15it/s]  7%|▋         | 286/4328 [04:09<58:44,  1.15it/s]  7%|▋         | 287/4328 [04:10<58:49,  1.15it/s]  7%|▋         | 288/4328 [04:11<58:46,  1.15it/s]  7%|▋         | 289/4328 [04:12<58:46,  1.15it/s]  7%|▋         | 290/4328 [04:13<58:58,  1.14it/s]  7%|▋         | 291/4328 [04:14<58:47,  1.14it/s]  7%|▋         | 292/4328 [04:15<58:43,  1.15it/s]  7%|▋         | 293/4328 [04:16<58:49,  1.14it/s]  7%|▋         | 294/4328 [04:16<58:42,  1.15it/s]  7%|▋         | 295/4328 [04:17<58:39,  1.15it/s]  7%|▋         | 296/4328 [04:18<58:42,  1.14it/s]  7%|▋         | 297/4328 [04:19<58:37,  1.15it/s]  7%|▋         | 298/4328 [04:20<58:33,  1.15it/s]  7%|▋         | 299/4328 [04:21<58:30,  1.15it/s]  7%|▋         | 300/4328 [04:22<58:27,  1.15it/s]  7%|▋         | 301/4328 [04:23<58:58,  1.14it/s]  7%|▋         | 302/4328 [04:23<58:51,  1.14it/s]  7%|▋         | 303/4328 [04:24<58:42,  1.14it/s]  7%|▋         | 304/4328 [04:25<58:41,  1.14it/s]  7%|▋         | 305/4328 [04:26<58:35,  1.14it/s]  7%|▋         | 306/4328 [04:27<58:29,  1.15it/s]  7%|▋         | 307/4328 [04:28<58:31,  1.15it/s]  7%|▋         | 308/4328 [04:29<58:27,  1.15it/s]  7%|▋         | 309/4328 [04:30<58:23,  1.15it/s]  7%|▋         | 310/4328 [04:30<58:21,  1.15it/s]  7%|▋         | 311/4328 [04:31<58:19,  1.15it/s]  7%|▋         | 312/4328 [04:32<58:34,  1.14it/s]  7%|▋         | 313/4328 [04:33<58:49,  1.14it/s]  7%|▋         | 314/4328 [04:34<58:40,  1.14it/s]  7%|▋         | 315/4328 [04:35<58:32,  1.14it/s]  7%|▋         | 316/4328 [04:36<58:25,  1.14it/s]  7%|▋         | 317/4328 [04:37<58:20,  1.15it/s]  7%|▋         | 318/4328 [04:37<58:49,  1.14it/s]  7%|▋         | 319/4328 [04:38<58:42,  1.14it/s]  7%|▋         | 320/4328 [04:39<58:33,  1.14it/s]  7%|▋         | 321/4328 [04:40<58:32,  1.14it/s]  7%|▋         | 322/4328 [04:41<58:25,  1.14it/s]  7%|▋         | 323/4328 [04:42<58:19,  1.14it/s]  7%|▋         | 324/4328 [04:43<58:14,  1.15it/s]  8%|▊         | 325/4328 [04:44<58:09,  1.15it/s]  8%|▊         | 326/4328 [04:44<58:24,  1.14it/s]  8%|▊         | 327/4328 [04:45<58:36,  1.14it/s]  8%|▊         | 328/4328 [04:46<58:25,  1.14it/s]  8%|▊         | 329/4328 [04:47<58:18,  1.14it/s]  8%|▊         | 330/4328 [04:48<58:20,  1.14it/s]  8%|▊         | 331/4328 [04:49<58:16,  1.14it/s]  8%|▊         | 332/4328 [04:50<58:26,  1.14it/s]  8%|▊         | 333/4328 [04:51<58:22,  1.14it/s]  8%|▊         | 334/4328 [04:51<58:14,  1.14it/s]  8%|▊         | 335/4328 [04:52<58:14,  1.14it/s]  8%|▊         | 336/4328 [04:53<58:11,  1.14it/s]  8%|▊         | 337/4328 [04:54<58:05,  1.15it/s]  8%|▊         | 338/4328 [04:55<58:04,  1.15it/s]  8%|▊         | 339/4328 [04:56<58:02,  1.15it/s]  8%|▊         | 340/4328 [04:57<57:59,  1.15it/s]  8%|▊         | 341/4328 [04:58<58:01,  1.15it/s]  8%|▊         | 342/4328 [04:58<57:59,  1.15it/s]  8%|▊         | 343/4328 [04:59<57:59,  1.15it/s]  8%|▊         | 344/4328 [05:00<58:12,  1.14it/s]  8%|▊         | 345/4328 [05:01<58:05,  1.14it/s]  8%|▊         | 346/4328 [05:02<57:59,  1.14it/s]  8%|▊         | 347/4328 [05:03<58:03,  1.14it/s]  8%|▊         | 348/4328 [05:04<57:58,  1.14it/s]  8%|▊         | 349/4328 [05:05<57:55,  1.15it/s]  8%|▊         | 350/4328 [05:05<57:50,  1.15it/s]  8%|▊         | 351/4328 [05:06<57:46,  1.15it/s]  8%|▊         | 352/4328 [05:07<58:17,  1.14it/s]  8%|▊         | 353/4328 [05:08<58:10,  1.14it/s]  8%|▊         | 354/4328 [05:09<58:01,  1.14it/s]  8%|▊         | 355/4328 [05:10<57:54,  1.14it/s]  8%|▊         | 356/4328 [05:11<57:48,  1.15it/s]  8%|▊         | 357/4328 [05:12<58:01,  1.14it/s]  8%|▊         | 358/4328 [05:12<58:12,  1.14it/s]  8%|▊         | 359/4328 [05:13<58:02,  1.14it/s]  8%|▊         | 360/4328 [05:14<57:55,  1.14it/s]  8%|▊         | 361/4328 [05:15<57:48,  1.14it/s]  8%|▊         | 362/4328 [05:16<57:42,  1.15it/s]  8%|▊         | 363/4328 [05:17<58:10,  1.14it/s]  8%|▊         | 364/4328 [05:18<58:03,  1.14it/s]  8%|▊         | 365/4328 [05:19<57:53,  1.14it/s]  8%|▊         | 366/4328 [05:19<57:54,  1.14it/s]  8%|▊         | 367/4328 [05:20<57:49,  1.14it/s]  9%|▊         | 368/4328 [05:21<57:45,  1.14it/s]  9%|▊         | 369/4328 [05:22<57:58,  1.14it/s]  9%|▊         | 370/4328 [05:23<57:50,  1.14it/s]  9%|▊         | 371/4328 [05:24<57:43,  1.14it/s]  9%|▊         | 372/4328 [05:25<57:42,  1.14it/s]  9%|▊         | 373/4328 [05:26<57:34,  1.14it/s]  9%|▊         | 374/4328 [05:26<57:31,  1.15it/s]  9%|▊         | 375/4328 [05:27<57:33,  1.14it/s]  9%|▊         | 376/4328 [05:28<57:28,  1.15it/s]  9%|▊         | 377/4328 [05:29<57:24,  1.15it/s]  9%|▊         | 378/4328 [05:30<57:28,  1.15it/s]  9%|▉         | 379/4328 [05:31<57:23,  1.15it/s]  9%|▉         | 380/4328 [05:32<57:19,  1.15it/s]  9%|▉         | 381/4328 [05:33<57:25,  1.15it/s]  9%|▉         | 382/4328 [05:33<57:23,  1.15it/s]  9%|▉         | 383/4328 [05:34<57:38,  1.14it/s]  9%|▉         | 384/4328 [05:35<57:33,  1.14it/s]  9%|▉         | 385/4328 [05:36<57:28,  1.14it/s]  9%|▉         | 386/4328 [05:37<57:31,  1.14it/s]  9%|▉         | 387/4328 [05:38<57:25,  1.14it/s]  9%|▉         | 388/4328 [05:39<57:22,  1.14it/s]  9%|▉         | 389/4328 [05:40<57:20,  1.15it/s]  9%|▉         | 390/4328 [05:40<57:14,  1.15it/s]  9%|▉         | 391/4328 [05:41<57:27,  1.14it/s]  9%|▉         | 392/4328 [05:42<57:39,  1.14it/s]  9%|▉         | 393/4328 [05:43<57:29,  1.14it/s]  9%|▉         | 394/4328 [05:44<57:21,  1.14it/s]  9%|▉         | 395/4328 [05:45<57:14,  1.15it/s]  9%|▉         | 396/4328 [05:46<57:09,  1.15it/s]  9%|▉         | 397/4328 [05:47<57:39,  1.14it/s]  9%|▉         | 398/4328 [05:47<57:31,  1.14it/s]  9%|▉         | 399/4328 [05:48<57:23,  1.14it/s]  9%|▉         | 400/4328 [05:49<57:16,  1.14it/s]  9%|▉         | 401/4328 [05:50<57:09,  1.15it/s]  9%|▉         | 402/4328 [05:51<57:03,  1.15it/s]  9%|▉         | 403/4328 [05:52<57:21,  1.14it/s]  9%|▉         | 404/4328 [05:53<57:15,  1.14it/s]  9%|▉         | 405/4328 [05:54<57:06,  1.14it/s]  9%|▉         | 406/4328 [05:54<57:15,  1.14it/s]  9%|▉         | 407/4328 [05:55<57:09,  1.14it/s]  9%|▉         | 408/4328 [05:56<57:03,  1.14it/s]  9%|▉         | 409/4328 [05:57<57:06,  1.14it/s]  9%|▉         | 410/4328 [05:58<57:00,  1.15it/s]  9%|▉         | 411/4328 [05:59<56:54,  1.15it/s] 10%|▉         | 412/4328 [06:00<57:01,  1.14it/s] 10%|▉         | 413/4328 [06:01<56:56,  1.15it/s] 10%|▉         | 414/4328 [06:01<56:52,  1.15it/s] 10%|▉         | 415/4328 [06:02<56:49,  1.15it/s] 10%|▉         | 416/4328 [06:03<56:46,  1.15it/s] 10%|▉         | 417/4328 [06:04<57:18,  1.14it/s] 10%|▉         | 418/4328 [06:05<57:10,  1.14it/s] 10%|▉         | 419/4328 [06:06<57:01,  1.14it/s] 10%|▉         | 420/4328 [06:07<57:03,  1.14it/s] 10%|▉         | 421/4328 [06:08<56:57,  1.14it/s] 10%|▉         | 422/4328 [06:08<56:52,  1.14it/s] 10%|▉         | 423/4328 [06:09<56:52,  1.14it/s] 10%|▉         | 424/4328 [06:10<56:50,  1.14it/s] 10%|▉         | 425/4328 [06:11<56:48,  1.14it/s] 10%|▉         | 426/4328 [06:12<57:05,  1.14it/s] 10%|▉         | 427/4328 [06:13<56:57,  1.14it/s] 10%|▉         | 428/4328 [06:14<56:51,  1.14it/s] 10%|▉         | 429/4328 [06:15<56:53,  1.14it/s] 10%|▉         | 430/4328 [06:15<56:46,  1.14it/s] 10%|▉         | 431/4328 [06:16<56:39,  1.15it/s] 10%|▉         | 432/4328 [06:17<56:44,  1.14it/s] 10%|█         | 433/4328 [06:18<56:41,  1.14it/s] 10%|█         | 434/4328 [06:19<56:55,  1.14it/s] 10%|█         | 435/4328 [06:20<56:50,  1.14it/s] 10%|█         | 436/4328 [06:21<56:44,  1.14it/s] 10%|█         | 437/4328 [06:22<56:47,  1.14it/s] 10%|█         | 438/4328 [06:22<56:41,  1.14it/s] 10%|█         | 439/4328 [06:23<56:35,  1.15it/s] 10%|█         | 440/4328 [06:24<56:39,  1.14it/s] 10%|█         | 441/4328 [06:25<56:36,  1.14it/s] 10%|█         | 442/4328 [06:26<56:33,  1.14it/s] 10%|█         | 443/4328 [06:27<56:48,  1.14it/s] 10%|█         | 444/4328 [06:28<56:40,  1.14it/s] 10%|█         | 445/4328 [06:29<56:35,  1.14it/s] 10%|█         | 446/4328 [06:29<56:39,  1.14it/s] 10%|█         | 447/4328 [06:30<56:32,  1.14it/s] 10%|█         | 448/4328 [06:31<56:27,  1.15it/s] 10%|█         | 449/4328 [06:32<56:29,  1.14it/s] 10%|█         | 450/4328 [06:33<56:27,  1.14it/s] 10%|█         | 451/4328 [06:34<56:40,  1.14it/s] 10%|█         | 452/4328 [06:35<56:35,  1.14it/s] 10%|█         | 453/4328 [06:36<56:29,  1.14it/s] 10%|█         | 454/4328 [06:36<56:32,  1.14it/s] 11%|█         | 455/4328 [06:37<56:26,  1.14it/s] 11%|█         | 456/4328 [06:38<56:19,  1.15it/s] 11%|█         | 457/4328 [06:39<56:21,  1.14it/s] 11%|█         | 458/4328 [06:40<56:20,  1.14it/s] 11%|█         | 459/4328 [06:41<56:17,  1.15it/s] 11%|█         | 460/4328 [06:42<56:32,  1.14it/s] 11%|█         | 461/4328 [06:43<56:25,  1.14it/s] 11%|█         | 462/4328 [06:43<56:19,  1.14it/s] 11%|█         | 463/4328 [06:44<56:23,  1.14it/s] 11%|█         | 464/4328 [06:45<56:15,  1.14it/s] 11%|█         | 465/4328 [06:46<56:11,  1.15it/s] 11%|█         | 466/4328 [06:47<56:14,  1.14it/s] 11%|█         | 467/4328 [06:48<56:08,  1.15it/s] 11%|█         | 468/4328 [06:49<56:05,  1.15it/s] 11%|█         | 469/4328 [06:50<56:11,  1.14it/s] 11%|█         | 470/4328 [06:50<56:05,  1.15it/s] 11%|█         | 471/4328 [06:51<56:01,  1.15it/s] 11%|█         | 472/4328 [06:52<56:07,  1.15it/s] 11%|█         | 473/4328 [06:53<56:05,  1.15it/s] 11%|█         | 474/4328 [06:54<56:18,  1.14it/s] 11%|█         | 475/4328 [06:55<56:14,  1.14it/s] 11%|█         | 476/4328 [06:56<56:08,  1.14it/s] 11%|█         | 477/4328 [06:57<56:11,  1.14it/s] 11%|█         | 478/4328 [06:57<56:05,  1.14it/s] 11%|█         | 479/4328 [06:58<56:00,  1.15it/s] 11%|█         | 480/4328 [06:59<55:56,  1.15it/s] 11%|█         | 481/4328 [07:00<55:51,  1.15it/s] 11%|█         | 482/4328 [07:01<56:04,  1.14it/s] 11%|█         | 483/4328 [07:02<56:17,  1.14it/s] 11%|█         | 484/4328 [07:03<56:07,  1.14it/s] 11%|█         | 485/4328 [07:04<56:00,  1.14it/s] 11%|█         | 486/4328 [07:04<56:02,  1.14it/s] 11%|█▏        | 487/4328 [07:05<55:55,  1.14it/s] 11%|█▏        | 488/4328 [07:06<55:49,  1.15it/s] 11%|█▏        | 489/4328 [07:07<55:52,  1.15it/s] 11%|█▏        | 490/4328 [07:08<55:51,  1.15it/s] 11%|█▏        | 491/4328 [07:09<56:02,  1.14it/s] 11%|█▏        | 492/4328 [07:10<55:57,  1.14it/s] 11%|█▏        | 493/4328 [07:11<55:51,  1.14it/s] 11%|█▏        | 494/4328 [07:11<55:53,  1.14it/s] 11%|█▏        | 495/4328 [07:12<55:49,  1.14it/s] 11%|█▏        | 496/4328 [07:13<55:45,  1.15it/s] 11%|█▏        | 497/4328 [07:14<55:45,  1.14it/s] 12%|█▏        | 498/4328 [07:15<55:44,  1.15it/s] 12%|█▏        | 499/4328 [07:16<55:39,  1.15it/s] 12%|█▏        | 500/4328 [07:17<55:41,  1.15it/s] 12%|█▏        | 501/4328 [07:17<55:39,  1.15it/s] 12%|█▏        | 502/4328 [07:18<55:38,  1.15it/s] 12%|█▏        | 503/4328 [07:19<55:54,  1.14it/s] 12%|█▏        | 504/4328 [07:20<55:46,  1.14it/s] 12%|█▏        | 505/4328 [07:21<55:39,  1.14it/s] 12%|█▏        | 506/4328 [07:22<55:44,  1.14it/s] 12%|█▏        | 507/4328 [07:23<55:38,  1.14it/s] 12%|█▏        | 508/4328 [07:24<55:33,  1.15it/s] 12%|█▏        | 509/4328 [07:24<55:38,  1.14it/s] 12%|█▏        | 510/4328 [07:25<55:32,  1.15it/s] 12%|█▏        | 511/4328 [07:26<55:29,  1.15it/s] 12%|█▏        | 512/4328 [07:27<55:25,  1.15it/s] 12%|█▏        | 513/4328 [07:28<55:23,  1.15it/s] 12%|█▏        | 514/4328 [07:29<55:54,  1.14it/s] 12%|█▏        | 515/4328 [07:30<55:47,  1.14it/s] 12%|█▏        | 516/4328 [07:31<55:37,  1.14it/s] 12%|█▏        | 517/4328 [07:31<55:37,  1.14it/s] 12%|█▏        | 518/4328 [07:32<55:33,  1.14it/s] 12%|█▏        | 519/4328 [07:33<55:27,  1.14it/s] 12%|█▏        | 520/4328 [07:34<55:28,  1.14it/s] 12%|█▏        | 521/4328 [07:35<55:25,  1.14it/s] 12%|█▏        | 522/4328 [07:36<55:23,  1.15it/s] 12%|█▏        | 523/4328 [07:37<55:39,  1.14it/s] 12%|█▏        | 524/4328 [07:38<55:31,  1.14it/s] 12%|█▏        | 525/4328 [07:38<55:25,  1.14it/s] 12%|█▏        | 526/4328 [07:39<55:30,  1.14it/s] 12%|█▏        | 527/4328 [07:40<55:23,  1.14it/s] 12%|█▏        | 528/4328 [07:41<55:16,  1.15it/s] 12%|█▏        | 529/4328 [07:42<55:18,  1.14it/s] 12%|█▏        | 530/4328 [07:43<55:17,  1.14it/s] 12%|█▏        | 531/4328 [07:44<55:28,  1.14it/s] 12%|█▏        | 532/4328 [07:45<55:25,  1.14it/s] 12%|█▏        | 533/4328 [07:45<55:19,  1.14it/s] 12%|█▏        | 534/4328 [07:46<55:23,  1.14it/s] 12%|█▏        | 535/4328 [07:47<55:17,  1.14it/s] 12%|█▏        | 536/4328 [07:48<55:09,  1.15it/s] 12%|█▏        | 537/4328 [07:49<55:10,  1.15it/s] 12%|█▏        | 538/4328 [07:50<55:07,  1.15it/s] 12%|█▏        | 539/4328 [07:51<55:03,  1.15it/s] 12%|█▏        | 540/4328 [07:52<55:05,  1.15it/s] 12%|█▎        | 541/4328 [07:52<55:04,  1.15it/s] 13%|█▎        | 542/4328 [07:53<55:03,  1.15it/s] 13%|█▎        | 543/4328 [07:54<55:18,  1.14it/s] 13%|█▎        | 544/4328 [07:55<55:12,  1.14it/s] 13%|█▎        | 545/4328 [07:56<55:06,  1.14it/s] 13%|█▎        | 546/4328 [07:57<55:09,  1.14it/s] 13%|█▎        | 547/4328 [07:58<55:03,  1.14it/s] 13%|█▎        | 548/4328 [07:59<54:58,  1.15it/s] 13%|█▎        | 549/4328 [07:59<54:54,  1.15it/s] 13%|█▎        | 550/4328 [08:00<54:51,  1.15it/s] 13%|█▎        | 551/4328 [08:01<55:19,  1.14it/s] 13%|█▎        | 552/4328 [08:02<55:13,  1.14it/s] 13%|█▎        | 553/4328 [08:03<55:05,  1.14it/s] 13%|█▎        | 554/4328 [08:04<55:04,  1.14it/s] 13%|█▎        | 555/4328 [08:05<54:58,  1.14it/s] 13%|█▎        | 556/4328 [08:06<54:52,  1.15it/s] 13%|█▎        | 557/4328 [08:06<54:54,  1.14it/s] 13%|█▎        | 558/4328 [08:07<54:50,  1.15it/s] 13%|█▎        | 559/4328 [08:08<54:49,  1.15it/s] 13%|█▎        | 560/4328 [08:09<55:03,  1.14it/s] 13%|█▎        | 561/4328 [08:10<54:57,  1.14it/s] 13%|█▎        | 562/4328 [08:11<54:52,  1.14it/s] 13%|█▎        | 563/4328 [08:12<54:57,  1.14it/s] 13%|█▎        | 564/4328 [08:13<54:49,  1.14it/s] 13%|█▎        | 565/4328 [08:13<54:44,  1.15it/s] 13%|█▎        | 566/4328 [08:14<54:46,  1.14it/s] 13%|█▎        | 567/4328 [08:15<54:35,  1.15it/s] 13%|█▎        | 568/4328 [08:16<54:33,  1.15it/s] 13%|█▎        | 569/4328 [08:17<54:39,  1.15it/s] 13%|█▎        | 570/4328 [08:18<54:36,  1.15it/s] 13%|█▎        | 571/4328 [08:19<54:33,  1.15it/s] 13%|█▎        | 572/4328 [08:20<54:38,  1.15it/s] 13%|█▎        | 573/4328 [08:20<54:35,  1.15it/s] 13%|█▎        | 574/4328 [08:21<54:32,  1.15it/s] 13%|█▎        | 575/4328 [08:22<54:37,  1.15it/s] 13%|█▎        | 576/4328 [08:23<54:35,  1.15it/s] 13%|█▎        | 577/4328 [08:24<54:48,  1.14it/s] 13%|█▎        | 578/4328 [08:25<54:45,  1.14it/s] 13%|█▎        | 579/4328 [08:26<54:38,  1.14it/s] 13%|█▎        | 580/4328 [08:27<54:39,  1.14it/s] 13%|█▎        | 581/4328 [08:27<54:33,  1.14it/s] 13%|█▎        | 582/4328 [08:28<54:28,  1.15it/s] 13%|█▎        | 583/4328 [08:29<54:31,  1.14it/s] 13%|█▎        | 584/4328 [08:30<54:28,  1.15it/s] 14%|█▎        | 585/4328 [08:31<54:26,  1.15it/s] 14%|█▎        | 586/4328 [08:32<54:40,  1.14it/s] 14%|█▎        | 587/4328 [08:33<54:34,  1.14it/s] 14%|█▎        | 588/4328 [08:34<54:29,  1.14it/s] 14%|█▎        | 589/4328 [08:34<54:33,  1.14it/s] 14%|█▎        | 590/4328 [08:35<54:26,  1.14it/s] 14%|█▎        | 591/4328 [08:36<54:22,  1.15it/s] 14%|█▎        | 592/4328 [08:37<54:25,  1.14it/s] 14%|█▎        | 593/4328 [08:38<54:22,  1.14it/s] 14%|█▎        | 594/4328 [08:39<54:36,  1.14it/s] 14%|█▎        | 595/4328 [08:40<54:32,  1.14it/s] 14%|█▍        | 596/4328 [08:41<54:26,  1.14it/s] 14%|█▍        | 597/4328 [08:41<54:27,  1.14it/s] 14%|█▍        | 598/4328 [08:42<54:23,  1.14it/s] 14%|█▍        | 599/4328 [08:43<54:21,  1.14it/s] 14%|█▍        | 600/4328 [08:44<54:35,  1.14it/s] 14%|█▍        | 601/4328 [08:45<54:27,  1.14it/s] 14%|█▍        | 602/4328 [08:46<54:21,  1.14it/s] 14%|█▍        | 603/4328 [08:47<54:24,  1.14it/s] 14%|█▍        | 604/4328 [08:48<54:16,  1.14it/s] 14%|█▍        | 605/4328 [08:48<54:10,  1.15it/s] 14%|█▍        | 606/4328 [08:49<54:13,  1.14it/s] 14%|█▍        | 607/4328 [08:50<54:10,  1.14it/s] 14%|█▍        | 608/4328 [08:51<54:23,  1.14it/s] 14%|█▍        | 609/4328 [08:52<54:19,  1.14it/s] 14%|█▍        | 610/4328 [08:53<54:13,  1.14it/s] 14%|█▍        | 611/4328 [08:54<54:07,  1.14it/s] 14%|█▍        | 612/4328 [08:55<54:02,  1.15it/s] 14%|█▍        | 613/4328 [08:55<54:15,  1.14it/s] 14%|█▍        | 614/4328 [08:56<54:26,  1.14it/s] 14%|█▍        | 615/4328 [08:57<54:15,  1.14it/s] 14%|█▍        | 616/4328 [08:58<54:06,  1.14it/s] 14%|█▍        | 617/4328 [08:59<54:10,  1.14it/s] 14%|█▍        | 618/4328 [09:00<54:02,  1.14it/s] 14%|█▍        | 619/4328 [09:01<53:56,  1.15it/s] 14%|█▍        | 620/4328 [09:02<54:00,  1.14it/s] 14%|█▍        | 621/4328 [09:02<53:54,  1.15it/s] 14%|█▍        | 622/4328 [09:03<53:49,  1.15it/s] 14%|█▍        | 623/4328 [09:04<53:55,  1.15it/s] 14%|█▍        | 624/4328 [09:05<53:52,  1.15it/s] 14%|█▍        | 625/4328 [09:06<53:48,  1.15it/s] 14%|█▍        | 626/4328 [09:07<53:44,  1.15it/s] 14%|█▍        | 627/4328 [09:08<53:43,  1.15it/s] 15%|█▍        | 628/4328 [09:09<54:10,  1.14it/s] 15%|█▍        | 629/4328 [09:09<54:03,  1.14it/s] 15%|█▍        | 630/4328 [09:10<53:56,  1.14it/s] 15%|█▍        | 631/4328 [09:11<53:57,  1.14it/s] 15%|█▍        | 632/4328 [09:12<53:52,  1.14it/s] 15%|█▍        | 633/4328 [09:13<53:46,  1.15it/s] 15%|█▍        | 634/4328 [09:14<53:47,  1.14it/s] 15%|█▍        | 635/4328 [09:15<53:44,  1.15it/s] 15%|█▍        | 636/4328 [09:16<53:44,  1.15it/s] 15%|█▍        | 637/4328 [09:16<53:57,  1.14it/s] 15%|█▍        | 638/4328 [09:17<53:50,  1.14it/s] 15%|█▍        | 639/4328 [09:18<53:44,  1.14it/s] 15%|█▍        | 640/4328 [09:19<53:50,  1.14it/s] 15%|█▍        | 641/4328 [09:20<53:41,  1.14it/s] 15%|█▍        | 642/4328 [09:21<53:35,  1.15it/s] 15%|█▍        | 643/4328 [09:22<53:37,  1.15it/s] 15%|█▍        | 644/4328 [09:23<53:31,  1.15it/s] 15%|█▍        | 645/4328 [09:23<53:29,  1.15it/s] 15%|█▍        | 646/4328 [09:24<53:33,  1.15it/s] 15%|█▍        | 647/4328 [09:25<53:31,  1.15it/s] 15%|█▍        | 648/4328 [09:26<53:45,  1.14it/s] 15%|█▍        | 649/4328 [09:27<53:42,  1.14it/s] 15%|█▌        | 650/4328 [09:28<53:37,  1.14it/s] 15%|█▌        | 651/4328 [09:29<53:39,  1.14it/s] 15%|█▌        | 652/4328 [09:30<53:34,  1.14it/s] 15%|█▌        | 653/4328 [09:30<53:30,  1.14it/s] 15%|█▌        | 654/4328 [09:31<53:32,  1.14it/s] 15%|█▌        | 655/4328 [09:32<53:29,  1.14it/s] 15%|█▌        | 656/4328 [09:33<53:28,  1.14it/s] 15%|█▌        | 657/4328 [09:34<53:38,  1.14it/s] 15%|█▌        | 658/4328 [09:35<53:31,  1.14it/s] 15%|█▌        | 659/4328 [09:36<53:25,  1.14it/s] 15%|█▌        | 660/4328 [09:37<53:27,  1.14it/s] 15%|█▌        | 661/4328 [09:37<53:21,  1.15it/s] 15%|█▌        | 662/4328 [09:38<53:17,  1.15it/s] 15%|█▌        | 663/4328 [09:39<53:21,  1.14it/s] 15%|█▌        | 664/4328 [09:40<53:17,  1.15it/s] 15%|█▌        | 665/4328 [09:41<53:12,  1.15it/s] 15%|█▌        | 666/4328 [09:42<53:19,  1.14it/s] 15%|█▌        | 667/4328 [09:43<53:16,  1.15it/s] 15%|█▌        | 668/4328 [09:43<53:30,  1.14it/s] 15%|█▌        | 669/4328 [09:44<53:25,  1.14it/s] 15%|█▌        | 670/4328 [09:45<53:18,  1.14it/s] 16%|█▌        | 671/4328 [09:46<53:21,  1.14it/s] 16%|█▌        | 672/4328 [09:47<53:17,  1.14it/s] 16%|█▌        | 673/4328 [09:48<53:12,  1.14it/s] 16%|█▌        | 674/4328 [09:49<53:13,  1.14it/s] 16%|█▌        | 675/4328 [09:50<53:10,  1.15it/s] 16%|█▌        | 676/4328 [09:50<53:08,  1.15it/s] 16%|█▌        | 677/4328 [09:51<53:23,  1.14it/s] 16%|█▌        | 678/4328 [09:52<53:15,  1.14it/s] 16%|█▌        | 679/4328 [09:53<53:09,  1.14it/s] 16%|█▌        | 680/4328 [09:54<53:14,  1.14it/s] 16%|█▌        | 681/4328 [09:55<53:07,  1.14it/s] 16%|█▌        | 682/4328 [09:56<53:01,  1.15it/s] 16%|█▌        | 683/4328 [09:57<53:05,  1.14it/s] 16%|█▌        | 684/4328 [09:57<53:00,  1.15it/s] 16%|█▌        | 685/4328 [09:58<52:56,  1.15it/s] 16%|█▌        | 686/4328 [09:59<53:00,  1.14it/s] 16%|█▌        | 687/4328 [10:00<52:59,  1.15it/s] 16%|█▌        | 688/4328 [10:01<53:10,  1.14it/s] 16%|█▌        | 689/4328 [10:02<53:06,  1.14it/s] 16%|█▌        | 690/4328 [10:03<53:00,  1.14it/s] 16%|█▌        | 691/4328 [10:04<53:05,  1.14it/s] 16%|█▌        | 692/4328 [10:04<52:59,  1.14it/s] 16%|█▌        | 693/4328 [10:05<52:53,  1.15it/s] 16%|█▌        | 694/4328 [10:06<52:55,  1.14it/s] 16%|█▌        | 695/4328 [10:07<52:52,  1.15it/s] 16%|█▌        | 696/4328 [10:08<52:50,  1.15it/s] 16%|█▌        | 697/4328 [10:09<53:04,  1.14it/s] 16%|█▌        | 698/4328 [10:10<52:57,  1.14it/s] 16%|█▌        | 699/4328 [10:11<52:52,  1.14it/s] 16%|█▌        | 700/4328 [10:11<52:56,  1.14it/s] 16%|█▌        | 701/4328 [10:12<52:49,  1.14it/s] 16%|█▌        | 702/4328 [10:13<52:42,  1.15it/s] 16%|█▌        | 703/4328 [10:14<52:47,  1.14it/s] 16%|█▋        | 704/4328 [10:15<52:42,  1.15it/s] 16%|█▋        | 705/4328 [10:16<52:37,  1.15it/s] 16%|█▋        | 706/4328 [10:17<52:43,  1.15it/s] 16%|█▋        | 707/4328 [10:18<52:41,  1.15it/s] 16%|█▋        | 708/4328 [10:18<52:50,  1.14it/s] 16%|█▋        | 709/4328 [10:19<52:46,  1.14it/s] 16%|█▋        | 710/4328 [10:20<52:40,  1.14it/s] 16%|█▋        | 711/4328 [10:21<52:38,  1.15it/s] 16%|█▋        | 712/4328 [10:22<52:36,  1.15it/s] 16%|█▋        | 713/4328 [10:23<52:33,  1.15it/s] 16%|█▋        | 714/4328 [10:24<52:35,  1.15it/s] 17%|█▋        | 715/4328 [10:25<52:33,  1.15it/s] 17%|█▋        | 716/4328 [10:25<52:29,  1.15it/s] 17%|█▋        | 717/4328 [10:26<52:30,  1.15it/s] 17%|█▋        | 718/4328 [10:27<52:28,  1.15it/s] 17%|█▋        | 719/4328 [10:28<52:28,  1.15it/s] 17%|█▋        | 720/4328 [10:29<52:44,  1.14it/s] 17%|█▋        | 721/4328 [10:30<52:37,  1.14it/s] 17%|█▋        | 722/4328 [10:31<52:31,  1.14it/s] 17%|█▋        | 723/4328 [10:32<52:37,  1.14it/s] 17%|█▋        | 724/4328 [10:32<52:29,  1.14it/s] 17%|█▋        | 725/4328 [10:33<52:23,  1.15it/s] 17%|█▋        | 726/4328 [10:34<52:28,  1.14it/s] 17%|█▋        | 727/4328 [10:35<52:22,  1.15it/s] 17%|█▋        | 728/4328 [10:36<52:17,  1.15it/s] 17%|█▋        | 729/4328 [10:37<52:22,  1.15it/s] 17%|█▋        | 730/4328 [10:38<52:16,  1.15it/s] 17%|█▋        | 731/4328 [10:39<52:12,  1.15it/s] 17%|█▋        | 732/4328 [10:39<52:16,  1.15it/s] 17%|█▋        | 733/4328 [10:40<52:12,  1.15it/s] 17%|█▋        | 734/4328 [10:41<52:08,  1.15it/s] 17%|█▋        | 735/4328 [10:42<52:13,  1.15it/s] 17%|█▋        | 736/4328 [10:43<52:11,  1.15it/s] 17%|█▋        | 737/4328 [10:44<52:23,  1.14it/s] 17%|█▋        | 738/4328 [10:45<52:21,  1.14it/s] 17%|█▋        | 739/4328 [10:46<52:15,  1.14it/s] 17%|█▋        | 740/4328 [10:46<52:20,  1.14it/s] 17%|█▋        | 741/4328 [10:47<52:16,  1.14it/s] 17%|█▋        | 742/4328 [10:48<52:10,  1.15it/s] 17%|█▋        | 743/4328 [10:49<52:10,  1.15it/s] 17%|█▋        | 744/4328 [10:50<52:06,  1.15it/s] 17%|█▋        | 745/4328 [10:51<52:02,  1.15it/s] 17%|█▋        | 746/4328 [10:52<52:04,  1.15it/s] 17%|█▋        | 747/4328 [10:53<52:02,  1.15it/s] 17%|█▋        | 748/4328 [10:53<52:00,  1.15it/s] 17%|█▋        | 749/4328 [10:54<52:14,  1.14it/s] 17%|█▋        | 750/4328 [10:55<52:07,  1.14it/s] 17%|█▋        | 751/4328 [10:56<52:04,  1.14it/s] 17%|█▋        | 752/4328 [10:57<52:09,  1.14it/s] 17%|█▋        | 753/4328 [10:58<52:03,  1.14it/s] 17%|█▋        | 754/4328 [10:59<51:58,  1.15it/s] 17%|█▋        | 755/4328 [10:59<52:00,  1.14it/s] 17%|█▋        | 756/4328 [11:00<51:57,  1.15it/s] 17%|█▋        | 757/4328 [11:01<51:53,  1.15it/s] 18%|█▊        | 758/4328 [11:02<51:57,  1.15it/s] 18%|█▊        | 759/4328 [11:03<51:54,  1.15it/s] 18%|█▊        | 760/4328 [11:04<52:06,  1.14it/s] 18%|█▊        | 761/4328 [11:05<52:02,  1.14it/s] 18%|█▊        | 762/4328 [11:06<51:56,  1.14it/s] 18%|█▊        | 763/4328 [11:06<51:59,  1.14it/s] 18%|█▊        | 764/4328 [11:07<51:55,  1.14it/s] 18%|█▊        | 765/4328 [11:08<51:50,  1.15it/s] 18%|█▊        | 766/4328 [11:09<51:51,  1.14it/s] 18%|█▊        | 767/4328 [11:10<51:49,  1.15it/s] 18%|█▊        | 768/4328 [11:11<51:44,  1.15it/s] 18%|█▊        | 769/4328 [11:12<51:45,  1.15it/s] 18%|█▊        | 770/4328 [11:13<51:42,  1.15it/s] 18%|█▊        | 771/4328 [11:13<51:38,  1.15it/s] 18%|█▊        | 772/4328 [11:14<51:36,  1.15it/s] 18%|█▊        | 773/4328 [11:15<51:35,  1.15it/s] 18%|█▊        | 774/4328 [11:16<51:44,  1.14it/s] 18%|█▊        | 775/4328 [11:17<51:56,  1.14it/s] 18%|█▊        | 776/4328 [11:18<51:52,  1.14it/s] 18%|█▊        | 777/4328 [11:19<51:46,  1.14it/s] 18%|█▊        | 778/4328 [11:20<51:49,  1.14it/s] 18%|█▊        | 779/4328 [11:20<51:43,  1.14it/s] 18%|█▊        | 780/4328 [11:21<51:37,  1.15it/s] 18%|█▊        | 781/4328 [11:22<51:32,  1.15it/s] 18%|█▊        | 782/4328 [11:23<51:28,  1.15it/s] 18%|█▊        | 783/4328 [11:24<51:51,  1.14it/s] 18%|█▊        | 784/4328 [11:25<51:48,  1.14it/s] 18%|█▊        | 785/4328 [11:26<51:40,  1.14it/s] 18%|█▊        | 786/4328 [11:27<51:41,  1.14it/s] 18%|█▊        | 787/4328 [11:27<51:34,  1.14it/s] 18%|█▊        | 788/4328 [11:28<51:30,  1.15it/s] 18%|█▊        | 789/4328 [11:29<51:31,  1.14it/s] 18%|█▊        | 790/4328 [11:30<51:29,  1.15it/s] 18%|█▊        | 791/4328 [11:31<51:25,  1.15it/s] 18%|█▊        | 792/4328 [11:32<51:28,  1.15it/s] 18%|█▊        | 793/4328 [11:33<51:27,  1.15it/s] 18%|█▊        | 794/4328 [11:34<51:26,  1.15it/s] 18%|█▊        | 795/4328 [11:34<51:40,  1.14it/s] 18%|█▊        | 796/4328 [11:35<51:34,  1.14it/s] 18%|█▊        | 797/4328 [11:36<51:28,  1.14it/s] 18%|█▊        | 798/4328 [11:37<51:23,  1.14it/s] 18%|█▊        | 799/4328 [11:38<51:19,  1.15it/s] 18%|█▊        | 800/4328 [11:39<51:44,  1.14it/s] 19%|█▊        | 801/4328 [11:40<51:38,  1.14it/s] 19%|█▊        | 802/4328 [11:41<51:31,  1.14it/s] 19%|█▊        | 803/4328 [11:41<51:25,  1.14it/s] 19%|█▊        | 804/4328 [11:42<51:18,  1.14it/s] 19%|█▊        | 805/4328 [11:43<51:29,  1.14it/s] 19%|█▊        | 806/4328 [11:44<51:39,  1.14it/s] 19%|█▊        | 807/4328 [11:45<51:31,  1.14it/s] 19%|█▊        | 808/4328 [11:46<51:24,  1.14it/s] 19%|█▊        | 809/4328 [11:47<51:25,  1.14it/s] 19%|█▊        | 810/4328 [11:48<51:21,  1.14it/s] 19%|█▊        | 811/4328 [11:48<51:28,  1.14it/s] 19%|█▉        | 812/4328 [11:49<51:21,  1.14it/s] 19%|█▉        | 813/4328 [11:50<51:15,  1.14it/s] 19%|█▉        | 814/4328 [11:51<51:16,  1.14it/s] 19%|█▉        | 815/4328 [11:52<51:12,  1.14it/s] 19%|█▉        | 816/4328 [11:53<51:06,  1.15it/s] 19%|█▉        | 817/4328 [11:54<51:08,  1.14it/s] 19%|█▉        | 818/4328 [11:55<51:06,  1.14it/s] 19%|█▉        | 819/4328 [11:55<51:04,  1.15it/s] 19%|█▉        | 820/4328 [11:56<51:18,  1.14it/s] 19%|█▉        | 821/4328 [11:57<51:12,  1.14it/s] 19%|█▉        | 822/4328 [11:58<51:07,  1.14it/s] 19%|█▉        | 823/4328 [11:59<51:10,  1.14it/s] 19%|█▉        | 824/4328 [12:00<51:04,  1.14it/s] 19%|█▉        | 825/4328 [12:01<50:59,  1.14it/s] 19%|█▉        | 826/4328 [12:02<51:01,  1.14it/s] 19%|█▉        | 827/4328 [12:02<51:00,  1.14it/s] 19%|█▉        | 828/4328 [12:03<51:11,  1.14it/s] 19%|█▉        | 829/4328 [12:04<51:07,  1.14it/s] 19%|█▉        | 830/4328 [12:05<51:02,  1.14it/s] 19%|█▉        | 831/4328 [12:06<51:04,  1.14it/s] 19%|█▉        | 832/4328 [12:07<50:59,  1.14it/s] 19%|█▉        | 833/4328 [12:08<50:54,  1.14it/s] 19%|█▉        | 834/4328 [12:09<50:55,  1.14it/s] 19%|█▉        | 835/4328 [12:09<50:52,  1.14it/s] 19%|█▉        | 836/4328 [12:10<50:52,  1.14it/s] 19%|█▉        | 837/4328 [12:11<51:07,  1.14it/s] 19%|█▉        | 838/4328 [12:12<51:00,  1.14it/s] 19%|█▉        | 839/4328 [12:13<50:54,  1.14it/s] 19%|█▉        | 840/4328 [12:14<50:49,  1.14it/s] 19%|█▉        | 841/4328 [12:15<50:44,  1.15it/s] 19%|█▉        | 842/4328 [12:16<51:08,  1.14it/s] 19%|█▉        | 843/4328 [12:16<51:03,  1.14it/s] 20%|█▉        | 844/4328 [12:17<50:54,  1.14it/s] 20%|█▉        | 845/4328 [12:18<50:47,  1.14it/s] 20%|█▉        | 846/4328 [12:19<50:41,  1.14it/s] 20%|█▉        | 847/4328 [12:20<50:53,  1.14it/s] 20%|█▉        | 848/4328 [12:21<51:02,  1.14it/s] 20%|█▉        | 849/4328 [12:22<50:53,  1.14it/s] 20%|█▉        | 850/4328 [12:23<50:47,  1.14it/s] 20%|█▉        | 851/4328 [12:23<50:41,  1.14it/s] 20%|█▉        | 852/4328 [12:24<50:35,  1.15it/s] 20%|█▉        | 853/4328 [12:25<51:02,  1.13it/s] 20%|█▉        | 854/4328 [12:26<50:54,  1.14it/s] 20%|█▉        | 855/4328 [12:27<50:46,  1.14it/s] 20%|█▉        | 856/4328 [12:28<50:40,  1.14it/s] 20%|█▉        | 857/4328 [12:29<50:34,  1.14it/s] 20%|█▉        | 858/4328 [12:30<50:45,  1.14it/s] 20%|█▉        | 859/4328 [12:31<50:53,  1.14it/s] 20%|█▉        | 860/4328 [12:31<50:47,  1.14it/s] 20%|█▉        | 861/4328 [12:32<50:42,  1.14it/s] 20%|█▉        | 862/4328 [12:33<50:34,  1.14it/s] 20%|█▉        | 863/4328 [12:34<50:27,  1.14it/s] 20%|█▉        | 864/4328 [12:35<50:35,  1.14it/s] 20%|█▉        | 865/4328 [12:36<50:31,  1.14it/s] 20%|██        | 866/4328 [12:37<50:26,  1.14it/s] 20%|██        | 867/4328 [12:38<50:27,  1.14it/s] 20%|██        | 868/4328 [12:38<50:24,  1.14it/s] 20%|██        | 869/4328 [12:39<50:19,  1.15it/s] 20%|██        | 870/4328 [12:40<50:21,  1.14it/s] 20%|██        | 871/4328 [12:41<50:20,  1.14it/s] 20%|██        | 872/4328 [12:42<50:18,  1.14it/s] 20%|██        | 873/4328 [12:43<50:32,  1.14it/s] 20%|██        | 874/4328 [12:44<50:26,  1.14it/s] 20%|██        | 875/4328 [12:45<50:22,  1.14it/s] 20%|██        | 876/4328 [12:45<50:26,  1.14it/s] 20%|██        | 877/4328 [12:46<50:22,  1.14it/s] 20%|██        | 878/4328 [12:47<50:18,  1.14it/s] 20%|██        | 879/4328 [12:48<50:13,  1.14it/s] 20%|██        | 880/4328 [12:49<50:09,  1.15it/s] 20%|██        | 881/4328 [12:50<50:35,  1.14it/s] 20%|██        | 882/4328 [12:51<50:31,  1.14it/s] 20%|██        | 883/4328 [12:52<50:26,  1.14it/s] 20%|██        | 884/4328 [12:52<50:34,  1.13it/s] 20%|██        | 885/4328 [12:53<50:23,  1.14it/s] 20%|██        | 886/4328 [12:54<50:16,  1.14it/s] 20%|██        | 887/4328 [12:55<50:09,  1.14it/s] 21%|██        | 888/4328 [12:56<50:03,  1.15it/s] 21%|██        | 889/4328 [12:57<50:29,  1.14it/s] 21%|██        | 890/4328 [12:58<50:22,  1.14it/s] 21%|██        | 891/4328 [12:59<50:14,  1.14it/s] 21%|██        | 892/4328 [12:59<50:08,  1.14it/s] 21%|██        | 893/4328 [13:00<50:02,  1.14it/s] 21%|██        | 894/4328 [13:01<50:13,  1.14it/s] 21%|██        | 895/4328 [13:02<50:22,  1.14it/s] 21%|██        | 896/4328 [13:03<50:13,  1.14it/s] 21%|██        | 897/4328 [13:04<50:06,  1.14it/s] 21%|██        | 898/4328 [13:05<50:00,  1.14it/s] 21%|██        | 899/4328 [13:06<49:55,  1.14it/s] 21%|██        | 900/4328 [13:06<50:18,  1.14it/s] 21%|██        | 901/4328 [13:07<50:12,  1.14it/s] 21%|██        | 902/4328 [13:08<50:03,  1.14it/s] 21%|██        | 903/4328 [13:09<50:02,  1.14it/s] 21%|██        | 904/4328 [13:10<49:59,  1.14it/s] 21%|██        | 905/4328 [13:11<49:54,  1.14it/s] 21%|██        | 906/4328 [13:12<50:04,  1.14it/s] 21%|██        | 907/4328 [13:13<49:56,  1.14it/s] 21%|██        | 908/4328 [13:13<49:52,  1.14it/s] 21%|██        | 909/4328 [13:14<49:53,  1.14it/s] 21%|██        | 910/4328 [13:15<49:48,  1.14it/s] 21%|██        | 911/4328 [13:16<49:45,  1.14it/s] 21%|██        | 912/4328 [13:17<49:47,  1.14it/s] 21%|██        | 913/4328 [13:18<49:45,  1.14it/s] 21%|██        | 914/4328 [13:19<49:54,  1.14it/s] 21%|██        | 915/4328 [13:20<49:51,  1.14it/s] 21%|██        | 916/4328 [13:20<49:45,  1.14it/s] 21%|██        | 917/4328 [13:21<49:40,  1.14it/s] 21%|██        | 918/4328 [13:22<49:37,  1.15it/s] 21%|██        | 919/4328 [13:23<49:49,  1.14it/s] 21%|██▏       | 920/4328 [13:24<50:00,  1.14it/s] 21%|██▏       | 921/4328 [13:25<49:51,  1.14it/s] 21%|██▏       | 922/4328 [13:26<49:43,  1.14it/s] 21%|██▏       | 923/4328 [13:27<49:38,  1.14it/s] 21%|██▏       | 924/4328 [13:27<49:33,  1.14it/s] 21%|██▏       | 925/4328 [13:28<49:43,  1.14it/s] 21%|██▏       | 926/4328 [13:29<49:41,  1.14it/s] 21%|██▏       | 927/4328 [13:30<49:35,  1.14it/s] 21%|██▏       | 928/4328 [13:31<49:42,  1.14it/s] 21%|██▏       | 929/4328 [13:32<49:37,  1.14it/s] 21%|██▏       | 930/4328 [13:33<49:33,  1.14it/s] 22%|██▏       | 931/4328 [13:34<49:28,  1.14it/s] 22%|██▏       | 932/4328 [13:34<49:24,  1.15it/s] 22%|██▏       | 933/4328 [13:35<49:37,  1.14it/s] 22%|██▏       | 934/4328 [13:36<49:47,  1.14it/s] 22%|██▏       | 935/4328 [13:37<49:39,  1.14it/s] 22%|██▏       | 936/4328 [13:38<49:33,  1.14it/s] 22%|██▏       | 937/4328 [13:39<49:27,  1.14it/s] 22%|██▏       | 938/4328 [13:40<49:22,  1.14it/s] 22%|██▏       | 939/4328 [13:41<49:45,  1.14it/s] 22%|██▏       | 940/4328 [13:42<49:39,  1.14it/s] 22%|██▏       | 941/4328 [13:42<49:35,  1.14it/s] 22%|██▏       | 942/4328 [13:43<49:28,  1.14it/s] 22%|██▏       | 943/4328 [13:44<49:21,  1.14it/s] 22%|██▏       | 944/4328 [13:45<49:16,  1.14it/s] 22%|██▏       | 945/4328 [13:46<49:24,  1.14it/s] 22%|██▏       | 946/4328 [13:47<49:18,  1.14it/s] 22%|██▏       | 947/4328 [13:48<49:13,  1.14it/s] 22%|██▏       | 948/4328 [13:48<49:17,  1.14it/s] 22%|██▏       | 949/4328 [13:49<49:13,  1.14it/s] 22%|██▏       | 950/4328 [13:50<49:09,  1.15it/s] 22%|██▏       | 951/4328 [13:51<49:14,  1.14it/s] 22%|██▏       | 952/4328 [13:52<49:11,  1.14it/s] 22%|██▏       | 953/4328 [13:53<49:18,  1.14it/s] 22%|██▏       | 954/4328 [13:54<49:15,  1.14it/s] 22%|██▏       | 955/4328 [13:55<49:08,  1.14it/s] 22%|██▏       | 956/4328 [13:55<49:04,  1.15it/s] 22%|██▏       | 957/4328 [13:56<49:01,  1.15it/s] 22%|██▏       | 958/4328 [13:57<49:13,  1.14it/s] 22%|██▏       | 959/4328 [13:58<49:23,  1.14it/s] 22%|██▏       | 960/4328 [13:59<49:16,  1.14it/s] 22%|██▏       | 961/4328 [14:00<49:09,  1.14it/s] 22%|██▏       | 962/4328 [14:01<49:04,  1.14it/s] 22%|██▏       | 963/4328 [14:02<48:59,  1.14it/s] 22%|██▏       | 964/4328 [14:03<49:10,  1.14it/s] 22%|██▏       | 965/4328 [14:03<49:09,  1.14it/s] 22%|██▏       | 966/4328 [14:04<49:03,  1.14it/s] 22%|██▏       | 967/4328 [14:05<49:07,  1.14it/s] 22%|██▏       | 968/4328 [14:06<49:03,  1.14it/s] 22%|██▏       | 969/4328 [14:07<48:59,  1.14it/s] 22%|██▏       | 970/4328 [14:08<48:59,  1.14it/s] 22%|██▏       | 971/4328 [14:09<48:58,  1.14it/s] 22%|██▏       | 972/4328 [14:10<48:55,  1.14it/s] 22%|██▏       | 973/4328 [14:10<49:04,  1.14it/s] 23%|██▎       | 974/4328 [14:11<48:59,  1.14it/s] 23%|██▎       | 975/4328 [14:12<48:56,  1.14it/s] 23%|██▎       | 976/4328 [14:13<48:52,  1.14it/s] 23%|██▎       | 977/4328 [14:14<48:47,  1.14it/s] 23%|██▎       | 978/4328 [14:15<48:59,  1.14it/s] 23%|██▎       | 979/4328 [14:16<48:59,  1.14it/s] 23%|██▎       | 980/4328 [14:17<48:54,  1.14it/s] 23%|██▎       | 981/4328 [14:17<48:58,  1.14it/s] 23%|██▎       | 982/4328 [14:18<48:54,  1.14it/s] 23%|██▎       | 983/4328 [14:19<48:48,  1.14it/s] 23%|██▎       | 984/4328 [14:20<48:45,  1.14it/s] 23%|██▎       | 985/4328 [14:21<48:41,  1.14it/s] 23%|██▎       | 986/4328 [14:22<48:54,  1.14it/s] 23%|██▎       | 987/4328 [14:23<49:01,  1.14it/s] 23%|██▎       | 988/4328 [14:24<48:55,  1.14it/s] 23%|██▎       | 989/4328 [14:24<49:02,  1.13it/s] 23%|██▎       | 990/4328 [14:25<48:56,  1.14it/s] 23%|██▎       | 991/4328 [14:26<48:49,  1.14it/s] 23%|██▎       | 992/4328 [14:27<48:43,  1.14it/s] 23%|██▎       | 993/4328 [14:28<48:37,  1.14it/s] 23%|██▎       | 994/4328 [14:29<48:48,  1.14it/s] 23%|██▎       | 995/4328 [14:30<48:56,  1.13it/s] 23%|██▎       | 996/4328 [14:31<48:53,  1.14it/s] 23%|██▎       | 997/4328 [14:31<48:59,  1.13it/s] 23%|██▎       | 998/4328 [14:32<48:51,  1.14it/s] 23%|██▎       | 999/4328 [14:33<48:44,  1.14it/s] 23%|██▎       | 1000/4328 [14:34<48:36,  1.14it/s] 23%|██▎       | 1001/4328 [14:35<48:30,  1.14it/s] 23%|██▎       | 1002/4328 [14:36<48:41,  1.14it/s] 23%|██▎       | 1003/4328 [14:37<48:50,  1.13it/s] 23%|██▎       | 1004/4328 [14:38<48:47,  1.14it/s] 23%|██▎       | 1005/4328 [14:38<48:50,  1.13it/s] 23%|██▎       | 1006/4328 [14:39<48:42,  1.14it/s] 23%|██▎       | 1007/4328 [14:40<48:34,  1.14it/s] 23%|██▎       | 1008/4328 [14:41<48:33,  1.14it/s] 23%|██▎       | 1009/4328 [14:42<48:30,  1.14it/s] 23%|██▎       | 1010/4328 [14:43<48:28,  1.14it/s] 23%|██▎       | 1011/4328 [14:44<48:35,  1.14it/s] 23%|██▎       | 1012/4328 [14:45<48:29,  1.14it/s] 23%|██▎       | 1013/4328 [14:45<48:24,  1.14it/s] 23%|██▎       | 1014/4328 [14:46<48:19,  1.14it/s] 23%|██▎       | 1015/4328 [14:47<48:14,  1.14it/s] 23%|██▎       | 1016/4328 [14:48<48:39,  1.13it/s] 23%|██▎       | 1017/4328 [14:49<48:33,  1.14it/s] 24%|██▎       | 1018/4328 [14:50<48:28,  1.14it/s] 24%|██▎       | 1019/4328 [14:51<48:23,  1.14it/s] 24%|██▎       | 1020/4328 [14:52<48:16,  1.14it/s] 24%|██▎       | 1021/4328 [14:53<48:12,  1.14it/s] 24%|██▎       | 1022/4328 [14:53<48:19,  1.14it/s] 24%|██▎       | 1023/4328 [14:54<48:14,  1.14it/s] 24%|██▎       | 1024/4328 [14:55<48:10,  1.14it/s] 24%|██▎       | 1025/4328 [14:56<48:13,  1.14it/s] 24%|██▎       | 1026/4328 [14:57<48:12,  1.14it/s] 24%|██▎       | 1027/4328 [14:58<48:20,  1.14it/s] 24%|██▍       | 1028/4328 [14:59<48:15,  1.14it/s] 24%|██▍       | 1029/4328 [15:00<48:09,  1.14it/s] 24%|██▍       | 1030/4328 [15:00<48:11,  1.14it/s] 24%|██▍       | 1031/4328 [15:01<48:08,  1.14it/s] 24%|██▍       | 1032/4328 [15:02<48:07,  1.14it/s] 24%|██▍       | 1033/4328 [15:03<48:20,  1.14it/s] 24%|██▍       | 1034/4328 [15:04<48:12,  1.14it/s] 24%|██▍       | 1035/4328 [15:05<48:06,  1.14it/s] 24%|██▍       | 1036/4328 [15:06<48:01,  1.14it/s] 24%|██▍       | 1037/4328 [15:07<47:58,  1.14it/s] 24%|██▍       | 1038/4328 [15:07<48:23,  1.13it/s] 24%|██▍       | 1039/4328 [15:08<48:19,  1.13it/s] 24%|██▍       | 1040/4328 [15:09<48:14,  1.14it/s] 24%|██▍       | 1041/4328 [15:10<48:09,  1.14it/s] 24%|██▍       | 1042/4328 [15:11<48:01,  1.14it/s] 24%|██▍       | 1043/4328 [15:12<47:53,  1.14it/s] 24%|██▍       | 1044/4328 [15:13<47:58,  1.14it/s] 24%|██▍       | 1045/4328 [15:14<47:53,  1.14it/s] 24%|██▍       | 1046/4328 [15:14<47:50,  1.14it/s] 24%|██▍       | 1047/4328 [15:15<47:52,  1.14it/s] 24%|██▍       | 1048/4328 [15:16<47:50,  1.14it/s] 24%|██▍       | 1049/4328 [15:17<47:57,  1.14it/s] 24%|██▍       | 1050/4328 [15:18<47:55,  1.14it/s] 24%|██▍       | 1051/4328 [15:19<47:50,  1.14it/s] 24%|██▍       | 1052/4328 [15:20<47:50,  1.14it/s] 24%|██▍       | 1053/4328 [15:21<47:48,  1.14it/s] 24%|██▍       | 1054/4328 [15:21<47:46,  1.14it/s] 24%|██▍       | 1055/4328 [15:22<47:55,  1.14it/s] 24%|██▍       | 1056/4328 [15:23<47:48,  1.14it/s] 24%|██▍       | 1057/4328 [15:24<47:44,  1.14it/s] 24%|██▍       | 1058/4328 [15:25<47:46,  1.14it/s] 24%|██▍       | 1059/4328 [15:26<47:44,  1.14it/s] 24%|██▍       | 1060/4328 [15:27<47:52,  1.14it/s] 25%|██▍       | 1061/4328 [15:28<47:47,  1.14it/s] 25%|██▍       | 1062/4328 [15:28<47:44,  1.14it/s] 25%|██▍       | 1063/4328 [15:29<47:45,  1.14it/s] 25%|██▍       | 1064/4328 [15:30<47:42,  1.14it/s] 25%|██▍       | 1065/4328 [15:31<47:39,  1.14it/s] 25%|██▍       | 1066/4328 [15:32<47:44,  1.14it/s] 25%|██▍       | 1067/4328 [15:33<47:38,  1.14it/s] 25%|██▍       | 1068/4328 [15:34<47:35,  1.14it/s] 25%|██▍       | 1069/4328 [15:35<47:30,  1.14it/s] 25%|██▍       | 1070/4328 [15:35<47:26,  1.14it/s] 25%|██▍       | 1071/4328 [15:36<47:38,  1.14it/s] 25%|██▍       | 1072/4328 [15:37<47:37,  1.14it/s] 25%|██▍       | 1073/4328 [15:38<47:31,  1.14it/s] 25%|██▍       | 1074/4328 [15:39<47:34,  1.14it/s] 25%|██▍       | 1075/4328 [15:40<47:30,  1.14it/s] 25%|██▍       | 1076/4328 [15:41<47:29,  1.14it/s] 25%|██▍       | 1077/4328 [15:42<47:40,  1.14it/s] 25%|██▍       | 1078/4328 [15:42<47:32,  1.14it/s] 25%|██▍       | 1079/4328 [15:43<47:26,  1.14it/s] 25%|██▍       | 1080/4328 [15:44<47:29,  1.14it/s] 25%|██▍       | 1081/4328 [15:45<47:25,  1.14it/s] 25%|██▌       | 1082/4328 [15:46<47:35,  1.14it/s] 25%|██▌       | 1083/4328 [15:47<47:32,  1.14it/s] 25%|██▌       | 1084/4328 [15:48<47:25,  1.14it/s] 25%|██▌       | 1085/4328 [15:49<47:19,  1.14it/s] 25%|██▌       | 1086/4328 [15:50<47:15,  1.14it/s] 25%|██▌       | 1087/4328 [15:50<47:24,  1.14it/s] 25%|██▌       | 1088/4328 [15:51<47:35,  1.13it/s] 25%|██▌       | 1089/4328 [15:52<47:27,  1.14it/s] 25%|██▌       | 1090/4328 [15:53<47:21,  1.14it/s] 25%|██▌       | 1091/4328 [15:54<47:15,  1.14it/s] 25%|██▌       | 1092/4328 [15:55<47:10,  1.14it/s] 25%|██▌       | 1093/4328 [15:56<47:30,  1.13it/s] 25%|██▌       | 1094/4328 [15:57<47:24,  1.14it/s] 25%|██▌       | 1095/4328 [15:57<47:20,  1.14it/s] 25%|██▌       | 1096/4328 [15:58<47:27,  1.14it/s] 25%|██▌       | 1097/4328 [15:59<47:18,  1.14it/s] 25%|██▌       | 1098/4328 [16:00<47:11,  1.14it/s] 25%|██▌       | 1099/4328 [16:01<47:06,  1.14it/s] 25%|██▌       | 1100/4328 [16:02<47:02,  1.14it/s] 25%|██▌       | 1101/4328 [16:03<47:24,  1.13it/s] 25%|██▌       | 1102/4328 [16:04<47:20,  1.14it/s] 25%|██▌       | 1103/4328 [16:04<47:13,  1.14it/s] 26%|██▌       | 1104/4328 [16:05<47:08,  1.14it/s] 26%|██▌       | 1105/4328 [16:06<47:02,  1.14it/s] 26%|██▌       | 1106/4328 [16:07<47:12,  1.14it/s] 26%|██▌       | 1107/4328 [16:08<47:14,  1.14it/s] 26%|██▌       | 1108/4328 [16:09<47:08,  1.14it/s] 26%|██▌       | 1109/4328 [16:10<47:14,  1.14it/s] 26%|██▌       | 1110/4328 [16:11<47:07,  1.14it/s] 26%|██▌       | 1111/4328 [16:11<47:01,  1.14it/s] 26%|██▌       | 1112/4328 [16:12<46:55,  1.14it/s] 26%|██▌       | 1113/4328 [16:13<46:51,  1.14it/s] 26%|██▌       | 1114/4328 [16:14<47:02,  1.14it/s] 26%|██▌       | 1115/4328 [16:15<47:08,  1.14it/s] 26%|██▌       | 1116/4328 [16:16<47:02,  1.14it/s] 26%|██▌       | 1117/4328 [16:17<46:58,  1.14it/s] 26%|██▌       | 1118/4328 [16:18<46:52,  1.14it/s] 26%|██▌       | 1119/4328 [16:18<46:46,  1.14it/s] 26%|██▌       | 1120/4328 [16:19<47:04,  1.14it/s] 26%|██▌       | 1121/4328 [16:20<46:59,  1.14it/s] 26%|██▌       | 1122/4328 [16:21<46:54,  1.14it/s] 26%|██▌       | 1123/4328 [16:22<47:00,  1.14it/s] 26%|██▌       | 1124/4328 [16:23<46:52,  1.14it/s] 26%|██▌       | 1125/4328 [16:24<46:46,  1.14it/s] 26%|██▌       | 1126/4328 [16:25<46:40,  1.14it/s] 26%|██▌       | 1127/4328 [16:25<46:35,  1.15it/s] 26%|██▌       | 1128/4328 [16:26<46:46,  1.14it/s] 26%|██▌       | 1129/4328 [16:27<46:45,  1.14it/s] 26%|██▌       | 1130/4328 [16:28<46:41,  1.14it/s] 26%|██▌       | 1131/4328 [16:29<46:44,  1.14it/s] 26%|██▌       | 1132/4328 [16:30<46:41,  1.14it/s] 26%|██▌       | 1133/4328 [16:31<46:37,  1.14it/s] 26%|██▌       | 1134/4328 [16:32<46:33,  1.14it/s] 26%|██▌       | 1135/4328 [16:33<46:29,  1.14it/s] 26%|██▌       | 1136/4328 [16:33<46:39,  1.14it/s] 26%|██▋       | 1137/4328 [16:34<46:50,  1.14it/s] 26%|██▋       | 1138/4328 [16:35<46:44,  1.14it/s] 26%|██▋       | 1139/4328 [16:36<46:37,  1.14it/s] 26%|██▋       | 1140/4328 [16:37<46:31,  1.14it/s] 26%|██▋       | 1141/4328 [16:38<46:26,  1.14it/s] 26%|██▋       | 1142/4328 [16:39<46:31,  1.14it/s] 26%|██▋       | 1143/4328 [16:40<46:29,  1.14it/s] 26%|██▋       | 1144/4328 [16:40<46:24,  1.14it/s] 26%|██▋       | 1145/4328 [16:41<46:27,  1.14it/s] 26%|██▋       | 1146/4328 [16:42<46:26,  1.14it/s] 27%|██▋       | 1147/4328 [16:43<46:24,  1.14it/s] 27%|██▋       | 1148/4328 [16:44<46:22,  1.14it/s] 27%|██▋       | 1149/4328 [16:45<46:18,  1.14it/s] 27%|██▋       | 1150/4328 [16:46<46:14,  1.15it/s] 27%|██▋       | 1151/4328 [16:47<46:24,  1.14it/s] 27%|██▋       | 1152/4328 [16:47<46:20,  1.14it/s] 27%|██▋       | 1153/4328 [16:48<46:17,  1.14it/s] 27%|██▋       | 1154/4328 [16:49<46:15,  1.14it/s] 27%|██▋       | 1155/4328 [16:50<46:12,  1.14it/s] 27%|██▋       | 1156/4328 [16:51<46:35,  1.13it/s] 27%|██▋       | 1157/4328 [16:52<46:30,  1.14it/s] 27%|██▋       | 1158/4328 [16:53<46:26,  1.14it/s] 27%|██▋       | 1159/4328 [16:54<46:22,  1.14it/s] 27%|██▋       | 1160/4328 [16:54<46:14,  1.14it/s] 27%|██▋       | 1161/4328 [16:55<46:08,  1.14it/s] 27%|██▋       | 1162/4328 [16:56<46:15,  1.14it/s] 27%|██▋       | 1163/4328 [16:57<46:10,  1.14it/s] 27%|██▋       | 1164/4328 [16:58<46:06,  1.14it/s] 27%|██▋       | 1165/4328 [16:59<46:09,  1.14it/s] 27%|██▋       | 1166/4328 [17:00<46:07,  1.14it/s] 27%|██▋       | 1167/4328 [17:01<46:14,  1.14it/s] 27%|██▋       | 1168/4328 [17:01<46:11,  1.14it/s] 27%|██▋       | 1169/4328 [17:02<46:05,  1.14it/s] 27%|██▋       | 1170/4328 [17:03<46:03,  1.14it/s] 27%|██▋       | 1171/4328 [17:04<45:57,  1.14it/s] 27%|██▋       | 1172/4328 [17:05<45:54,  1.15it/s] 27%|██▋       | 1173/4328 [17:06<46:10,  1.14it/s] 27%|██▋       | 1174/4328 [17:07<46:05,  1.14it/s] 27%|██▋       | 1175/4328 [17:08<46:01,  1.14it/s] 27%|██▋       | 1176/4328 [17:08<46:06,  1.14it/s] 27%|██▋       | 1177/4328 [17:09<46:00,  1.14it/s] 27%|██▋       | 1178/4328 [17:10<45:56,  1.14it/s] 27%|██▋       | 1179/4328 [17:11<45:53,  1.14it/s] 27%|██▋       | 1180/4328 [17:12<45:48,  1.15it/s] 27%|██▋       | 1181/4328 [17:13<45:59,  1.14it/s] 27%|██▋       | 1182/4328 [17:14<45:56,  1.14it/s] 27%|██▋       | 1183/4328 [17:15<45:54,  1.14it/s] 27%|██▋       | 1184/4328 [17:15<45:56,  1.14it/s] 27%|██▋       | 1185/4328 [17:16<45:53,  1.14it/s] 27%|██▋       | 1186/4328 [17:17<45:49,  1.14it/s] 27%|██▋       | 1187/4328 [17:18<45:44,  1.14it/s] 27%|██▋       | 1188/4328 [17:19<45:40,  1.15it/s] 27%|██▋       | 1189/4328 [17:20<45:52,  1.14it/s] 27%|██▋       | 1190/4328 [17:21<46:02,  1.14it/s] 28%|██▊       | 1191/4328 [17:22<45:59,  1.14it/s] 28%|██▊       | 1192/4328 [17:22<46:04,  1.13it/s] 28%|██▊       | 1193/4328 [17:23<45:58,  1.14it/s] 28%|██▊       | 1194/4328 [17:24<45:50,  1.14it/s] 28%|██▊       | 1195/4328 [17:25<45:44,  1.14it/s] 28%|██▊       | 1196/4328 [17:26<45:39,  1.14it/s] 28%|██▊       | 1197/4328 [17:27<45:49,  1.14it/s] 28%|██▊       | 1198/4328 [17:28<45:56,  1.14it/s] 28%|██▊       | 1199/4328 [17:29<45:53,  1.14it/s] 28%|██▊       | 1200/4328 [17:29<45:59,  1.13it/s] 28%|██▊       | 1201/4328 [17:30<45:50,  1.14it/s] 28%|██▊       | 1202/4328 [17:31<45:44,  1.14it/s] 28%|██▊       | 1203/4328 [17:32<45:41,  1.14it/s] 28%|██▊       | 1204/4328 [17:33<45:37,  1.14it/s] 28%|██▊       | 1205/4328 [17:34<45:34,  1.14it/s] 28%|██▊       | 1206/4328 [17:35<45:44,  1.14it/s] 28%|██▊       | 1207/4328 [17:36<45:36,  1.14it/s] 28%|██▊       | 1208/4328 [17:36<45:30,  1.14it/s] 28%|██▊       | 1209/4328 [17:37<45:32,  1.14it/s] 28%|██▊       | 1210/4328 [17:38<45:28,  1.14it/s] 28%|██▊       | 1211/4328 [17:39<45:38,  1.14it/s] 28%|██▊       | 1212/4328 [17:40<45:35,  1.14it/s] 28%|██▊       | 1213/4328 [17:41<45:30,  1.14it/s] 28%|██▊       | 1214/4328 [17:42<45:24,  1.14it/s] 28%|██▊       | 1215/4328 [17:43<45:19,  1.14it/s] 28%|██▊       | 1216/4328 [17:44<45:29,  1.14it/s] 28%|██▊       | 1217/4328 [17:44<45:37,  1.14it/s] 28%|██▊       | 1218/4328 [17:45<45:29,  1.14it/s] 28%|██▊       | 1219/4328 [17:46<45:23,  1.14it/s] 28%|██▊       | 1220/4328 [17:47<45:18,  1.14it/s] 28%|██▊       | 1221/4328 [17:48<45:12,  1.15it/s] 28%|██▊       | 1222/4328 [17:49<45:35,  1.14it/s] 28%|██▊       | 1223/4328 [17:50<45:29,  1.14it/s] 28%|██▊       | 1224/4328 [17:51<45:22,  1.14it/s] 28%|██▊       | 1225/4328 [17:51<45:16,  1.14it/s] 28%|██▊       | 1226/4328 [17:52<45:11,  1.14it/s] 28%|██▊       | 1227/4328 [17:53<45:21,  1.14it/s] 28%|██▊       | 1228/4328 [17:54<45:28,  1.14it/s] 28%|██▊       | 1229/4328 [17:55<45:20,  1.14it/s] 28%|██▊       | 1230/4328 [17:56<45:15,  1.14it/s] 28%|██▊       | 1231/4328 [17:57<45:08,  1.14it/s] 28%|██▊       | 1232/4328 [17:58<45:03,  1.15it/s] 28%|██▊       | 1233/4328 [17:58<45:13,  1.14it/s] 29%|██▊       | 1234/4328 [17:59<45:11,  1.14it/s] 29%|██▊       | 1235/4328 [18:00<45:06,  1.14it/s] 29%|██▊       | 1236/4328 [18:01<45:10,  1.14it/s] 29%|██▊       | 1237/4328 [18:02<45:06,  1.14it/s] 29%|██▊       | 1238/4328 [18:03<45:02,  1.14it/s] 29%|██▊       | 1239/4328 [18:04<45:01,  1.14it/s] 29%|██▊       | 1240/4328 [18:05<45:00,  1.14it/s] 29%|██▊       | 1241/4328 [18:05<44:57,  1.14it/s] 29%|██▊       | 1242/4328 [18:06<44:54,  1.15it/s] 29%|██▊       | 1243/4328 [18:07<44:51,  1.15it/s] 29%|██▊       | 1244/4328 [18:08<45:02,  1.14it/s] 29%|██▉       | 1245/4328 [18:09<45:09,  1.14it/s] 29%|██▉       | 1246/4328 [18:10<45:03,  1.14it/s] 29%|██▉       | 1247/4328 [18:11<44:58,  1.14it/s] 29%|██▉       | 1248/4328 [18:12<44:52,  1.14it/s] 29%|██▉       | 1249/4328 [18:12<44:49,  1.14it/s] 29%|██▉       | 1250/4328 [18:13<45:08,  1.14it/s] 29%|██▉       | 1251/4328 [18:14<45:00,  1.14it/s] 29%|██▉       | 1252/4328 [18:15<44:55,  1.14it/s] 29%|██▉       | 1253/4328 [18:16<44:50,  1.14it/s] 29%|██▉       | 1254/4328 [18:17<44:44,  1.15it/s] 29%|██▉       | 1255/4328 [18:18<44:54,  1.14it/s] 29%|██▉       | 1256/4328 [18:19<45:03,  1.14it/s] 29%|██▉       | 1257/4328 [18:19<44:55,  1.14it/s] 29%|██▉       | 1258/4328 [18:20<44:49,  1.14it/s] 29%|██▉       | 1259/4328 [18:21<44:43,  1.14it/s] 29%|██▉       | 1260/4328 [18:22<44:39,  1.15it/s] 29%|██▉       | 1261/4328 [18:23<45:01,  1.14it/s] 29%|██▉       | 1262/4328 [18:24<44:55,  1.14it/s] 29%|██▉       | 1263/4328 [18:25<44:45,  1.14it/s] 29%|██▉       | 1264/4328 [18:26<44:44,  1.14it/s] 29%|██▉       | 1265/4328 [18:26<44:43,  1.14it/s] 29%|██▉       | 1266/4328 [18:27<44:37,  1.14it/s] 29%|██▉       | 1267/4328 [18:28<44:49,  1.14it/s] 29%|██▉       | 1268/4328 [18:29<44:42,  1.14it/s] 29%|██▉       | 1269/4328 [18:30<44:37,  1.14it/s] 29%|██▉       | 1270/4328 [18:31<44:31,  1.14it/s] 29%|██▉       | 1271/4328 [18:32<44:27,  1.15it/s] 29%|██▉       | 1272/4328 [18:33<44:50,  1.14it/s] 29%|██▉       | 1273/4328 [18:33<44:45,  1.14it/s] 29%|██▉       | 1274/4328 [18:34<44:37,  1.14it/s] 29%|██▉       | 1275/4328 [18:35<44:31,  1.14it/s] 29%|██▉       | 1276/4328 [18:36<44:26,  1.14it/s] 30%|██▉       | 1277/4328 [18:37<44:36,  1.14it/s] 30%|██▉       | 1278/4328 [18:38<44:43,  1.14it/s] 30%|██▉       | 1279/4328 [18:39<44:34,  1.14it/s] 30%|██▉       | 1280/4328 [18:40<44:29,  1.14it/s] 30%|██▉       | 1281/4328 [18:40<44:29,  1.14it/s] 30%|██▉       | 1282/4328 [18:41<44:22,  1.14it/s] 30%|██▉       | 1283/4328 [18:42<44:18,  1.15it/s] 30%|██▉       | 1284/4328 [18:43<44:21,  1.14it/s] 30%|██▉       | 1285/4328 [18:44<44:17,  1.15it/s] 30%|██▉       | 1286/4328 [18:45<44:13,  1.15it/s] 30%|██▉       | 1287/4328 [18:46<44:16,  1.14it/s] 30%|██▉       | 1288/4328 [18:47<44:15,  1.14it/s] 30%|██▉       | 1289/4328 [18:47<44:26,  1.14it/s] 30%|██▉       | 1290/4328 [18:48<44:24,  1.14it/s] 30%|██▉       | 1291/4328 [18:49<44:19,  1.14it/s] 30%|██▉       | 1292/4328 [18:50<44:12,  1.14it/s] 30%|██▉       | 1293/4328 [18:51<44:07,  1.15it/s] 30%|██▉       | 1294/4328 [18:52<44:03,  1.15it/s] 30%|██▉       | 1295/4328 [18:53<44:18,  1.14it/s] 30%|██▉       | 1296/4328 [18:54<44:15,  1.14it/s] 30%|██▉       | 1297/4328 [18:54<44:13,  1.14it/s] 30%|██▉       | 1298/4328 [18:55<44:21,  1.14it/s] 30%|███       | 1299/4328 [18:56<44:13,  1.14it/s] 30%|███       | 1300/4328 [18:57<44:07,  1.14it/s] 30%|███       | 1301/4328 [18:58<44:09,  1.14it/s] 30%|███       | 1302/4328 [18:59<44:06,  1.14it/s] 30%|███       | 1303/4328 [19:00<44:16,  1.14it/s] 30%|███       | 1304/4328 [19:01<44:12,  1.14it/s] 30%|███       | 1305/4328 [19:01<44:06,  1.14it/s] 30%|███       | 1306/4328 [19:02<44:05,  1.14it/s] 30%|███       | 1307/4328 [19:03<44:02,  1.14it/s] 30%|███       | 1308/4328 [19:04<43:59,  1.14it/s] 30%|███       | 1309/4328 [19:05<43:59,  1.14it/s] 30%|███       | 1310/4328 [19:06<43:56,  1.14it/s] 30%|███       | 1311/4328 [19:07<43:56,  1.14it/s] 30%|███       | 1312/4328 [19:08<44:07,  1.14it/s] 30%|███       | 1313/4328 [19:08<44:00,  1.14it/s] 30%|███       | 1314/4328 [19:09<43:54,  1.14it/s] 30%|███       | 1315/4328 [19:10<43:57,  1.14it/s] 30%|███       | 1316/4328 [19:11<43:53,  1.14it/s] 30%|███       | 1317/4328 [19:12<43:50,  1.14it/s] 30%|███       | 1318/4328 [19:13<43:53,  1.14it/s] 30%|███       | 1319/4328 [19:14<43:49,  1.14it/s] 30%|███       | 1320/4328 [19:15<43:57,  1.14it/s] 31%|███       | 1321/4328 [19:15<43:49,  1.14it/s] 31%|███       | 1322/4328 [19:16<43:45,  1.15it/s] 31%|███       | 1323/4328 [19:17<43:49,  1.14it/s] 31%|███       | 1324/4328 [19:18<43:46,  1.14it/s] 31%|███       | 1325/4328 [19:19<43:42,  1.15it/s] 31%|███       | 1326/4328 [19:20<43:45,  1.14it/s] 31%|███       | 1327/4328 [19:21<43:42,  1.14it/s] 31%|███       | 1328/4328 [19:22<43:41,  1.14it/s] 31%|███       | 1329/4328 [19:22<43:48,  1.14it/s] 31%|███       | 1330/4328 [19:23<43:43,  1.14it/s] 31%|███       | 1331/4328 [19:24<43:39,  1.14it/s] 31%|███       | 1332/4328 [19:25<43:42,  1.14it/s] 31%|███       | 1333/4328 [19:26<43:38,  1.14it/s] 31%|███       | 1334/4328 [19:27<43:34,  1.15it/s] 31%|███       | 1335/4328 [19:28<43:36,  1.14it/s] 31%|███       | 1336/4328 [19:29<43:34,  1.14it/s] 31%|███       | 1337/4328 [19:29<43:44,  1.14it/s] 31%|███       | 1338/4328 [19:30<43:41,  1.14it/s] 31%|███       | 1339/4328 [19:31<43:35,  1.14it/s] 31%|███       | 1340/4328 [19:32<43:37,  1.14it/s] 31%|███       | 1341/4328 [19:33<43:33,  1.14it/s] 31%|███       | 1342/4328 [19:34<43:28,  1.14it/s] 31%|███       | 1343/4328 [19:35<43:26,  1.15it/s] 31%|███       | 1344/4328 [19:36<43:22,  1.15it/s] 31%|███       | 1345/4328 [19:36<43:31,  1.14it/s] 31%|███       | 1346/4328 [19:37<43:42,  1.14it/s] 31%|███       | 1347/4328 [19:38<43:34,  1.14it/s] 31%|███       | 1348/4328 [19:39<43:27,  1.14it/s] 31%|███       | 1349/4328 [19:40<43:29,  1.14it/s] 31%|███       | 1350/4328 [19:41<43:25,  1.14it/s] 31%|███       | 1351/4328 [19:42<43:33,  1.14it/s] 31%|███       | 1352/4328 [19:43<43:28,  1.14it/s] 31%|███▏      | 1353/4328 [19:43<43:22,  1.14it/s] 31%|███▏      | 1354/4328 [19:44<43:23,  1.14it/s] 31%|███▏      | 1355/4328 [19:45<43:20,  1.14it/s] 31%|███▏      | 1356/4328 [19:46<43:16,  1.14it/s] 31%|███▏      | 1357/4328 [19:47<43:14,  1.14it/s] 31%|███▏      | 1358/4328 [19:48<43:10,  1.15it/s] 31%|███▏      | 1359/4328 [19:49<43:20,  1.14it/s] 31%|███▏      | 1360/4328 [19:50<43:28,  1.14it/s] 31%|███▏      | 1361/4328 [19:50<43:21,  1.14it/s] 31%|███▏      | 1362/4328 [19:51<43:17,  1.14it/s] 31%|███▏      | 1363/4328 [19:52<43:11,  1.14it/s] 32%|███▏      | 1364/4328 [19:53<43:07,  1.15it/s] 32%|███▏      | 1365/4328 [19:54<43:29,  1.14it/s] 32%|███▏      | 1366/4328 [19:55<43:23,  1.14it/s] 32%|███▏      | 1367/4328 [19:56<43:16,  1.14it/s] 32%|███▏      | 1368/4328 [19:57<43:12,  1.14it/s] 32%|███▏      | 1369/4328 [19:57<43:06,  1.14it/s] 32%|███▏      | 1370/4328 [19:58<43:14,  1.14it/s] 32%|███▏      | 1371/4328 [19:59<43:23,  1.14it/s] 32%|███▏      | 1372/4328 [20:00<43:16,  1.14it/s] 32%|███▏      | 1373/4328 [20:01<43:11,  1.14it/s] 32%|███▏      | 1374/4328 [20:02<43:05,  1.14it/s] 32%|███▏      | 1375/4328 [20:03<43:00,  1.14it/s] 32%|███▏      | 1376/4328 [20:04<43:06,  1.14it/s] 32%|███▏      | 1377/4328 [20:04<43:03,  1.14it/s] 32%|███▏      | 1378/4328 [20:05<42:58,  1.14it/s] 32%|███▏      | 1379/4328 [20:06<43:00,  1.14it/s] 32%|███▏      | 1380/4328 [20:07<42:57,  1.14it/s] 32%|███▏      | 1381/4328 [20:08<42:53,  1.15it/s] 32%|███▏      | 1382/4328 [20:09<42:51,  1.15it/s] 32%|███▏      | 1383/4328 [20:10<42:47,  1.15it/s] 32%|███▏      | 1384/4328 [20:11<42:58,  1.14it/s] 32%|███▏      | 1385/4328 [20:11<43:07,  1.14it/s] 32%|███▏      | 1386/4328 [20:12<43:00,  1.14it/s] 32%|███▏      | 1387/4328 [20:13<42:53,  1.14it/s] 32%|███▏      | 1388/4328 [20:14<42:55,  1.14it/s] 32%|███▏      | 1389/4328 [20:15<42:49,  1.14it/s] 32%|███▏      | 1390/4328 [20:16<42:44,  1.15it/s] 32%|███▏      | 1391/4328 [20:17<42:47,  1.14it/s] 32%|███▏      | 1392/4328 [20:18<42:44,  1.14it/s] 32%|███▏      | 1393/4328 [20:18<42:42,  1.15it/s] 32%|███▏      | 1394/4328 [20:19<42:40,  1.15it/s] 32%|███▏      | 1395/4328 [20:20<42:37,  1.15it/s] 32%|███▏      | 1396/4328 [20:21<42:57,  1.14it/s] 32%|███▏      | 1397/4328 [20:22<42:54,  1.14it/s] 32%|███▏      | 1398/4328 [20:23<42:47,  1.14it/s] 32%|███▏      | 1399/4328 [20:24<42:41,  1.14it/s] 32%|███▏      | 1400/4328 [20:25<42:36,  1.15it/s] 32%|███▏      | 1401/4328 [20:25<42:45,  1.14it/s] 32%|███▏      | 1402/4328 [20:26<42:52,  1.14it/s] 32%|███▏      | 1403/4328 [20:27<42:46,  1.14it/s] 32%|███▏      | 1404/4328 [20:28<42:41,  1.14it/s] 32%|███▏      | 1405/4328 [20:29<42:35,  1.14it/s] 32%|███▏      | 1406/4328 [20:30<42:30,  1.15it/s] 33%|███▎      | 1407/4328 [20:31<42:50,  1.14it/s] 33%|███▎      | 1408/4328 [20:32<42:45,  1.14it/s] 33%|███▎      | 1409/4328 [20:32<42:38,  1.14it/s] 33%|███▎      | 1410/4328 [20:33<42:32,  1.14it/s] 33%|███▎      | 1411/4328 [20:34<42:27,  1.15it/s] 33%|███▎      | 1412/4328 [20:35<42:35,  1.14it/s] 33%|███▎      | 1413/4328 [20:36<42:43,  1.14it/s] 33%|███▎      | 1414/4328 [20:37<42:36,  1.14it/s] 33%|███▎      | 1415/4328 [20:38<42:31,  1.14it/s] 33%|███▎      | 1416/4328 [20:39<42:24,  1.14it/s] 33%|███▎      | 1417/4328 [20:39<42:20,  1.15it/s] 33%|███▎      | 1418/4328 [20:40<42:40,  1.14it/s] 33%|███▎      | 1419/4328 [20:41<42:35,  1.14it/s] 33%|███▎      | 1420/4328 [20:42<42:28,  1.14it/s] 33%|███▎      | 1421/4328 [20:43<42:28,  1.14it/s] 33%|███▎      | 1422/4328 [20:44<42:23,  1.14it/s] 33%|███▎      | 1423/4328 [20:45<42:19,  1.14it/s] 33%|███▎      | 1424/4328 [20:46<42:15,  1.15it/s] 33%|███▎      | 1425/4328 [20:46<42:12,  1.15it/s] 33%|███▎      | 1426/4328 [20:47<42:07,  1.15it/s] 33%|███▎      | 1427/4328 [20:48<42:20,  1.14it/s] 33%|███▎      | 1428/4328 [20:49<42:16,  1.14it/s] 33%|███▎      | 1429/4328 [20:50<42:13,  1.14it/s] 33%|███▎      | 1430/4328 [20:51<42:21,  1.14it/s] 33%|███▎      | 1431/4328 [20:52<42:14,  1.14it/s] 33%|███▎      | 1432/4328 [20:53<42:10,  1.14it/s] 33%|███▎      | 1433/4328 [20:54<42:12,  1.14it/s] 33%|███▎      | 1434/4328 [20:54<42:08,  1.14it/s] 33%|███▎      | 1435/4328 [20:55<42:04,  1.15it/s] 33%|███▎      | 1436/4328 [20:56<42:07,  1.14it/s] 33%|███▎      | 1437/4328 [20:57<42:04,  1.14it/s] 33%|███▎      | 1438/4328 [20:58<42:13,  1.14it/s] 33%|███▎      | 1439/4328 [20:59<42:11,  1.14it/s] 33%|███▎      | 1440/4328 [21:00<42:06,  1.14it/s] 33%|███▎      | 1441/4328 [21:01<42:08,  1.14it/s] 33%|███▎      | 1442/4328 [21:01<42:04,  1.14it/s] 33%|███▎      | 1443/4328 [21:02<42:01,  1.14it/s] 33%|███▎      | 1444/4328 [21:03<41:57,  1.15it/s] 33%|███▎      | 1445/4328 [21:04<41:52,  1.15it/s] 33%|███▎      | 1446/4328 [21:05<42:03,  1.14it/s] 33%|███▎      | 1447/4328 [21:06<42:12,  1.14it/s] 33%|███▎      | 1448/4328 [21:07<42:05,  1.14it/s] 33%|███▎      | 1449/4328 [21:08<42:00,  1.14it/s] 34%|███▎      | 1450/4328 [21:08<41:56,  1.14it/s] 34%|███▎      | 1451/4328 [21:09<41:52,  1.15it/s] 34%|███▎      | 1452/4328 [21:10<42:12,  1.14it/s] 34%|███▎      | 1453/4328 [21:11<42:06,  1.14it/s] 34%|███▎      | 1454/4328 [21:12<41:59,  1.14it/s] 34%|███▎      | 1455/4328 [21:13<41:59,  1.14it/s] 34%|███▎      | 1456/4328 [21:14<41:55,  1.14it/s] 34%|███▎      | 1457/4328 [21:15<41:51,  1.14it/s] 34%|███▎      | 1458/4328 [21:15<41:59,  1.14it/s] 34%|███▎      | 1459/4328 [21:16<41:52,  1.14it/s] 34%|███▎      | 1460/4328 [21:17<41:48,  1.14it/s] 34%|███▍      | 1461/4328 [21:18<41:49,  1.14it/s] 34%|███▍      | 1462/4328 [21:19<41:44,  1.14it/s] 34%|███▍      | 1463/4328 [21:20<41:40,  1.15it/s] 34%|███▍      | 1464/4328 [21:21<41:42,  1.14it/s] 34%|███▍      | 1465/4328 [21:22<41:41,  1.14it/s] 34%|███▍      | 1466/4328 [21:22<41:50,  1.14it/s] 34%|███▍      | 1467/4328 [21:23<41:48,  1.14it/s] 34%|███▍      | 1468/4328 [21:24<41:43,  1.14it/s] 34%|███▍      | 1469/4328 [21:25<41:40,  1.14it/s] 34%|███▍      | 1470/4328 [21:26<41:37,  1.14it/s] 34%|███▍      | 1471/4328 [21:27<41:46,  1.14it/s] 34%|███▍      | 1472/4328 [21:28<41:54,  1.14it/s] 34%|███▍      | 1473/4328 [21:29<41:46,  1.14it/s] 34%|███▍      | 1474/4328 [21:29<41:39,  1.14it/s] 34%|███▍      | 1475/4328 [21:30<41:40,  1.14it/s] 34%|███▍      | 1476/4328 [21:31<41:37,  1.14it/s] 34%|███▍      | 1477/4328 [21:32<41:45,  1.14it/s] 34%|███▍      | 1478/4328 [21:33<41:41,  1.14it/s] 34%|███▍      | 1479/4328 [21:34<41:35,  1.14it/s] 34%|███▍      | 1480/4328 [21:35<41:34,  1.14it/s] 34%|███▍      | 1481/4328 [21:36<41:31,  1.14it/s] 34%|███▍      | 1482/4328 [21:36<41:27,  1.14it/s] 34%|███▍      | 1483/4328 [21:37<41:26,  1.14it/s] 34%|███▍      | 1484/4328 [21:38<41:25,  1.14it/s] 34%|███▍      | 1485/4328 [21:39<41:24,  1.14it/s] 34%|███▍      | 1486/4328 [21:40<41:36,  1.14it/s] 34%|███▍      | 1487/4328 [21:41<41:28,  1.14it/s] 34%|███▍      | 1488/4328 [21:42<41:22,  1.14it/s] 34%|███▍      | 1489/4328 [21:43<41:25,  1.14it/s] 34%|███▍      | 1490/4328 [21:43<41:21,  1.14it/s] 34%|███▍      | 1491/4328 [21:44<41:18,  1.14it/s] 34%|███▍      | 1492/4328 [21:45<41:20,  1.14it/s] 34%|███▍      | 1493/4328 [21:46<41:17,  1.14it/s] 35%|███▍      | 1494/4328 [21:47<41:28,  1.14it/s] 35%|███▍      | 1495/4328 [21:48<41:25,  1.14it/s] 35%|███▍      | 1496/4328 [21:49<41:20,  1.14it/s] 35%|███▍      | 1497/4328 [21:50<41:14,  1.14it/s] 35%|███▍      | 1498/4328 [21:50<41:11,  1.15it/s] 35%|███▍      | 1499/4328 [21:51<41:20,  1.14it/s] 35%|███▍      | 1500/4328 [21:52<41:28,  1.14it/s] 35%|███▍      | 1501/4328 [21:53<41:20,  1.14it/s] 35%|███▍      | 1502/4328 [21:54<41:14,  1.14it/s] 35%|███▍      | 1503/4328 [21:55<41:15,  1.14it/s] 35%|███▍      | 1504/4328 [21:56<41:11,  1.14it/s] 35%|███▍      | 1505/4328 [21:57<41:19,  1.14it/s] 35%|███▍      | 1506/4328 [21:57<41:15,  1.14it/s] 35%|███▍      | 1507/4328 [21:58<41:09,  1.14it/s] 35%|███▍      | 1508/4328 [21:59<41:09,  1.14it/s] 35%|███▍      | 1509/4328 [22:00<41:06,  1.14it/s] 35%|███▍      | 1510/4328 [22:01<41:03,  1.14it/s] 35%|███▍      | 1511/4328 [22:02<40:59,  1.15it/s] 35%|███▍      | 1512/4328 [22:03<40:57,  1.15it/s] 35%|███▍      | 1513/4328 [22:04<41:08,  1.14it/s] 35%|███▍      | 1514/4328 [22:04<41:16,  1.14it/s] 35%|███▌      | 1515/4328 [22:05<41:10,  1.14it/s] 35%|███▌      | 1516/4328 [22:06<41:06,  1.14it/s] 35%|███▌      | 1517/4328 [22:07<41:01,  1.14it/s] 35%|███▌      | 1518/4328 [22:08<40:55,  1.14it/s] 35%|███▌      | 1519/4328 [22:09<41:02,  1.14it/s] 35%|███▌      | 1520/4328 [22:10<40:59,  1.14it/s] 35%|███▌      | 1521/4328 [22:11<40:55,  1.14it/s] 35%|███▌      | 1522/4328 [22:11<40:56,  1.14it/s] 35%|███▌      | 1523/4328 [22:12<40:54,  1.14it/s] 35%|███▌      | 1524/4328 [22:13<40:52,  1.14it/s] 35%|███▌      | 1525/4328 [22:14<41:03,  1.14it/s] 35%|███▌      | 1526/4328 [22:15<40:55,  1.14it/s] 35%|███▌      | 1527/4328 [22:16<40:50,  1.14it/s] 35%|███▌      | 1528/4328 [22:17<40:52,  1.14it/s] 35%|███▌      | 1529/4328 [22:18<40:47,  1.14it/s] 35%|███▌      | 1530/4328 [22:18<40:43,  1.14it/s] 35%|███▌      | 1531/4328 [22:19<40:45,  1.14it/s] 35%|███▌      | 1532/4328 [22:20<40:43,  1.14it/s] 35%|███▌      | 1533/4328 [22:21<40:51,  1.14it/s] 35%|███▌      | 1534/4328 [22:22<40:47,  1.14it/s] 35%|███▌      | 1535/4328 [22:23<40:43,  1.14it/s] 35%|███▌      | 1536/4328 [22:24<40:44,  1.14it/s] 36%|███▌      | 1537/4328 [22:25<40:42,  1.14it/s] 36%|███▌      | 1538/4328 [22:25<40:40,  1.14it/s] 36%|███▌      | 1539/4328 [22:26<40:37,  1.14it/s] 36%|███▌      | 1540/4328 [22:27<40:34,  1.15it/s] 36%|███▌      | 1541/4328 [22:28<40:43,  1.14it/s] 36%|███▌      | 1542/4328 [22:29<40:51,  1.14it/s] 36%|███▌      | 1543/4328 [22:30<40:43,  1.14it/s] 36%|███▌      | 1544/4328 [22:31<40:37,  1.14it/s] 36%|███▌      | 1545/4328 [22:32<40:32,  1.14it/s] 36%|███▌      | 1546/4328 [22:32<40:28,  1.15it/s] 36%|███▌      | 1547/4328 [22:33<40:36,  1.14it/s] 36%|███▌      | 1548/4328 [22:34<40:33,  1.14it/s] 36%|███▌      | 1549/4328 [22:35<40:29,  1.14it/s] 36%|███▌      | 1550/4328 [22:36<40:32,  1.14it/s] 36%|███▌      | 1551/4328 [22:37<40:29,  1.14it/s] 36%|███▌      | 1552/4328 [22:38<40:24,  1.14it/s] 36%|███▌      | 1553/4328 [22:39<40:26,  1.14it/s] 36%|███▌      | 1554/4328 [22:39<40:24,  1.14it/s] 36%|███▌      | 1555/4328 [22:40<40:23,  1.14it/s] 36%|███▌      | 1556/4328 [22:41<40:33,  1.14it/s] 36%|███▌      | 1557/4328 [22:42<40:28,  1.14it/s] 36%|███▌      | 1558/4328 [22:43<40:24,  1.14it/s] 36%|███▌      | 1559/4328 [22:44<40:25,  1.14it/s] 36%|███▌      | 1560/4328 [22:45<40:24,  1.14it/s] 36%|███▌      | 1561/4328 [22:46<40:32,  1.14it/s] 36%|███▌      | 1562/4328 [22:46<40:27,  1.14it/s] 36%|███▌      | 1563/4328 [22:47<40:22,  1.14it/s] 36%|███▌      | 1564/4328 [22:48<40:18,  1.14it/s] 36%|███▌      | 1565/4328 [22:49<40:13,  1.14it/s] 36%|███▌      | 1566/4328 [22:50<40:22,  1.14it/s] 36%|███▌      | 1567/4328 [22:51<40:29,  1.14it/s] 36%|███▌      | 1568/4328 [22:52<40:21,  1.14it/s] 36%|███▋      | 1569/4328 [22:53<40:16,  1.14it/s] 36%|███▋      | 1570/4328 [22:53<40:16,  1.14it/s] 36%|███▋      | 1571/4328 [22:54<40:14,  1.14it/s] 36%|███▋      | 1572/4328 [22:55<40:19,  1.14it/s] 36%|███▋      | 1573/4328 [22:56<40:15,  1.14it/s] 36%|███▋      | 1574/4328 [22:57<40:10,  1.14it/s] 36%|███▋      | 1575/4328 [22:58<40:10,  1.14it/s] 36%|███▋      | 1576/4328 [22:59<40:06,  1.14it/s] 36%|███▋      | 1577/4328 [23:00<40:03,  1.14it/s] 36%|███▋      | 1578/4328 [23:00<40:02,  1.14it/s] 36%|███▋      | 1579/4328 [23:01<40:02,  1.14it/s] 37%|███▋      | 1580/4328 [23:02<40:00,  1.14it/s] 37%|███▋      | 1581/4328 [23:03<40:11,  1.14it/s] 37%|███▋      | 1582/4328 [23:04<40:05,  1.14it/s] 37%|███▋      | 1583/4328 [23:05<40:00,  1.14it/s] 37%|███▋      | 1584/4328 [23:06<40:01,  1.14it/s] 37%|███▋      | 1585/4328 [23:07<39:56,  1.14it/s] 37%|███▋      | 1586/4328 [23:07<39:53,  1.15it/s] 37%|███▋      | 1587/4328 [23:08<39:56,  1.14it/s] 37%|███▋      | 1588/4328 [23:09<39:54,  1.14it/s] 37%|███▋      | 1589/4328 [23:10<40:01,  1.14it/s] 37%|███▋      | 1590/4328 [23:11<39:59,  1.14it/s] 37%|███▋      | 1591/4328 [23:12<39:54,  1.14it/s] 37%|███▋      | 1592/4328 [23:13<39:57,  1.14it/s] 37%|███▋      | 1593/4328 [23:14<39:53,  1.14it/s] 37%|███▋      | 1594/4328 [23:14<39:50,  1.14it/s] 37%|███▋      | 1595/4328 [23:15<39:48,  1.14it/s] 37%|███▋      | 1596/4328 [23:16<39:45,  1.15it/s] 37%|███▋      | 1597/4328 [23:17<39:53,  1.14it/s] 37%|███▋      | 1598/4328 [23:18<40:04,  1.14it/s] 37%|███▋      | 1599/4328 [23:19<39:57,  1.14it/s] 37%|███▋      | 1600/4328 [23:20<39:53,  1.14it/s] 37%|███▋      | 1601/4328 [23:21<39:47,  1.14it/s] 37%|███▋      | 1602/4328 [23:21<39:41,  1.14it/s] 37%|███▋      | 1603/4328 [23:22<39:48,  1.14it/s] 37%|███▋      | 1604/4328 [23:23<39:45,  1.14it/s] 37%|███▋      | 1605/4328 [23:24<39:40,  1.14it/s] 37%|███▋      | 1606/4328 [23:25<39:42,  1.14it/s] 37%|███▋      | 1607/4328 [23:26<39:40,  1.14it/s] 37%|███▋      | 1608/4328 [23:27<39:36,  1.14it/s] 37%|███▋      | 1609/4328 [23:28<39:35,  1.14it/s] 37%|███▋      | 1610/4328 [23:28<39:30,  1.15it/s] 37%|███▋      | 1611/4328 [23:29<39:39,  1.14it/s] 37%|███▋      | 1612/4328 [23:30<39:48,  1.14it/s] 37%|███▋      | 1613/4328 [23:31<39:41,  1.14it/s] 37%|███▋      | 1614/4328 [23:32<39:36,  1.14it/s] 37%|███▋      | 1615/4328 [23:33<39:36,  1.14it/s] 37%|███▋      | 1616/4328 [23:34<39:33,  1.14it/s] 37%|███▋      | 1617/4328 [23:35<39:37,  1.14it/s] 37%|███▋      | 1618/4328 [23:35<39:35,  1.14it/s] 37%|███▋      | 1619/4328 [23:36<39:30,  1.14it/s] 37%|███▋      | 1620/4328 [23:37<39:31,  1.14it/s] 37%|███▋      | 1621/4328 [23:38<39:29,  1.14it/s] 37%|███▋      | 1622/4328 [23:39<39:26,  1.14it/s] 38%|███▊      | 1623/4328 [23:40<39:36,  1.14it/s] 38%|███▊      | 1624/4328 [23:41<39:29,  1.14it/s] 38%|███▊      | 1625/4328 [23:42<39:25,  1.14it/s] 38%|███▊      | 1626/4328 [23:42<39:26,  1.14it/s] 38%|███▊      | 1627/4328 [23:43<39:21,  1.14it/s] 38%|███▊      | 1628/4328 [23:44<39:18,  1.14it/s] 38%|███▊      | 1629/4328 [23:45<39:20,  1.14it/s] 38%|███▊      | 1630/4328 [23:46<39:18,  1.14it/s] 38%|███▊      | 1631/4328 [23:47<39:24,  1.14it/s] 38%|███▊      | 1632/4328 [23:48<39:22,  1.14it/s] 38%|███▊      | 1633/4328 [23:49<39:18,  1.14it/s] 38%|███▊      | 1634/4328 [23:49<39:19,  1.14it/s] 38%|███▊      | 1635/4328 [23:50<39:16,  1.14it/s] 38%|███▊      | 1636/4328 [23:51<39:15,  1.14it/s] 38%|███▊      | 1637/4328 [23:52<39:26,  1.14it/s] 38%|███▊      | 1638/4328 [23:53<39:19,  1.14it/s] 38%|███▊      | 1639/4328 [23:54<39:14,  1.14it/s] 38%|███▊      | 1640/4328 [23:55<39:09,  1.14it/s] 38%|███▊      | 1641/4328 [23:56<39:07,  1.14it/s] 38%|███▊      | 1642/4328 [23:57<39:25,  1.14it/s] 38%|███▊      | 1643/4328 [23:57<39:21,  1.14it/s] 38%|███▊      | 1644/4328 [23:58<39:15,  1.14it/s] 38%|███▊      | 1645/4328 [23:59<39:15,  1.14it/s] 38%|███▊      | 1646/4328 [24:00<39:11,  1.14it/s] 38%|███▊      | 1647/4328 [24:01<39:07,  1.14it/s] 38%|███▊      | 1648/4328 [24:02<39:15,  1.14it/s] 38%|███▊      | 1649/4328 [24:03<39:11,  1.14it/s] 38%|███▊      | 1650/4328 [24:04<39:07,  1.14it/s] 38%|███▊      | 1651/4328 [24:04<39:01,  1.14it/s] 38%|███▊      | 1652/4328 [24:05<38:55,  1.15it/s] 38%|███▊      | 1653/4328 [24:06<39:15,  1.14it/s] 38%|███▊      | 1654/4328 [24:07<39:10,  1.14it/s] 38%|███▊      | 1655/4328 [24:08<39:03,  1.14it/s] 38%|███▊      | 1656/4328 [24:09<38:59,  1.14it/s] 38%|███▊      | 1657/4328 [24:10<38:55,  1.14it/s] 38%|███▊      | 1658/4328 [24:11<39:03,  1.14it/s] 38%|███▊      | 1659/4328 [24:11<39:10,  1.14it/s] 38%|███▊      | 1660/4328 [24:12<39:02,  1.14it/s] 38%|███▊      | 1661/4328 [24:13<38:56,  1.14it/s] 38%|███▊      | 1662/4328 [24:14<38:57,  1.14it/s] 38%|███▊      | 1663/4328 [24:15<38:53,  1.14it/s] 38%|███▊      | 1664/4328 [24:16<39:02,  1.14it/s] 38%|███▊      | 1665/4328 [24:17<38:58,  1.14it/s] 38%|███▊      | 1666/4328 [24:18<38:52,  1.14it/s] 39%|███▊      | 1667/4328 [24:18<38:47,  1.14it/s] 39%|███▊      | 1668/4328 [24:19<38:43,  1.14it/s] 39%|███▊      | 1669/4328 [24:20<38:52,  1.14it/s] 39%|███▊      | 1670/4328 [24:21<39:00,  1.14it/s] 39%|███▊      | 1671/4328 [24:22<38:53,  1.14it/s] 39%|███▊      | 1672/4328 [24:23<38:48,  1.14it/s] 39%|███▊      | 1673/4328 [24:24<38:43,  1.14it/s] 39%|███▊      | 1674/4328 [24:25<38:39,  1.14it/s] 39%|███▊      | 1675/4328 [24:25<38:57,  1.13it/s] 39%|███▊      | 1676/4328 [24:26<38:53,  1.14it/s] 39%|███▊      | 1677/4328 [24:27<38:47,  1.14it/s] 39%|███▉      | 1678/4328 [24:28<38:41,  1.14it/s] 39%|███▉      | 1679/4328 [24:29<38:37,  1.14it/s] 39%|███▉      | 1680/4328 [24:30<38:43,  1.14it/s] 39%|███▉      | 1681/4328 [24:31<38:48,  1.14it/s] 39%|███▉      | 1682/4328 [24:32<38:43,  1.14it/s] 39%|███▉      | 1683/4328 [24:32<38:38,  1.14it/s] 39%|███▉      | 1684/4328 [24:33<38:32,  1.14it/s] 39%|███▉      | 1685/4328 [24:34<38:28,  1.14it/s] 39%|███▉      | 1686/4328 [24:35<38:47,  1.14it/s] 39%|███▉      | 1687/4328 [24:36<38:43,  1.14it/s] 39%|███▉      | 1688/4328 [24:37<38:40,  1.14it/s] 39%|███▉      | 1689/4328 [24:38<38:45,  1.13it/s] 39%|███▉      | 1690/4328 [24:39<38:37,  1.14it/s] 39%|███▉      | 1691/4328 [24:39<38:32,  1.14it/s] 39%|███▉      | 1692/4328 [24:40<38:31,  1.14it/s] 39%|███▉      | 1693/4328 [24:41<38:26,  1.14it/s] 39%|███▉      | 1694/4328 [24:42<38:23,  1.14it/s] 39%|███▉      | 1695/4328 [24:43<38:20,  1.14it/s] 39%|███▉      | 1696/4328 [24:44<38:16,  1.15it/s] 39%|███▉      | 1697/4328 [24:45<38:25,  1.14it/s] 39%|███▉      | 1698/4328 [24:46<38:23,  1.14it/s] 39%|███▉      | 1699/4328 [24:46<38:21,  1.14it/s] 39%|███▉      | 1700/4328 [24:47<38:23,  1.14it/s] 39%|███▉      | 1701/4328 [24:48<38:20,  1.14it/s] 39%|███▉      | 1702/4328 [24:49<38:18,  1.14it/s] 39%|███▉      | 1703/4328 [24:50<38:28,  1.14it/s] 39%|███▉      | 1704/4328 [24:51<38:22,  1.14it/s] 39%|███▉      | 1705/4328 [24:52<38:17,  1.14it/s] 39%|███▉      | 1706/4328 [24:53<38:13,  1.14it/s] 39%|███▉      | 1707/4328 [24:53<38:09,  1.14it/s] 39%|███▉      | 1708/4328 [24:54<38:16,  1.14it/s] 39%|███▉      | 1709/4328 [24:55<38:15,  1.14it/s] 40%|███▉      | 1710/4328 [24:56<38:10,  1.14it/s] 40%|███▉      | 1711/4328 [24:57<38:15,  1.14it/s] 40%|███▉      | 1712/4328 [24:58<38:12,  1.14it/s] 40%|███▉      | 1713/4328 [24:59<38:08,  1.14it/s] 40%|███▉      | 1714/4328 [25:00<38:05,  1.14it/s] 40%|███▉      | 1715/4328 [25:01<38:02,  1.14it/s] 40%|███▉      | 1716/4328 [25:01<38:11,  1.14it/s] 40%|███▉      | 1717/4328 [25:02<38:19,  1.14it/s] 40%|███▉      | 1718/4328 [25:03<38:15,  1.14it/s] 40%|███▉      | 1719/4328 [25:04<38:19,  1.13it/s] 40%|███▉      | 1720/4328 [25:05<38:13,  1.14it/s] 40%|███▉      | 1721/4328 [25:06<38:07,  1.14it/s] 40%|███▉      | 1722/4328 [25:07<38:06,  1.14it/s] 40%|███▉      | 1723/4328 [25:08<38:03,  1.14it/s] 40%|███▉      | 1724/4328 [25:08<38:00,  1.14it/s] 40%|███▉      | 1725/4328 [25:09<38:08,  1.14it/s] 40%|███▉      | 1726/4328 [25:10<38:02,  1.14it/s] 40%|███▉      | 1727/4328 [25:11<37:58,  1.14it/s] 40%|███▉      | 1728/4328 [25:12<37:59,  1.14it/s] 40%|███▉      | 1729/4328 [25:13<37:56,  1.14it/s] 40%|███▉      | 1730/4328 [25:14<38:00,  1.14it/s] 40%|███▉      | 1731/4328 [25:15<37:58,  1.14it/s] 40%|████      | 1732/4328 [25:15<37:53,  1.14it/s] 40%|████      | 1733/4328 [25:16<37:48,  1.14it/s] 40%|████      | 1734/4328 [25:17<37:44,  1.15it/s] 40%|████      | 1735/4328 [25:18<37:53,  1.14it/s] 40%|████      | 1736/4328 [25:19<38:01,  1.14it/s] 40%|████      | 1737/4328 [25:20<37:55,  1.14it/s] 40%|████      | 1738/4328 [25:21<37:51,  1.14it/s] 40%|████      | 1739/4328 [25:22<37:46,  1.14it/s] 40%|████      | 1740/4328 [25:22<37:41,  1.14it/s] 40%|████      | 1741/4328 [25:23<37:49,  1.14it/s] 40%|████      | 1742/4328 [25:24<37:47,  1.14it/s] 40%|████      | 1743/4328 [25:25<37:44,  1.14it/s] 40%|████      | 1744/4328 [25:26<37:46,  1.14it/s] 40%|████      | 1745/4328 [25:27<37:43,  1.14it/s] 40%|████      | 1746/4328 [25:28<37:38,  1.14it/s] 40%|████      | 1747/4328 [25:29<37:38,  1.14it/s] 40%|████      | 1748/4328 [25:29<37:37,  1.14it/s] 40%|████      | 1749/4328 [25:30<37:35,  1.14it/s] 40%|████      | 1750/4328 [25:31<37:44,  1.14it/s] 40%|████      | 1751/4328 [25:32<37:39,  1.14it/s] 40%|████      | 1752/4328 [25:33<37:35,  1.14it/s] 41%|████      | 1753/4328 [25:34<37:36,  1.14it/s] 41%|████      | 1754/4328 [25:35<37:35,  1.14it/s] 41%|████      | 1755/4328 [25:36<37:40,  1.14it/s] 41%|████      | 1756/4328 [25:36<37:36,  1.14it/s] 41%|████      | 1757/4328 [25:37<37:31,  1.14it/s] 41%|████      | 1758/4328 [25:38<37:32,  1.14it/s] 41%|████      | 1759/4328 [25:39<37:30,  1.14it/s] 41%|████      | 1760/4328 [25:40<37:29,  1.14it/s] 41%|████      | 1761/4328 [25:41<37:38,  1.14it/s] 41%|████      | 1762/4328 [25:42<37:31,  1.14it/s] 41%|████      | 1763/4328 [25:43<37:27,  1.14it/s] 41%|████      | 1764/4328 [25:43<37:28,  1.14it/s] 41%|████      | 1765/4328 [25:44<37:26,  1.14it/s] 41%|████      | 1766/4328 [25:45<37:30,  1.14it/s] 41%|████      | 1767/4328 [25:46<37:26,  1.14it/s] 41%|████      | 1768/4328 [25:47<37:22,  1.14it/s] 41%|████      | 1769/4328 [25:48<37:18,  1.14it/s] 41%|████      | 1770/4328 [25:49<37:16,  1.14it/s] 41%|████      | 1771/4328 [25:50<37:24,  1.14it/s] 41%|████      | 1772/4328 [25:51<37:32,  1.13it/s] 41%|████      | 1773/4328 [25:51<37:28,  1.14it/s] 41%|████      | 1774/4328 [25:52<37:25,  1.14it/s] 41%|████      | 1775/4328 [25:53<37:19,  1.14it/s] 41%|████      | 1776/4328 [25:54<37:14,  1.14it/s] 41%|████      | 1777/4328 [25:55<37:15,  1.14it/s] 41%|████      | 1778/4328 [25:56<37:13,  1.14it/s] 41%|████      | 1779/4328 [25:57<37:10,  1.14it/s] 41%|████      | 1780/4328 [25:58<37:10,  1.14it/s] 41%|████      | 1781/4328 [25:58<37:09,  1.14it/s] 41%|████      | 1782/4328 [25:59<37:08,  1.14it/s] 41%|████      | 1783/4328 [26:00<37:15,  1.14it/s] 41%|████      | 1784/4328 [26:01<37:09,  1.14it/s] 41%|████      | 1785/4328 [26:02<37:05,  1.14it/s] 41%|████▏     | 1786/4328 [26:03<37:03,  1.14it/s] 41%|████▏     | 1787/4328 [26:04<36:59,  1.14it/s] 41%|████▏     | 1788/4328 [26:05<37:18,  1.13it/s] 41%|████▏     | 1789/4328 [26:05<37:13,  1.14it/s] 41%|████▏     | 1790/4328 [26:06<37:09,  1.14it/s] 41%|████▏     | 1791/4328 [26:07<37:05,  1.14it/s] 41%|████▏     | 1792/4328 [26:08<36:59,  1.14it/s] 41%|████▏     | 1793/4328 [26:09<36:56,  1.14it/s] 41%|████▏     | 1794/4328 [26:10<37:04,  1.14it/s] 41%|████▏     | 1795/4328 [26:11<36:59,  1.14it/s] 41%|████▏     | 1796/4328 [26:12<36:55,  1.14it/s] 42%|████▏     | 1797/4328 [26:12<36:58,  1.14it/s] 42%|████▏     | 1798/4328 [26:13<36:55,  1.14it/s] 42%|████▏     | 1799/4328 [26:14<36:54,  1.14it/s] 42%|████▏     | 1800/4328 [26:15<36:51,  1.14it/s] 42%|████▏     | 1801/4328 [26:16<36:47,  1.14it/s] 42%|████▏     | 1802/4328 [26:17<37:01,  1.14it/s] 42%|████▏     | 1803/4328 [26:18<36:58,  1.14it/s] 42%|████▏     | 1804/4328 [26:19<36:54,  1.14it/s] 42%|████▏     | 1805/4328 [26:19<36:59,  1.14it/s] 42%|████▏     | 1806/4328 [26:20<36:53,  1.14it/s] 42%|████▏     | 1807/4328 [26:21<36:49,  1.14it/s] 42%|████▏     | 1808/4328 [26:22<36:50,  1.14it/s] 42%|████▏     | 1809/4328 [26:23<36:48,  1.14it/s] 42%|████▏     | 1810/4328 [26:24<36:52,  1.14it/s] 42%|████▏     | 1811/4328 [26:25<36:48,  1.14it/s] 42%|████▏     | 1812/4328 [26:26<36:44,  1.14it/s] 42%|████▏     | 1813/4328 [26:26<36:44,  1.14it/s] 42%|████▏     | 1814/4328 [26:27<36:41,  1.14it/s] 42%|████▏     | 1815/4328 [26:28<36:39,  1.14it/s] 42%|████▏     | 1816/4328 [26:29<36:48,  1.14it/s] 42%|████▏     | 1817/4328 [26:30<36:43,  1.14it/s] 42%|████▏     | 1818/4328 [26:31<36:41,  1.14it/s] 42%|████▏     | 1819/4328 [26:32<36:37,  1.14it/s] 42%|████▏     | 1820/4328 [26:33<36:33,  1.14it/s] 42%|████▏     | 1821/4328 [26:33<36:51,  1.13it/s] 42%|████▏     | 1822/4328 [26:34<36:47,  1.14it/s] 42%|████▏     | 1823/4328 [26:35<36:43,  1.14it/s] 42%|████▏     | 1824/4328 [26:36<36:48,  1.13it/s] 42%|████▏     | 1825/4328 [26:37<36:40,  1.14it/s] 42%|████▏     | 1826/4328 [26:38<36:36,  1.14it/s] 42%|████▏     | 1827/4328 [26:39<36:32,  1.14it/s] 42%|████▏     | 1828/4328 [26:40<36:28,  1.14it/s] 42%|████▏     | 1829/4328 [26:41<36:44,  1.13it/s] 42%|████▏     | 1830/4328 [26:41<36:39,  1.14it/s] 42%|████▏     | 1831/4328 [26:42<36:35,  1.14it/s] 42%|████▏     | 1832/4328 [26:43<36:40,  1.13it/s] 42%|████▏     | 1833/4328 [26:44<36:32,  1.14it/s] 42%|████▏     | 1834/4328 [26:45<36:27,  1.14it/s] 42%|████▏     | 1835/4328 [26:46<36:22,  1.14it/s] 42%|████▏     | 1836/4328 [26:47<36:17,  1.14it/s] 42%|████▏     | 1837/4328 [26:48<36:26,  1.14it/s] 42%|████▏     | 1838/4328 [26:48<36:24,  1.14it/s] 42%|████▏     | 1839/4328 [26:49<36:20,  1.14it/s] 43%|████▎     | 1840/4328 [26:50<36:20,  1.14it/s] 43%|████▎     | 1841/4328 [26:51<36:17,  1.14it/s] 43%|████▎     | 1842/4328 [26:52<36:14,  1.14it/s] 43%|████▎     | 1843/4328 [26:53<36:14,  1.14it/s] 43%|████▎     | 1844/4328 [26:54<36:13,  1.14it/s] 43%|████▎     | 1845/4328 [26:55<36:12,  1.14it/s] 43%|████▎     | 1846/4328 [26:55<36:19,  1.14it/s] 43%|████▎     | 1847/4328 [26:56<36:14,  1.14it/s] 43%|████▎     | 1848/4328 [26:57<36:11,  1.14it/s] 43%|████▎     | 1849/4328 [26:58<36:07,  1.14it/s] 43%|████▎     | 1850/4328 [26:59<36:04,  1.14it/s] 43%|████▎     | 1851/4328 [27:00<36:23,  1.13it/s] 43%|████▎     | 1852/4328 [27:01<36:19,  1.14it/s] 43%|████▎     | 1853/4328 [27:02<36:13,  1.14it/s] 43%|████▎     | 1854/4328 [27:02<36:08,  1.14it/s] 43%|████▎     | 1855/4328 [27:03<36:04,  1.14it/s] 43%|████▎     | 1856/4328 [27:04<36:01,  1.14it/s] 43%|████▎     | 1857/4328 [27:05<36:07,  1.14it/s] 43%|████▎     | 1858/4328 [27:06<36:03,  1.14it/s] 43%|████▎     | 1859/4328 [27:07<36:00,  1.14it/s] 43%|████▎     | 1860/4328 [27:08<36:02,  1.14it/s] 43%|████▎     | 1861/4328 [27:09<36:00,  1.14it/s] 43%|████▎     | 1862/4328 [27:09<36:06,  1.14it/s] 43%|████▎     | 1863/4328 [27:10<36:03,  1.14it/s] 43%|████▎     | 1864/4328 [27:11<35:58,  1.14it/s] 43%|████▎     | 1865/4328 [27:12<35:58,  1.14it/s] 43%|████▎     | 1866/4328 [27:13<35:56,  1.14it/s] 43%|████▎     | 1867/4328 [27:14<35:54,  1.14it/s] 43%|████▎     | 1868/4328 [27:15<36:03,  1.14it/s] 43%|████▎     | 1869/4328 [27:16<35:57,  1.14it/s] 43%|████▎     | 1870/4328 [27:16<35:53,  1.14it/s] 43%|████▎     | 1871/4328 [27:17<35:49,  1.14it/s] 43%|████▎     | 1872/4328 [27:18<35:45,  1.14it/s] 43%|████▎     | 1873/4328 [27:19<35:53,  1.14it/s] 43%|████▎     | 1874/4328 [27:20<35:51,  1.14it/s] 43%|████▎     | 1875/4328 [27:21<35:48,  1.14it/s] 43%|████▎     | 1876/4328 [27:22<35:45,  1.14it/s] 43%|████▎     | 1877/4328 [27:23<35:43,  1.14it/s] 43%|████▎     | 1878/4328 [27:23<35:51,  1.14it/s] 43%|████▎     | 1879/4328 [27:24<35:58,  1.13it/s] 43%|████▎     | 1880/4328 [27:25<35:51,  1.14it/s] 43%|████▎     | 1881/4328 [27:26<35:47,  1.14it/s] 43%|████▎     | 1882/4328 [27:27<35:42,  1.14it/s] 44%|████▎     | 1883/4328 [27:28<35:36,  1.14it/s] 44%|████▎     | 1884/4328 [27:29<35:43,  1.14it/s] 44%|████▎     | 1885/4328 [27:30<35:42,  1.14it/s] 44%|████▎     | 1886/4328 [27:30<35:38,  1.14it/s] 44%|████▎     | 1887/4328 [27:31<35:34,  1.14it/s] 44%|████▎     | 1888/4328 [27:32<35:31,  1.14it/s] 44%|████▎     | 1889/4328 [27:33<35:40,  1.14it/s] 44%|████▎     | 1890/4328 [27:34<35:47,  1.14it/s] 44%|████▎     | 1891/4328 [27:35<35:40,  1.14it/s] 44%|████▎     | 1892/4328 [27:36<35:35,  1.14it/s] 44%|████▎     | 1893/4328 [27:37<35:30,  1.14it/s] 44%|████▍     | 1894/4328 [27:37<35:27,  1.14it/s] 44%|████▍     | 1895/4328 [27:38<35:34,  1.14it/s] 44%|████▍     | 1896/4328 [27:39<35:33,  1.14it/s] 44%|████▍     | 1897/4328 [27:40<35:28,  1.14it/s] 44%|████▍     | 1898/4328 [27:41<35:32,  1.14it/s] 44%|████▍     | 1899/4328 [27:42<35:28,  1.14it/s] 44%|████▍     | 1900/4328 [27:43<35:24,  1.14it/s] 44%|████▍     | 1901/4328 [27:44<35:24,  1.14it/s] 44%|████▍     | 1902/4328 [27:44<35:23,  1.14it/s] 44%|████▍     | 1903/4328 [27:45<35:21,  1.14it/s] 44%|████▍     | 1904/4328 [27:46<35:29,  1.14it/s] 44%|████▍     | 1905/4328 [27:47<35:24,  1.14it/s] 44%|████▍     | 1906/4328 [27:48<35:20,  1.14it/s] 44%|████▍     | 1907/4328 [27:49<35:21,  1.14it/s] 44%|████▍     | 1908/4328 [27:50<35:20,  1.14it/s] 44%|████▍     | 1909/4328 [27:51<35:26,  1.14it/s] 44%|████▍     | 1910/4328 [27:52<35:23,  1.14it/s] 44%|████▍     | 1911/4328 [27:52<35:18,  1.14it/s] 44%|████▍     | 1912/4328 [27:53<35:15,  1.14it/s] 44%|████▍     | 1913/4328 [27:54<35:11,  1.14it/s] 44%|████▍     | 1914/4328 [27:55<35:19,  1.14it/s] 44%|████▍     | 1915/4328 [27:56<35:26,  1.13it/s] 44%|████▍     | 1916/4328 [27:57<35:19,  1.14it/s] 44%|████▍     | 1917/4328 [27:58<35:14,  1.14it/s] 44%|████▍     | 1918/4328 [27:59<35:10,  1.14it/s] 44%|████▍     | 1919/4328 [27:59<35:07,  1.14it/s] 44%|████▍     | 1920/4328 [28:00<35:20,  1.14it/s] 44%|████▍     | 1921/4328 [28:01<35:15,  1.14it/s] 44%|████▍     | 1922/4328 [28:02<35:12,  1.14it/s] 44%|████▍     | 1923/4328 [28:03<35:18,  1.14it/s] 44%|████▍     | 1924/4328 [28:04<35:11,  1.14it/s] 44%|████▍     | 1925/4328 [28:05<35:07,  1.14it/s] 45%|████▍     | 1926/4328 [28:06<35:02,  1.14it/s] 45%|████▍     | 1927/4328 [28:06<34:59,  1.14it/s] 45%|████▍     | 1928/4328 [28:07<35:16,  1.13it/s] 45%|████▍     | 1929/4328 [28:08<35:11,  1.14it/s] 45%|████▍     | 1930/4328 [28:09<35:06,  1.14it/s] 45%|████▍     | 1931/4328 [28:10<35:02,  1.14it/s] 45%|████▍     | 1932/4328 [28:11<34:57,  1.14it/s] 45%|████▍     | 1933/4328 [28:12<35:05,  1.14it/s] 45%|████▍     | 1934/4328 [28:13<35:07,  1.14it/s] 45%|████▍     | 1935/4328 [28:13<35:02,  1.14it/s] 45%|████▍     | 1936/4328 [28:14<35:06,  1.14it/s] 45%|████▍     | 1937/4328 [28:15<35:01,  1.14it/s] 45%|████▍     | 1938/4328 [28:16<34:55,  1.14it/s] 45%|████▍     | 1939/4328 [28:17<34:50,  1.14it/s] 45%|████▍     | 1940/4328 [28:18<34:46,  1.14it/s] 45%|████▍     | 1941/4328 [28:19<34:54,  1.14it/s] 45%|████▍     | 1942/4328 [28:20<35:00,  1.14it/s] 45%|████▍     | 1943/4328 [28:20<34:54,  1.14it/s] 45%|████▍     | 1944/4328 [28:21<34:49,  1.14it/s] 45%|████▍     | 1945/4328 [28:22<34:44,  1.14it/s] 45%|████▍     | 1946/4328 [28:23<34:40,  1.15it/s] 45%|████▍     | 1947/4328 [28:24<34:58,  1.13it/s] 45%|████▌     | 1948/4328 [28:25<34:53,  1.14it/s] 45%|████▌     | 1949/4328 [28:26<34:49,  1.14it/s] 45%|████▌     | 1950/4328 [28:27<34:44,  1.14it/s] 45%|████▌     | 1951/4328 [28:27<34:39,  1.14it/s] 45%|████▌     | 1952/4328 [28:28<34:46,  1.14it/s] 45%|████▌     | 1953/4328 [28:29<34:52,  1.13it/s] 45%|████▌     | 1954/4328 [28:30<34:47,  1.14it/s] 45%|████▌     | 1955/4328 [28:31<34:44,  1.14it/s] 45%|████▌     | 1956/4328 [28:32<34:39,  1.14it/s] 45%|████▌     | 1957/4328 [28:33<34:35,  1.14it/s] 45%|████▌     | 1958/4328 [28:34<34:38,  1.14it/s] 45%|████▌     | 1959/4328 [28:35<34:35,  1.14it/s] 45%|████▌     | 1960/4328 [28:35<34:32,  1.14it/s] 45%|████▌     | 1961/4328 [28:36<34:29,  1.14it/s] 45%|████▌     | 1962/4328 [28:37<34:26,  1.15it/s] 45%|████▌     | 1963/4328 [28:38<34:33,  1.14it/s] 45%|████▌     | 1964/4328 [28:39<34:41,  1.14it/s] 45%|████▌     | 1965/4328 [28:40<34:36,  1.14it/s] 45%|████▌     | 1966/4328 [28:41<34:32,  1.14it/s] 45%|████▌     | 1967/4328 [28:42<34:28,  1.14it/s] 45%|████▌     | 1968/4328 [28:42<34:24,  1.14it/s] 45%|████▌     | 1969/4328 [28:43<34:38,  1.14it/s] 46%|████▌     | 1970/4328 [28:44<34:33,  1.14it/s] 46%|████▌     | 1971/4328 [28:45<34:29,  1.14it/s] 46%|████▌     | 1972/4328 [28:46<34:36,  1.13it/s] 46%|████▌     | 1973/4328 [28:47<34:29,  1.14it/s] 46%|████▌     | 1974/4328 [28:48<34:24,  1.14it/s] 46%|████▌     | 1975/4328 [28:49<34:24,  1.14it/s] 46%|████▌     | 1976/4328 [28:49<34:21,  1.14it/s] 46%|████▌     | 1977/4328 [28:50<34:24,  1.14it/s] 46%|████▌     | 1978/4328 [28:51<34:23,  1.14it/s] 46%|████▌     | 1979/4328 [28:52<34:18,  1.14it/s] 46%|████▌     | 1980/4328 [28:53<34:15,  1.14it/s] 46%|████▌     | 1981/4328 [28:54<34:13,  1.14it/s] 46%|████▌     | 1982/4328 [28:55<34:20,  1.14it/s] 46%|████▌     | 1983/4328 [28:56<34:27,  1.13it/s] 46%|████▌     | 1984/4328 [28:56<34:23,  1.14it/s] 46%|████▌     | 1985/4328 [28:57<34:27,  1.13it/s] 46%|████▌     | 1986/4328 [28:58<34:21,  1.14it/s] 46%|████▌     | 1987/4328 [28:59<34:15,  1.14it/s] 46%|████▌     | 1988/4328 [29:00<34:10,  1.14it/s] 46%|████▌     | 1989/4328 [29:01<34:05,  1.14it/s] 46%|████▌     | 1990/4328 [29:02<34:01,  1.14it/s] 46%|████▌     | 1991/4328 [29:03<34:12,  1.14it/s] 46%|████▌     | 1992/4328 [29:03<34:08,  1.14it/s] 46%|████▌     | 1993/4328 [29:04<34:05,  1.14it/s] 46%|████▌     | 1994/4328 [29:05<34:08,  1.14it/s] 46%|████▌     | 1995/4328 [29:06<34:05,  1.14it/s] 46%|████▌     | 1996/4328 [29:07<34:12,  1.14it/s] 46%|████▌     | 1997/4328 [29:08<34:08,  1.14it/s] 46%|████▌     | 1998/4328 [29:09<34:03,  1.14it/s] 46%|████▌     | 1999/4328 [29:10<34:02,  1.14it/s] 46%|████▌     | 2000/4328 [29:10<34:01,  1.14it/s] 46%|████▌     | 2001/4328 [29:11<33:58,  1.14it/s] 46%|████▋     | 2002/4328 [29:12<34:04,  1.14it/s] 46%|████▋     | 2003/4328 [29:13<33:59,  1.14it/s] 46%|████▋     | 2004/4328 [29:14<33:55,  1.14it/s] 46%|████▋     | 2005/4328 [29:15<33:51,  1.14it/s] 46%|████▋     | 2006/4328 [29:16<33:47,  1.15it/s] 46%|████▋     | 2007/4328 [29:17<33:55,  1.14it/s] 46%|████▋     | 2008/4328 [29:17<33:54,  1.14it/s] 46%|████▋     | 2009/4328 [29:18<33:50,  1.14it/s] 46%|████▋     | 2010/4328 [29:19<33:52,  1.14it/s] 46%|████▋     | 2011/4328 [29:20<33:49,  1.14it/s] 46%|████▋     | 2012/4328 [29:21<33:45,  1.14it/s] 47%|████▋     | 2013/4328 [29:22<33:45,  1.14it/s] 47%|████▋     | 2014/4328 [29:23<33:44,  1.14it/s] 47%|████▋     | 2015/4328 [29:24<33:42,  1.14it/s] 47%|████▋     | 2016/4328 [29:24<33:48,  1.14it/s] 47%|████▋     | 2017/4328 [29:25<33:44,  1.14it/s] 47%|████▋     | 2018/4328 [29:26<33:41,  1.14it/s] 47%|████▋     | 2019/4328 [29:27<33:42,  1.14it/s] 47%|████▋     | 2020/4328 [29:28<33:40,  1.14it/s] 47%|████▋     | 2021/4328 [29:29<33:47,  1.14it/s] 47%|████▋     | 2022/4328 [29:30<33:44,  1.14it/s] 47%|████▋     | 2023/4328 [29:31<33:39,  1.14it/s] 47%|████▋     | 2024/4328 [29:32<33:36,  1.14it/s] 47%|████▋     | 2025/4328 [29:32<33:32,  1.14it/s] 47%|████▋     | 2026/4328 [29:33<33:40,  1.14it/s] 47%|████▋     | 2027/4328 [29:34<33:46,  1.14it/s] 47%|████▋     | 2028/4328 [29:35<33:41,  1.14it/s] 47%|████▋     | 2029/4328 [29:36<33:37,  1.14it/s] 47%|████▋     | 2030/4328 [29:37<33:32,  1.14it/s] 47%|████▋     | 2031/4328 [29:38<33:29,  1.14it/s] 47%|████▋     | 2032/4328 [29:39<33:42,  1.14it/s] 47%|████▋     | 2033/4328 [29:39<33:36,  1.14it/s] 47%|████▋     | 2034/4328 [29:40<33:33,  1.14it/s] 47%|████▋     | 2035/4328 [29:41<33:39,  1.14it/s] 47%|████▋     | 2036/4328 [29:42<33:33,  1.14it/s] 47%|████▋     | 2037/4328 [29:43<33:27,  1.14it/s] 47%|████▋     | 2038/4328 [29:44<33:22,  1.14it/s] 47%|████▋     | 2039/4328 [29:45<33:18,  1.15it/s] 47%|████▋     | 2040/4328 [29:46<33:36,  1.13it/s] 47%|████▋     | 2041/4328 [29:46<33:31,  1.14it/s] 47%|████▋     | 2042/4328 [29:47<33:27,  1.14it/s] 47%|████▋     | 2043/4328 [29:48<33:25,  1.14it/s] 47%|████▋     | 2044/4328 [29:49<33:20,  1.14it/s] 47%|████▋     | 2045/4328 [29:50<33:26,  1.14it/s] 47%|████▋     | 2046/4328 [29:51<33:31,  1.13it/s] 47%|████▋     | 2047/4328 [29:52<33:26,  1.14it/s] 47%|████▋     | 2048/4328 [29:53<33:23,  1.14it/s] 47%|████▋     | 2049/4328 [29:53<33:17,  1.14it/s] 47%|████▋     | 2050/4328 [29:54<33:13,  1.14it/s] 47%|████▋     | 2051/4328 [29:55<33:17,  1.14it/s] 47%|████▋     | 2052/4328 [29:56<33:15,  1.14it/s] 47%|████▋     | 2053/4328 [29:57<33:11,  1.14it/s] 47%|████▋     | 2054/4328 [29:58<33:10,  1.14it/s] 47%|████▋     | 2055/4328 [29:59<33:08,  1.14it/s] 48%|████▊     | 2056/4328 [30:00<33:05,  1.14it/s] 48%|████▊     | 2057/4328 [30:00<33:05,  1.14it/s] 48%|████▊     | 2058/4328 [30:01<33:05,  1.14it/s] 48%|████▊     | 2059/4328 [30:02<33:04,  1.14it/s] 48%|████▊     | 2060/4328 [30:03<33:10,  1.14it/s] 48%|████▊     | 2061/4328 [30:04<33:05,  1.14it/s] 48%|████▊     | 2062/4328 [30:05<33:02,  1.14it/s] 48%|████▊     | 2063/4328 [30:06<32:58,  1.14it/s] 48%|████▊     | 2064/4328 [30:07<32:55,  1.15it/s] 48%|████▊     | 2065/4328 [30:07<33:03,  1.14it/s] 48%|████▊     | 2066/4328 [30:08<33:01,  1.14it/s] 48%|████▊     | 2067/4328 [30:09<32:58,  1.14it/s] 48%|████▊     | 2068/4328 [30:10<33:00,  1.14it/s] 48%|████▊     | 2069/4328 [30:11<32:57,  1.14it/s] 48%|████▊     | 2070/4328 [30:12<32:53,  1.14it/s] 48%|████▊     | 2071/4328 [30:13<32:53,  1.14it/s] 48%|████▊     | 2072/4328 [30:14<32:52,  1.14it/s] 48%|████▊     | 2073/4328 [30:14<32:51,  1.14it/s] 48%|████▊     | 2074/4328 [30:15<32:59,  1.14it/s] 48%|████▊     | 2075/4328 [30:16<32:54,  1.14it/s] 48%|████▊     | 2076/4328 [30:17<32:50,  1.14it/s] 48%|████▊     | 2077/4328 [30:18<32:47,  1.14it/s] 48%|████▊     | 2078/4328 [30:19<32:44,  1.15it/s] 48%|████▊     | 2079/4328 [30:20<33:00,  1.14it/s] 48%|████▊     | 2080/4328 [30:21<32:56,  1.14it/s] 48%|████▊     | 2081/4328 [30:21<32:50,  1.14it/s] 48%|████▊     | 2082/4328 [30:22<32:50,  1.14it/s] 48%|████▊     | 2083/4328 [30:23<32:48,  1.14it/s] 48%|████▊     | 2084/4328 [30:24<32:46,  1.14it/s] 48%|████▊     | 2085/4328 [30:25<32:51,  1.14it/s] 48%|████▊     | 2086/4328 [30:26<32:46,  1.14it/s] 48%|████▊     | 2087/4328 [30:27<32:41,  1.14it/s] 48%|████▊     | 2088/4328 [30:28<32:37,  1.14it/s] 48%|████▊     | 2089/4328 [30:28<32:34,  1.15it/s] 48%|████▊     | 2090/4328 [30:29<32:41,  1.14it/s] 48%|████▊     | 2091/4328 [30:30<32:39,  1.14it/s] 48%|████▊     | 2092/4328 [30:31<32:36,  1.14it/s] 48%|████▊     | 2093/4328 [30:32<32:39,  1.14it/s] 48%|████▊     | 2094/4328 [30:33<32:36,  1.14it/s] 48%|████▊     | 2095/4328 [30:34<32:31,  1.14it/s] 48%|████▊     | 2096/4328 [30:35<32:33,  1.14it/s] 48%|████▊     | 2097/4328 [30:35<32:31,  1.14it/s] 48%|████▊     | 2098/4328 [30:36<32:30,  1.14it/s] 48%|████▊     | 2099/4328 [30:37<32:34,  1.14it/s] 49%|████▊     | 2100/4328 [30:38<32:30,  1.14it/s] 49%|████▊     | 2101/4328 [30:39<32:27,  1.14it/s] 49%|████▊     | 2102/4328 [30:40<32:30,  1.14it/s] 49%|████▊     | 2103/4328 [30:41<32:27,  1.14it/s] 49%|████▊     | 2104/4328 [30:42<32:24,  1.14it/s] 49%|████▊     | 2105/4328 [30:42<32:21,  1.14it/s] 49%|████▊     | 2106/4328 [30:43<32:18,  1.15it/s] 49%|████▊     | 2107/4328 [30:44<32:25,  1.14it/s] 49%|████▊     | 2108/4328 [30:45<32:24,  1.14it/s] 49%|████▊     | 2109/4328 [30:46<32:21,  1.14it/s] 49%|████▉     | 2110/4328 [30:47<32:23,  1.14it/s] 49%|████▉     | 2111/4328 [30:48<32:21,  1.14it/s] 49%|████▉     | 2112/4328 [30:49<32:20,  1.14it/s] 49%|████▉     | 2113/4328 [30:49<32:28,  1.14it/s] 49%|████▉     | 2114/4328 [30:50<32:23,  1.14it/s] 49%|████▉     | 2115/4328 [30:51<32:20,  1.14it/s] 49%|████▉     | 2116/4328 [30:52<32:15,  1.14it/s] 49%|████▉     | 2117/4328 [30:53<32:12,  1.14it/s] 49%|████▉     | 2118/4328 [30:54<32:27,  1.13it/s] 49%|████▉     | 2119/4328 [30:55<32:22,  1.14it/s] 49%|████▉     | 2120/4328 [30:56<32:16,  1.14it/s] 49%|████▉     | 2121/4328 [30:57<32:16,  1.14it/s] 49%|████▉     | 2122/4328 [30:57<32:13,  1.14it/s] 49%|████▉     | 2123/4328 [30:58<32:10,  1.14it/s] 49%|████▉     | 2124/4328 [30:59<32:15,  1.14it/s] 49%|████▉     | 2125/4328 [31:00<32:10,  1.14it/s] 49%|████▉     | 2126/4328 [31:01<32:06,  1.14it/s] 49%|████▉     | 2127/4328 [31:02<32:07,  1.14it/s] 49%|████▉     | 2128/4328 [31:03<32:05,  1.14it/s] 49%|████▉     | 2129/4328 [31:04<32:11,  1.14it/s] 49%|████▉     | 2130/4328 [31:04<32:09,  1.14it/s] 49%|████▉     | 2131/4328 [31:05<32:05,  1.14it/s] 49%|████▉     | 2132/4328 [31:06<32:04,  1.14it/s] 49%|████▉     | 2133/4328 [31:07<32:03,  1.14it/s] 49%|████▉     | 2134/4328 [31:08<32:01,  1.14it/s] 49%|████▉     | 2135/4328 [31:09<32:05,  1.14it/s] 49%|████▉     | 2136/4328 [31:10<32:00,  1.14it/s] 49%|████▉     | 2137/4328 [31:11<31:57,  1.14it/s] 49%|████▉     | 2138/4328 [31:11<31:53,  1.14it/s] 49%|████▉     | 2139/4328 [31:12<31:52,  1.14it/s] 49%|████▉     | 2140/4328 [31:13<32:07,  1.14it/s] 49%|████▉     | 2141/4328 [31:14<32:03,  1.14it/s] 49%|████▉     | 2142/4328 [31:15<31:57,  1.14it/s] 50%|████▉     | 2143/4328 [31:16<31:52,  1.14it/s] 50%|████▉     | 2144/4328 [31:17<31:48,  1.14it/s] 50%|████▉     | 2145/4328 [31:18<31:52,  1.14it/s] 50%|████▉     | 2146/4328 [31:18<31:59,  1.14it/s] 50%|████▉     | 2147/4328 [31:19<31:55,  1.14it/s] 50%|████▉     | 2148/4328 [31:20<31:50,  1.14it/s] 50%|████▉     | 2149/4328 [31:21<31:45,  1.14it/s] 50%|████▉     | 2150/4328 [31:22<31:42,  1.14it/s] 50%|████▉     | 2151/4328 [31:23<31:58,  1.13it/s] 50%|████▉     | 2152/4328 [31:24<31:53,  1.14it/s] 50%|████▉     | 2153/4328 [31:25<31:47,  1.14it/s] 50%|████▉     | 2154/4328 [31:25<31:46,  1.14it/s] 50%|████▉     | 2155/4328 [31:26<31:43,  1.14it/s] 50%|████▉     | 2156/4328 [31:27<31:41,  1.14it/s] 50%|████▉     | 2157/4328 [31:28<31:49,  1.14it/s] 50%|████▉     | 2158/4328 [31:29<31:44,  1.14it/s] 50%|████▉     | 2159/4328 [31:30<31:40,  1.14it/s] 50%|████▉     | 2160/4328 [31:31<31:36,  1.14it/s] 50%|████▉     | 2161/4328 [31:32<31:33,  1.14it/s] 50%|████▉     | 2162/4328 [31:32<31:49,  1.13it/s] 50%|████▉     | 2163/4328 [31:33<31:44,  1.14it/s] 50%|█████     | 2164/4328 [31:34<31:39,  1.14it/s] 50%|█████     | 2165/4328 [31:35<31:34,  1.14it/s] 50%|█████     | 2166/4328 [31:36<31:30,  1.14it/s] 50%|█████     | 2167/4328 [31:37<31:36,  1.14it/s] 50%|█████     | 2168/4328 [31:38<31:42,  1.14it/s] 50%|█████     | 2169/4328 [31:39<31:37,  1.14it/s] 50%|█████     | 2170/4328 [31:39<31:32,  1.14it/s] 50%|█████     | 2171/4328 [31:40<31:27,  1.14it/s] 50%|█████     | 2172/4328 [31:41<31:24,  1.14it/s] 50%|█████     | 2173/4328 [31:42<31:38,  1.14it/s] 50%|█████     | 2174/4328 [31:43<31:34,  1.14it/s] 50%|█████     | 2175/4328 [31:44<31:31,  1.14it/s] 50%|█████     | 2176/4328 [31:45<31:27,  1.14it/s] 50%|█████     | 2177/4328 [31:46<31:22,  1.14it/s] 50%|█████     | 2178/4328 [31:46<31:19,  1.14it/s] 50%|█████     | 2179/4328 [31:47<31:23,  1.14it/s] 50%|█████     | 2180/4328 [31:48<31:20,  1.14it/s] 50%|█████     | 2181/4328 [31:49<31:17,  1.14it/s] 50%|█████     | 2182/4328 [31:50<31:18,  1.14it/s] 50%|█████     | 2183/4328 [31:51<31:16,  1.14it/s] 50%|█████     | 2184/4328 [31:52<31:23,  1.14it/s] 50%|█████     | 2185/4328 [31:53<31:20,  1.14it/s] 51%|█████     | 2186/4328 [31:53<31:16,  1.14it/s] 51%|█████     | 2187/4328 [31:54<31:13,  1.14it/s] 51%|█████     | 2188/4328 [31:55<31:09,  1.14it/s] 51%|█████     | 2189/4328 [31:56<31:16,  1.14it/s] 51%|█████     | 2190/4328 [31:57<31:22,  1.14it/s] 51%|█████     | 2191/4328 [31:58<31:16,  1.14it/s] 51%|█████     | 2192/4328 [31:59<31:12,  1.14it/s] 51%|█████     | 2193/4328 [32:00<31:08,  1.14it/s] 51%|█████     | 2194/4328 [32:01<31:04,  1.14it/s] 51%|█████     | 2195/4328 [32:01<31:11,  1.14it/s] 51%|█████     | 2196/4328 [32:02<31:09,  1.14it/s] 51%|█████     | 2197/4328 [32:03<31:05,  1.14it/s] 51%|█████     | 2198/4328 [32:04<31:07,  1.14it/s] 51%|█████     | 2199/4328 [32:05<31:04,  1.14it/s] 51%|█████     | 2200/4328 [32:06<31:00,  1.14it/s] 51%|█████     | 2201/4328 [32:07<31:00,  1.14it/s] 51%|█████     | 2202/4328 [32:08<30:59,  1.14it/s] 51%|█████     | 2203/4328 [32:08<30:57,  1.14it/s] 51%|█████     | 2204/4328 [32:09<31:05,  1.14it/s] 51%|█████     | 2205/4328 [32:10<31:01,  1.14it/s] 51%|█████     | 2206/4328 [32:11<30:58,  1.14it/s] 51%|█████     | 2207/4328 [32:12<31:00,  1.14it/s] 51%|█████     | 2208/4328 [32:13<30:56,  1.14it/s] 51%|█████     | 2209/4328 [32:14<30:52,  1.14it/s] 51%|█████     | 2210/4328 [32:15<30:50,  1.14it/s] 51%|█████     | 2211/4328 [32:15<30:47,  1.15it/s] 51%|█████     | 2212/4328 [32:16<31:03,  1.14it/s] 51%|█████     | 2213/4328 [32:17<30:59,  1.14it/s] 51%|█████     | 2214/4328 [32:18<30:56,  1.14it/s] 51%|█████     | 2215/4328 [32:19<30:52,  1.14it/s] 51%|█████     | 2216/4328 [32:20<30:47,  1.14it/s] 51%|█████     | 2217/4328 [32:21<30:43,  1.14it/s] 51%|█████     | 2218/4328 [32:22<30:48,  1.14it/s] 51%|█████▏    | 2219/4328 [32:22<30:45,  1.14it/s] 51%|█████▏    | 2220/4328 [32:23<30:42,  1.14it/s] 51%|█████▏    | 2221/4328 [32:24<30:44,  1.14it/s] 51%|█████▏    | 2222/4328 [32:25<30:40,  1.14it/s] 51%|█████▏    | 2223/4328 [32:26<30:39,  1.14it/s] 51%|█████▏    | 2224/4328 [32:27<30:40,  1.14it/s] 51%|█████▏    | 2225/4328 [32:28<30:39,  1.14it/s] 51%|█████▏    | 2226/4328 [32:29<30:42,  1.14it/s] 51%|█████▏    | 2227/4328 [32:29<30:41,  1.14it/s] 51%|█████▏    | 2228/4328 [32:30<30:37,  1.14it/s] 52%|█████▏    | 2229/4328 [32:31<30:34,  1.14it/s] 52%|█████▏    | 2230/4328 [32:32<30:31,  1.15it/s] 52%|█████▏    | 2231/4328 [32:33<30:39,  1.14it/s] 52%|█████▏    | 2232/4328 [32:34<30:44,  1.14it/s] 52%|█████▏    | 2233/4328 [32:35<30:39,  1.14it/s] 52%|█████▏    | 2234/4328 [32:36<30:35,  1.14it/s] 52%|█████▏    | 2235/4328 [32:36<30:31,  1.14it/s] 52%|█████▏    | 2236/4328 [32:37<30:28,  1.14it/s] 52%|█████▏    | 2237/4328 [32:38<30:43,  1.13it/s] 52%|█████▏    | 2238/4328 [32:39<30:39,  1.14it/s] 52%|█████▏    | 2239/4328 [32:40<30:35,  1.14it/s] 52%|█████▏    | 2240/4328 [32:41<30:31,  1.14it/s] 52%|█████▏    | 2241/4328 [32:42<30:26,  1.14it/s] 52%|█████▏    | 2242/4328 [32:43<30:22,  1.14it/s] 52%|█████▏    | 2243/4328 [32:43<30:26,  1.14it/s] 52%|█████▏    | 2244/4328 [32:44<30:22,  1.14it/s] 52%|█████▏    | 2245/4328 [32:45<30:19,  1.14it/s] 52%|█████▏    | 2246/4328 [32:46<30:22,  1.14it/s] 52%|█████▏    | 2247/4328 [32:47<30:19,  1.14it/s] 52%|█████▏    | 2248/4328 [32:48<30:17,  1.14it/s] 52%|█████▏    | 2249/4328 [32:49<30:15,  1.15it/s] 52%|█████▏    | 2250/4328 [32:50<30:13,  1.15it/s] 52%|█████▏    | 2251/4328 [32:50<30:28,  1.14it/s] 52%|█████▏    | 2252/4328 [32:51<30:24,  1.14it/s] 52%|█████▏    | 2253/4328 [32:52<30:19,  1.14it/s] 52%|█████▏    | 2254/4328 [32:53<30:16,  1.14it/s] 52%|█████▏    | 2255/4328 [32:54<30:13,  1.14it/s] 52%|█████▏    | 2256/4328 [32:55<30:19,  1.14it/s] 52%|█████▏    | 2257/4328 [32:56<30:21,  1.14it/s] 52%|█████▏    | 2258/4328 [32:57<30:18,  1.14it/s] 52%|█████▏    | 2259/4328 [32:57<30:15,  1.14it/s] 52%|█████▏    | 2260/4328 [32:58<30:11,  1.14it/s] 52%|█████▏    | 2261/4328 [32:59<30:06,  1.14it/s] 52%|█████▏    | 2262/4328 [33:00<30:08,  1.14it/s] 52%|█████▏    | 2263/4328 [33:01<30:07,  1.14it/s] 52%|█████▏    | 2264/4328 [33:02<30:04,  1.14it/s] 52%|█████▏    | 2265/4328 [33:03<30:04,  1.14it/s] 52%|█████▏    | 2266/4328 [33:04<30:02,  1.14it/s] 52%|█████▏    | 2267/4328 [33:04<30:00,  1.14it/s] 52%|█████▏    | 2268/4328 [33:05<29:59,  1.14it/s] 52%|█████▏    | 2269/4328 [33:06<29:59,  1.14it/s] 52%|█████▏    | 2270/4328 [33:07<29:57,  1.14it/s] 52%|█████▏    | 2271/4328 [33:08<30:04,  1.14it/s] 52%|█████▏    | 2272/4328 [33:09<30:00,  1.14it/s] 53%|█████▎    | 2273/4328 [33:10<29:57,  1.14it/s] 53%|█████▎    | 2274/4328 [33:11<29:58,  1.14it/s] 53%|█████▎    | 2275/4328 [33:11<29:57,  1.14it/s] 53%|█████▎    | 2276/4328 [33:12<30:03,  1.14it/s] 53%|█████▎    | 2277/4328 [33:13<29:59,  1.14it/s] 53%|█████▎    | 2278/4328 [33:14<29:54,  1.14it/s] 53%|█████▎    | 2279/4328 [33:15<29:55,  1.14it/s] 53%|█████▎    | 2280/4328 [33:16<29:53,  1.14it/s] 53%|█████▎    | 2281/4328 [33:17<29:52,  1.14it/s] 53%|█████▎    | 2282/4328 [33:18<29:59,  1.14it/s] 53%|█████▎    | 2283/4328 [33:18<29:54,  1.14it/s] 53%|█████▎    | 2284/4328 [33:19<29:51,  1.14it/s] 53%|█████▎    | 2285/4328 [33:20<29:46,  1.14it/s] 53%|█████▎    | 2286/4328 [33:21<29:44,  1.14it/s] 53%|█████▎    | 2287/4328 [33:22<29:57,  1.14it/s] 53%|█████▎    | 2288/4328 [33:23<29:53,  1.14it/s] 53%|█████▎    | 2289/4328 [33:24<29:47,  1.14it/s] 53%|█████▎    | 2290/4328 [33:25<29:43,  1.14it/s] 53%|█████▎    | 2291/4328 [33:25<29:39,  1.14it/s] 53%|█████▎    | 2292/4328 [33:26<29:36,  1.15it/s] 53%|█████▎    | 2293/4328 [33:27<29:46,  1.14it/s] 53%|█████▎    | 2294/4328 [33:28<29:42,  1.14it/s] 53%|█████▎    | 2295/4328 [33:29<29:39,  1.14it/s] 53%|█████▎    | 2296/4328 [33:30<29:41,  1.14it/s] 53%|█████▎    | 2297/4328 [33:31<29:37,  1.14it/s] 53%|█████▎    | 2298/4328 [33:32<29:34,  1.14it/s] 53%|█████▎    | 2299/4328 [33:32<29:31,  1.15it/s] 53%|█████▎    | 2300/4328 [33:33<29:29,  1.15it/s] 53%|█████▎    | 2301/4328 [33:34<29:44,  1.14it/s] 53%|█████▎    | 2302/4328 [33:35<29:39,  1.14it/s] 53%|█████▎    | 2303/4328 [33:36<29:34,  1.14it/s] 53%|█████▎    | 2304/4328 [33:37<29:33,  1.14it/s] 53%|█████▎    | 2305/4328 [33:38<29:30,  1.14it/s] 53%|█████▎    | 2306/4328 [33:39<29:27,  1.14it/s] 53%|█████▎    | 2307/4328 [33:39<29:25,  1.14it/s] 53%|█████▎    | 2308/4328 [33:40<29:22,  1.15it/s] 53%|█████▎    | 2309/4328 [33:41<29:29,  1.14it/s] 53%|█████▎    | 2310/4328 [33:42<29:35,  1.14it/s] 53%|█████▎    | 2311/4328 [33:43<29:30,  1.14it/s] 53%|█████▎    | 2312/4328 [33:44<29:27,  1.14it/s] 53%|█████▎    | 2313/4328 [33:45<29:23,  1.14it/s] 53%|█████▎    | 2314/4328 [33:46<29:19,  1.14it/s] 53%|█████▎    | 2315/4328 [33:46<29:24,  1.14it/s] 54%|█████▎    | 2316/4328 [33:47<29:22,  1.14it/s] 54%|█████▎    | 2317/4328 [33:48<29:19,  1.14it/s] 54%|█████▎    | 2318/4328 [33:49<29:19,  1.14it/s] 54%|█████▎    | 2319/4328 [33:50<29:18,  1.14it/s] 54%|█████▎    | 2320/4328 [33:51<29:16,  1.14it/s] 54%|█████▎    | 2321/4328 [33:52<29:24,  1.14it/s] 54%|█████▎    | 2322/4328 [33:53<29:19,  1.14it/s] 54%|█████▎    | 2323/4328 [33:53<29:16,  1.14it/s] 54%|█████▎    | 2324/4328 [33:54<29:11,  1.14it/s] 54%|█████▎    | 2325/4328 [33:55<29:08,  1.15it/s] 54%|█████▎    | 2326/4328 [33:56<29:23,  1.14it/s] 54%|█████▍    | 2327/4328 [33:57<29:18,  1.14it/s] 54%|█████▍    | 2328/4328 [33:58<29:13,  1.14it/s] 54%|█████▍    | 2329/4328 [33:59<29:10,  1.14it/s] 54%|█████▍    | 2330/4328 [34:00<29:05,  1.14it/s] 54%|█████▍    | 2331/4328 [34:01<29:11,  1.14it/s] 54%|█████▍    | 2332/4328 [34:01<29:16,  1.14it/s] 54%|█████▍    | 2333/4328 [34:02<29:11,  1.14it/s] 54%|█████▍    | 2334/4328 [34:03<29:06,  1.14it/s] 54%|█████▍    | 2335/4328 [34:04<29:02,  1.14it/s] 54%|█████▍    | 2336/4328 [34:05<28:59,  1.15it/s] 54%|█████▍    | 2337/4328 [34:06<29:12,  1.14it/s] 54%|█████▍    | 2338/4328 [34:07<29:08,  1.14it/s] 54%|█████▍    | 2339/4328 [34:08<29:04,  1.14it/s] 54%|█████▍    | 2340/4328 [34:08<29:00,  1.14it/s] 54%|█████▍    | 2341/4328 [34:09<28:56,  1.14it/s] 54%|█████▍    | 2342/4328 [34:10<29:02,  1.14it/s] 54%|█████▍    | 2343/4328 [34:11<29:06,  1.14it/s] 54%|█████▍    | 2344/4328 [34:12<29:00,  1.14it/s] 54%|█████▍    | 2345/4328 [34:13<28:56,  1.14it/s] 54%|█████▍    | 2346/4328 [34:14<28:52,  1.14it/s] 54%|█████▍    | 2347/4328 [34:15<28:49,  1.15it/s] 54%|█████▍    | 2348/4328 [34:15<29:03,  1.14it/s] 54%|█████▍    | 2349/4328 [34:16<28:58,  1.14it/s] 54%|█████▍    | 2350/4328 [34:17<28:52,  1.14it/s] 54%|█████▍    | 2351/4328 [34:18<28:48,  1.14it/s] 54%|█████▍    | 2352/4328 [34:19<28:45,  1.15it/s] 54%|█████▍    | 2353/4328 [34:20<28:50,  1.14it/s] 54%|█████▍    | 2354/4328 [34:21<28:55,  1.14it/s] 54%|█████▍    | 2355/4328 [34:22<28:50,  1.14it/s] 54%|█████▍    | 2356/4328 [34:22<28:46,  1.14it/s] 54%|█████▍    | 2357/4328 [34:23<28:48,  1.14it/s] 54%|█████▍    | 2358/4328 [34:24<28:44,  1.14it/s] 55%|█████▍    | 2359/4328 [34:25<28:41,  1.14it/s] 55%|█████▍    | 2360/4328 [34:26<28:38,  1.15it/s] 55%|█████▍    | 2361/4328 [34:27<28:36,  1.15it/s] 55%|█████▍    | 2362/4328 [34:28<28:50,  1.14it/s] 55%|█████▍    | 2363/4328 [34:29<28:46,  1.14it/s] 55%|█████▍    | 2364/4328 [34:29<28:40,  1.14it/s] 55%|█████▍    | 2365/4328 [34:30<28:37,  1.14it/s] 55%|█████▍    | 2366/4328 [34:31<28:33,  1.14it/s] 55%|█████▍    | 2367/4328 [34:32<28:39,  1.14it/s] 55%|█████▍    | 2368/4328 [34:33<28:44,  1.14it/s] 55%|█████▍    | 2369/4328 [34:34<28:39,  1.14it/s] 55%|█████▍    | 2370/4328 [34:35<28:35,  1.14it/s] 55%|█████▍    | 2371/4328 [34:36<28:31,  1.14it/s] 55%|█████▍    | 2372/4328 [34:36<28:28,  1.14it/s] 55%|█████▍    | 2373/4328 [34:37<28:42,  1.14it/s] 55%|█████▍    | 2374/4328 [34:38<28:37,  1.14it/s] 55%|█████▍    | 2375/4328 [34:39<28:32,  1.14it/s] 55%|█████▍    | 2376/4328 [34:40<28:28,  1.14it/s] 55%|█████▍    | 2377/4328 [34:41<28:24,  1.14it/s] 55%|█████▍    | 2378/4328 [34:42<28:29,  1.14it/s] 55%|█████▍    | 2379/4328 [34:43<28:35,  1.14it/s] 55%|█████▍    | 2380/4328 [34:43<28:29,  1.14it/s] 55%|█████▌    | 2381/4328 [34:44<28:24,  1.14it/s] 55%|█████▌    | 2382/4328 [34:45<28:25,  1.14it/s] 55%|█████▌    | 2383/4328 [34:46<28:22,  1.14it/s] 55%|█████▌    | 2384/4328 [34:47<28:26,  1.14it/s] 55%|█████▌    | 2385/4328 [34:48<28:23,  1.14it/s] 55%|█████▌    | 2386/4328 [34:49<28:20,  1.14it/s] 55%|█████▌    | 2387/4328 [34:50<28:16,  1.14it/s] 55%|█████▌    | 2388/4328 [34:50<28:13,  1.15it/s] 55%|█████▌    | 2389/4328 [34:51<28:20,  1.14it/s] 55%|█████▌    | 2390/4328 [34:52<28:25,  1.14it/s] 55%|█████▌    | 2391/4328 [34:53<28:19,  1.14it/s] 55%|█████▌    | 2392/4328 [34:54<28:16,  1.14it/s] 55%|█████▌    | 2393/4328 [34:55<28:11,  1.14it/s] 55%|█████▌    | 2394/4328 [34:56<28:08,  1.15it/s] 55%|█████▌    | 2395/4328 [34:57<28:21,  1.14it/s] 55%|█████▌    | 2396/4328 [34:57<28:16,  1.14it/s] 55%|█████▌    | 2397/4328 [34:58<28:11,  1.14it/s] 55%|█████▌    | 2398/4328 [34:59<28:11,  1.14it/s] 55%|█████▌    | 2399/4328 [35:00<28:08,  1.14it/s] 55%|█████▌    | 2400/4328 [35:01<28:04,  1.14it/s] 55%|█████▌    | 2401/4328 [35:02<28:01,  1.15it/s] 55%|█████▌    | 2402/4328 [35:03<28:00,  1.15it/s] 56%|█████▌    | 2403/4328 [35:04<28:05,  1.14it/s] 56%|█████▌    | 2404/4328 [35:04<28:10,  1.14it/s] 56%|█████▌    | 2405/4328 [35:05<28:06,  1.14it/s] 56%|█████▌    | 2406/4328 [35:06<28:02,  1.14it/s] 56%|█████▌    | 2407/4328 [35:07<27:58,  1.14it/s] 56%|█████▌    | 2408/4328 [35:08<27:55,  1.15it/s] 56%|█████▌    | 2409/4328 [35:09<28:09,  1.14it/s] 56%|█████▌    | 2410/4328 [35:10<28:04,  1.14it/s] 56%|█████▌    | 2411/4328 [35:11<28:00,  1.14it/s] 56%|█████▌    | 2412/4328 [35:11<28:00,  1.14it/s] 56%|█████▌    | 2413/4328 [35:12<27:56,  1.14it/s] 56%|█████▌    | 2414/4328 [35:13<27:53,  1.14it/s] 56%|█████▌    | 2415/4328 [35:14<27:53,  1.14it/s] 56%|█████▌    | 2416/4328 [35:15<27:51,  1.14it/s] 56%|█████▌    | 2417/4328 [35:16<27:50,  1.14it/s] 56%|█████▌    | 2418/4328 [35:17<27:55,  1.14it/s] 56%|█████▌    | 2419/4328 [35:18<27:50,  1.14it/s] 56%|█████▌    | 2420/4328 [35:18<27:47,  1.14it/s] 56%|█████▌    | 2421/4328 [35:19<27:48,  1.14it/s] 56%|█████▌    | 2422/4328 [35:20<27:45,  1.14it/s] 56%|█████▌    | 2423/4328 [35:21<27:42,  1.15it/s] 56%|█████▌    | 2424/4328 [35:22<27:44,  1.14it/s] 56%|█████▌    | 2425/4328 [35:23<27:42,  1.14it/s] 56%|█████▌    | 2426/4328 [35:24<27:48,  1.14it/s] 56%|█████▌    | 2427/4328 [35:25<27:45,  1.14it/s] 56%|█████▌    | 2428/4328 [35:25<27:41,  1.14it/s] 56%|█████▌    | 2429/4328 [35:26<27:42,  1.14it/s] 56%|█████▌    | 2430/4328 [35:27<27:39,  1.14it/s] 56%|█████▌    | 2431/4328 [35:28<27:36,  1.14it/s] 56%|█████▌    | 2432/4328 [35:29<27:37,  1.14it/s] 56%|█████▌    | 2433/4328 [35:30<27:35,  1.14it/s] 56%|█████▌    | 2434/4328 [35:31<27:34,  1.14it/s] 56%|█████▋    | 2435/4328 [35:32<27:41,  1.14it/s] 56%|█████▋    | 2436/4328 [35:32<27:36,  1.14it/s] 56%|█████▋    | 2437/4328 [35:33<27:34,  1.14it/s] 56%|█████▋    | 2438/4328 [35:34<27:31,  1.14it/s] 56%|█████▋    | 2439/4328 [35:35<27:27,  1.15it/s] 56%|█████▋    | 2440/4328 [35:36<27:40,  1.14it/s] 56%|█████▋    | 2441/4328 [35:37<27:37,  1.14it/s] 56%|█████▋    | 2442/4328 [35:38<27:33,  1.14it/s] 56%|█████▋    | 2443/4328 [35:39<27:32,  1.14it/s] 56%|█████▋    | 2444/4328 [35:39<27:28,  1.14it/s] 56%|█████▋    | 2445/4328 [35:40<27:24,  1.14it/s] 57%|█████▋    | 2446/4328 [35:41<27:23,  1.14it/s] 57%|█████▋    | 2447/4328 [35:42<27:21,  1.15it/s] 57%|█████▋    | 2448/4328 [35:43<27:20,  1.15it/s] 57%|█████▋    | 2449/4328 [35:44<27:21,  1.15it/s] 57%|█████▋    | 2450/4328 [35:45<27:19,  1.15it/s] 57%|█████▋    | 2451/4328 [35:46<27:18,  1.15it/s] 57%|█████▋    | 2452/4328 [35:46<27:25,  1.14it/s] 57%|█████▋    | 2453/4328 [35:47<27:21,  1.14it/s] 57%|█████▋    | 2454/4328 [35:48<27:18,  1.14it/s] 57%|█████▋    | 2455/4328 [35:49<27:19,  1.14it/s] 57%|█████▋    | 2456/4328 [35:50<27:16,  1.14it/s] 57%|█████▋    | 2457/4328 [35:51<27:13,  1.15it/s] 57%|█████▋    | 2458/4328 [35:52<27:11,  1.15it/s] 57%|█████▋    | 2459/4328 [35:53<27:09,  1.15it/s] 57%|█████▋    | 2460/4328 [35:53<27:23,  1.14it/s] 57%|█████▋    | 2461/4328 [35:54<27:20,  1.14it/s] 57%|█████▋    | 2462/4328 [35:55<27:14,  1.14it/s] 57%|█████▋    | 2463/4328 [35:56<27:10,  1.14it/s] 57%|█████▋    | 2464/4328 [35:57<27:07,  1.15it/s] 57%|█████▋    | 2465/4328 [35:58<27:12,  1.14it/s] 57%|█████▋    | 2466/4328 [35:59<27:17,  1.14it/s] 57%|█████▋    | 2467/4328 [36:00<27:11,  1.14it/s] 57%|█████▋    | 2468/4328 [36:00<27:07,  1.14it/s] 57%|█████▋    | 2469/4328 [36:01<27:07,  1.14it/s] 57%|█████▋    | 2470/4328 [36:02<27:03,  1.14it/s] 57%|█████▋    | 2471/4328 [36:03<27:01,  1.15it/s] 57%|█████▋    | 2472/4328 [36:04<27:01,  1.14it/s] 57%|█████▋    | 2473/4328 [36:05<27:00,  1.15it/s] 57%|█████▋    | 2474/4328 [36:06<27:06,  1.14it/s] 57%|█████▋    | 2475/4328 [36:07<27:04,  1.14it/s] 57%|█████▋    | 2476/4328 [36:07<27:00,  1.14it/s] 57%|█████▋    | 2477/4328 [36:08<26:58,  1.14it/s] 57%|█████▋    | 2478/4328 [36:09<26:55,  1.15it/s] 57%|█████▋    | 2479/4328 [36:10<26:52,  1.15it/s] 57%|█████▋    | 2480/4328 [36:11<27:00,  1.14it/s] 57%|█████▋    | 2481/4328 [36:12<26:57,  1.14it/s] 57%|█████▋    | 2482/4328 [36:13<26:54,  1.14it/s] 57%|█████▋    | 2483/4328 [36:14<26:58,  1.14it/s] 57%|█████▋    | 2484/4328 [36:14<26:53,  1.14it/s] 57%|█████▋    | 2485/4328 [36:15<26:50,  1.14it/s] 57%|█████▋    | 2486/4328 [36:16<26:51,  1.14it/s] 57%|█████▋    | 2487/4328 [36:17<26:49,  1.14it/s] 57%|█████▋    | 2488/4328 [36:18<26:54,  1.14it/s] 58%|█████▊    | 2489/4328 [36:19<26:52,  1.14it/s] 58%|█████▊    | 2490/4328 [36:20<26:49,  1.14it/s] 58%|█████▊    | 2491/4328 [36:21<26:49,  1.14it/s] 58%|█████▊    | 2492/4328 [36:21<26:46,  1.14it/s] 58%|█████▊    | 2493/4328 [36:22<26:43,  1.14it/s] 58%|█████▊    | 2494/4328 [36:23<26:43,  1.14it/s] 58%|█████▊    | 2495/4328 [36:24<26:42,  1.14it/s] 58%|█████▊    | 2496/4328 [36:25<26:40,  1.14it/s] 58%|█████▊    | 2497/4328 [36:26<26:46,  1.14it/s] 58%|█████▊    | 2498/4328 [36:27<26:41,  1.14it/s] 58%|█████▊    | 2499/4328 [36:28<26:38,  1.14it/s] 58%|█████▊    | 2500/4328 [36:28<26:39,  1.14it/s] 58%|█████▊    | 2501/4328 [36:29<26:35,  1.14it/s] 58%|█████▊    | 2502/4328 [36:30<26:32,  1.15it/s] 58%|█████▊    | 2503/4328 [36:31<26:34,  1.14it/s] 58%|█████▊    | 2504/4328 [36:32<26:32,  1.15it/s] 58%|█████▊    | 2505/4328 [36:33<26:38,  1.14it/s] 58%|█████▊    | 2506/4328 [36:34<26:36,  1.14it/s] 58%|█████▊    | 2507/4328 [36:35<26:33,  1.14it/s] 58%|█████▊    | 2508/4328 [36:35<26:34,  1.14it/s] 58%|█████▊    | 2509/4328 [36:36<26:31,  1.14it/s] 58%|█████▊    | 2510/4328 [36:37<26:28,  1.14it/s] 58%|█████▊    | 2511/4328 [36:38<26:26,  1.15it/s] 58%|█████▊    | 2512/4328 [36:39<26:24,  1.15it/s] 58%|█████▊    | 2513/4328 [36:40<26:29,  1.14it/s] 58%|█████▊    | 2514/4328 [36:41<26:34,  1.14it/s] 58%|█████▊    | 2515/4328 [36:42<26:30,  1.14it/s] 58%|█████▊    | 2516/4328 [36:42<26:26,  1.14it/s] 58%|█████▊    | 2517/4328 [36:43<26:22,  1.14it/s] 58%|█████▊    | 2518/4328 [36:44<26:19,  1.15it/s] 58%|█████▊    | 2519/4328 [36:45<26:25,  1.14it/s] 58%|█████▊    | 2520/4328 [36:46<26:23,  1.14it/s] 58%|█████▊    | 2521/4328 [36:47<26:21,  1.14it/s] 58%|█████▊    | 2522/4328 [36:48<26:22,  1.14it/s] 58%|█████▊    | 2523/4328 [36:49<26:20,  1.14it/s] 58%|█████▊    | 2524/4328 [36:49<26:17,  1.14it/s] 58%|█████▊    | 2525/4328 [36:50<26:16,  1.14it/s] 58%|█████▊    | 2526/4328 [36:51<26:14,  1.14it/s] 58%|█████▊    | 2527/4328 [36:52<26:12,  1.15it/s] 58%|█████▊    | 2528/4328 [36:53<26:11,  1.15it/s] 58%|█████▊    | 2529/4328 [36:54<26:10,  1.15it/s] 58%|█████▊    | 2530/4328 [36:55<26:09,  1.15it/s] 58%|█████▊    | 2531/4328 [36:56<26:16,  1.14it/s] 59%|█████▊    | 2532/4328 [36:56<26:12,  1.14it/s] 59%|█████▊    | 2533/4328 [36:57<26:10,  1.14it/s] 59%|█████▊    | 2534/4328 [36:58<26:07,  1.14it/s] 59%|█████▊    | 2535/4328 [36:59<26:04,  1.15it/s] 59%|█████▊    | 2536/4328 [37:00<26:10,  1.14it/s] 59%|█████▊    | 2537/4328 [37:01<26:08,  1.14it/s] 59%|█████▊    | 2538/4328 [37:02<26:05,  1.14it/s] 59%|█████▊    | 2539/4328 [37:03<26:07,  1.14it/s] 59%|█████▊    | 2540/4328 [37:03<26:05,  1.14it/s] 59%|█████▊    | 2541/4328 [37:04<26:01,  1.14it/s] 59%|█████▊    | 2542/4328 [37:05<26:00,  1.14it/s] 59%|█████▉    | 2543/4328 [37:06<25:58,  1.15it/s] 59%|█████▉    | 2544/4328 [37:07<25:56,  1.15it/s] 59%|█████▉    | 2545/4328 [37:08<25:57,  1.14it/s] 59%|█████▉    | 2546/4328 [37:09<25:55,  1.15it/s] 59%|█████▉    | 2547/4328 [37:10<25:55,  1.15it/s] 59%|█████▉    | 2548/4328 [37:10<26:01,  1.14it/s] 59%|█████▉    | 2549/4328 [37:11<25:57,  1.14it/s] 59%|█████▉    | 2550/4328 [37:12<25:54,  1.14it/s] 59%|█████▉    | 2551/4328 [37:13<25:55,  1.14it/s] 59%|█████▉    | 2552/4328 [37:14<25:51,  1.14it/s] 59%|█████▉    | 2553/4328 [37:15<25:49,  1.15it/s] 59%|█████▉    | 2554/4328 [37:16<25:50,  1.14it/s] 59%|█████▉    | 2555/4328 [37:17<25:49,  1.14it/s] 59%|█████▉    | 2556/4328 [37:17<25:55,  1.14it/s] 59%|█████▉    | 2557/4328 [37:18<25:52,  1.14it/s] 59%|█████▉    | 2558/4328 [37:19<25:48,  1.14it/s] 59%|█████▉    | 2559/4328 [37:20<25:48,  1.14it/s] 59%|█████▉    | 2560/4328 [37:21<25:46,  1.14it/s] 59%|█████▉    | 2561/4328 [37:22<25:43,  1.14it/s] 59%|█████▉    | 2562/4328 [37:23<25:41,  1.15it/s] 59%|█████▉    | 2563/4328 [37:24<25:39,  1.15it/s] 59%|█████▉    | 2564/4328 [37:24<25:45,  1.14it/s] 59%|█████▉    | 2565/4328 [37:25<25:50,  1.14it/s] 59%|█████▉    | 2566/4328 [37:26<25:45,  1.14it/s] 59%|█████▉    | 2567/4328 [37:27<25:42,  1.14it/s] 59%|█████▉    | 2568/4328 [37:28<25:38,  1.14it/s] 59%|█████▉    | 2569/4328 [37:29<25:35,  1.15it/s] 59%|█████▉    | 2570/4328 [37:30<25:47,  1.14it/s] 59%|█████▉    | 2571/4328 [37:31<25:43,  1.14it/s] 59%|█████▉    | 2572/4328 [37:31<25:38,  1.14it/s] 59%|█████▉    | 2573/4328 [37:32<25:38,  1.14it/s] 59%|█████▉    | 2574/4328 [37:33<25:34,  1.14it/s] 59%|█████▉    | 2575/4328 [37:34<25:33,  1.14it/s] 60%|█████▉    | 2576/4328 [37:35<25:39,  1.14it/s] 60%|█████▉    | 2577/4328 [37:36<25:35,  1.14it/s] 60%|█████▉    | 2578/4328 [37:37<25:31,  1.14it/s] 60%|█████▉    | 2579/4328 [37:38<25:32,  1.14it/s] 60%|█████▉    | 2580/4328 [37:38<25:28,  1.14it/s] 60%|█████▉    | 2581/4328 [37:39<25:26,  1.14it/s] 60%|█████▉    | 2582/4328 [37:40<25:23,  1.15it/s] 60%|█████▉    | 2583/4328 [37:41<25:21,  1.15it/s] 60%|█████▉    | 2584/4328 [37:42<25:34,  1.14it/s] 60%|█████▉    | 2585/4328 [37:43<25:32,  1.14it/s] 60%|█████▉    | 2586/4328 [37:44<25:29,  1.14it/s] 60%|█████▉    | 2587/4328 [37:45<25:26,  1.14it/s] 60%|█████▉    | 2588/4328 [37:45<25:22,  1.14it/s] 60%|█████▉    | 2589/4328 [37:46<25:19,  1.14it/s] 60%|█████▉    | 2590/4328 [37:47<25:23,  1.14it/s] 60%|█████▉    | 2591/4328 [37:48<25:20,  1.14it/s] 60%|█████▉    | 2592/4328 [37:49<25:17,  1.14it/s] 60%|█████▉    | 2593/4328 [37:50<25:19,  1.14it/s] 60%|█████▉    | 2594/4328 [37:51<25:17,  1.14it/s] 60%|█████▉    | 2595/4328 [37:52<25:23,  1.14it/s] 60%|█████▉    | 2596/4328 [37:53<25:21,  1.14it/s] 60%|██████    | 2597/4328 [37:53<25:16,  1.14it/s] 60%|██████    | 2598/4328 [37:54<25:13,  1.14it/s] 60%|██████    | 2599/4328 [37:55<25:09,  1.15it/s] 60%|██████    | 2600/4328 [37:56<25:17,  1.14it/s] 60%|██████    | 2601/4328 [37:57<25:20,  1.14it/s] 60%|██████    | 2602/4328 [37:58<25:16,  1.14it/s] 60%|██████    | 2603/4328 [37:59<25:12,  1.14it/s] 60%|██████    | 2604/4328 [38:00<25:08,  1.14it/s] 60%|██████    | 2605/4328 [38:00<25:06,  1.14it/s] 60%|██████    | 2606/4328 [38:01<25:18,  1.13it/s] 60%|██████    | 2607/4328 [38:02<25:15,  1.14it/s] 60%|██████    | 2608/4328 [38:03<25:11,  1.14it/s] 60%|██████    | 2609/4328 [38:04<25:07,  1.14it/s] 60%|██████    | 2610/4328 [38:05<25:03,  1.14it/s] 60%|██████    | 2611/4328 [38:06<25:01,  1.14it/s] 60%|██████    | 2612/4328 [38:07<25:07,  1.14it/s] 60%|██████    | 2613/4328 [38:07<25:06,  1.14it/s] 60%|██████    | 2614/4328 [38:08<25:02,  1.14it/s] 60%|██████    | 2615/4328 [38:09<24:59,  1.14it/s] 60%|██████    | 2616/4328 [38:10<24:59,  1.14it/s] 60%|██████    | 2617/4328 [38:11<25:07,  1.13it/s] 60%|██████    | 2618/4328 [38:12<25:03,  1.14it/s] 61%|██████    | 2619/4328 [38:13<25:00,  1.14it/s] 61%|██████    | 2620/4328 [38:14<25:04,  1.14it/s] 61%|██████    | 2621/4328 [38:14<24:59,  1.14it/s] 61%|██████    | 2622/4328 [38:15<24:56,  1.14it/s] 61%|██████    | 2623/4328 [38:16<24:53,  1.14it/s] 61%|██████    | 2624/4328 [38:17<24:50,  1.14it/s] 61%|██████    | 2625/4328 [38:18<25:02,  1.13it/s] 61%|██████    | 2626/4328 [38:19<24:58,  1.14it/s] 61%|██████    | 2627/4328 [38:20<24:54,  1.14it/s] 61%|██████    | 2628/4328 [38:21<24:49,  1.14it/s] 61%|██████    | 2629/4328 [38:21<24:45,  1.14it/s] 61%|██████    | 2630/4328 [38:22<24:50,  1.14it/s] 61%|██████    | 2631/4328 [38:23<24:53,  1.14it/s] 61%|██████    | 2632/4328 [38:24<24:50,  1.14it/s] 61%|██████    | 2633/4328 [38:25<24:46,  1.14it/s] 61%|██████    | 2634/4328 [38:26<24:42,  1.14it/s] 61%|██████    | 2635/4328 [38:27<24:39,  1.14it/s] 61%|██████    | 2636/4328 [38:28<24:40,  1.14it/s] 61%|██████    | 2637/4328 [38:28<24:39,  1.14it/s] 61%|██████    | 2638/4328 [38:29<24:36,  1.14it/s] 61%|██████    | 2639/4328 [38:30<24:37,  1.14it/s] 61%|██████    | 2640/4328 [38:31<24:36,  1.14it/s] 61%|██████    | 2641/4328 [38:32<24:34,  1.14it/s] 61%|██████    | 2642/4328 [38:33<24:36,  1.14it/s] 61%|██████    | 2643/4328 [38:34<24:34,  1.14it/s] 61%|██████    | 2644/4328 [38:35<24:32,  1.14it/s] 61%|██████    | 2645/4328 [38:35<24:35,  1.14it/s] 61%|██████    | 2646/4328 [38:36<24:32,  1.14it/s] 61%|██████    | 2647/4328 [38:37<24:30,  1.14it/s] 61%|██████    | 2648/4328 [38:38<24:30,  1.14it/s] 61%|██████    | 2649/4328 [38:39<24:29,  1.14it/s] 61%|██████    | 2650/4328 [38:40<24:32,  1.14it/s] 61%|██████▏   | 2651/4328 [38:41<24:29,  1.14it/s] 61%|██████▏   | 2652/4328 [38:42<24:26,  1.14it/s] 61%|██████▏   | 2653/4328 [38:42<24:27,  1.14it/s] 61%|██████▏   | 2654/4328 [38:43<24:24,  1.14it/s] 61%|██████▏   | 2655/4328 [38:44<24:21,  1.14it/s] 61%|██████▏   | 2656/4328 [38:45<24:19,  1.15it/s] 61%|██████▏   | 2657/4328 [38:46<24:18,  1.15it/s] 61%|██████▏   | 2658/4328 [38:47<24:22,  1.14it/s] 61%|██████▏   | 2659/4328 [38:48<24:27,  1.14it/s] 61%|██████▏   | 2660/4328 [38:49<24:23,  1.14it/s] 61%|██████▏   | 2661/4328 [38:49<24:20,  1.14it/s] 62%|██████▏   | 2662/4328 [38:50<24:17,  1.14it/s] 62%|██████▏   | 2663/4328 [38:51<24:14,  1.14it/s] 62%|██████▏   | 2664/4328 [38:52<24:26,  1.13it/s] 62%|██████▏   | 2665/4328 [38:53<24:22,  1.14it/s] 62%|██████▏   | 2666/4328 [38:54<24:17,  1.14it/s] 62%|██████▏   | 2667/4328 [38:55<24:13,  1.14it/s] 62%|██████▏   | 2668/4328 [38:56<24:10,  1.14it/s] 62%|██████▏   | 2669/4328 [38:56<24:14,  1.14it/s] 62%|██████▏   | 2670/4328 [38:57<24:19,  1.14it/s] 62%|██████▏   | 2671/4328 [38:58<24:14,  1.14it/s] 62%|██████▏   | 2672/4328 [38:59<24:10,  1.14it/s] 62%|██████▏   | 2673/4328 [39:00<24:08,  1.14it/s] 62%|██████▏   | 2674/4328 [39:01<24:05,  1.14it/s] 62%|██████▏   | 2675/4328 [39:02<24:17,  1.13it/s] 62%|██████▏   | 2676/4328 [39:03<24:14,  1.14it/s] 62%|██████▏   | 2677/4328 [39:04<24:10,  1.14it/s] 62%|██████▏   | 2678/4328 [39:04<24:06,  1.14it/s] 62%|██████▏   | 2679/4328 [39:05<24:02,  1.14it/s] 62%|██████▏   | 2680/4328 [39:06<23:59,  1.15it/s] 62%|██████▏   | 2681/4328 [39:07<24:04,  1.14it/s] 62%|██████▏   | 2682/4328 [39:08<24:01,  1.14it/s] 62%|██████▏   | 2683/4328 [39:09<23:58,  1.14it/s] 62%|██████▏   | 2684/4328 [39:10<23:59,  1.14it/s] 62%|██████▏   | 2685/4328 [39:11<23:56,  1.14it/s] 62%|██████▏   | 2686/4328 [39:11<23:55,  1.14it/s] 62%|██████▏   | 2687/4328 [39:12<23:52,  1.15it/s] 62%|██████▏   | 2688/4328 [39:13<23:50,  1.15it/s] 62%|██████▏   | 2689/4328 [39:14<24:01,  1.14it/s] 62%|██████▏   | 2690/4328 [39:15<23:58,  1.14it/s] 62%|██████▏   | 2691/4328 [39:16<23:55,  1.14it/s] 62%|██████▏   | 2692/4328 [39:17<23:53,  1.14it/s] 62%|██████▏   | 2693/4328 [39:18<23:49,  1.14it/s] 62%|██████▏   | 2694/4328 [39:18<23:54,  1.14it/s] 62%|██████▏   | 2695/4328 [39:19<23:55,  1.14it/s] 62%|██████▏   | 2696/4328 [39:20<23:52,  1.14it/s] 62%|██████▏   | 2697/4328 [39:21<23:56,  1.14it/s] 62%|██████▏   | 2698/4328 [39:22<23:52,  1.14it/s] 62%|██████▏   | 2699/4328 [39:23<23:48,  1.14it/s] 62%|██████▏   | 2700/4328 [39:24<23:44,  1.14it/s] 62%|██████▏   | 2701/4328 [39:25<23:41,  1.14it/s] 62%|██████▏   | 2702/4328 [39:25<23:45,  1.14it/s] 62%|██████▏   | 2703/4328 [39:26<23:50,  1.14it/s] 62%|██████▏   | 2704/4328 [39:27<23:45,  1.14it/s] 62%|██████▎   | 2705/4328 [39:28<23:40,  1.14it/s] 63%|██████▎   | 2706/4328 [39:29<23:40,  1.14it/s] 63%|██████▎   | 2707/4328 [39:30<23:37,  1.14it/s] 63%|██████▎   | 2708/4328 [39:31<23:34,  1.15it/s] 63%|██████▎   | 2709/4328 [39:32<23:35,  1.14it/s] 63%|██████▎   | 2710/4328 [39:32<23:33,  1.14it/s] 63%|██████▎   | 2711/4328 [39:33<23:38,  1.14it/s] 63%|██████▎   | 2712/4328 [39:34<23:36,  1.14it/s] 63%|██████▎   | 2713/4328 [39:35<23:33,  1.14it/s] 63%|██████▎   | 2714/4328 [39:36<23:34,  1.14it/s] 63%|██████▎   | 2715/4328 [39:37<23:32,  1.14it/s] 63%|██████▎   | 2716/4328 [39:38<23:28,  1.14it/s] 63%|██████▎   | 2717/4328 [39:39<23:28,  1.14it/s] 63%|██████▎   | 2718/4328 [39:39<23:27,  1.14it/s] 63%|██████▎   | 2719/4328 [39:40<23:26,  1.14it/s] 63%|██████▎   | 2720/4328 [39:41<23:31,  1.14it/s] 63%|██████▎   | 2721/4328 [39:42<23:28,  1.14it/s] 63%|██████▎   | 2722/4328 [39:43<23:25,  1.14it/s] 63%|██████▎   | 2723/4328 [39:44<23:22,  1.14it/s] 63%|██████▎   | 2724/4328 [39:45<23:19,  1.15it/s] 63%|██████▎   | 2725/4328 [39:46<23:24,  1.14it/s] 63%|██████▎   | 2726/4328 [39:46<23:22,  1.14it/s] 63%|██████▎   | 2727/4328 [39:47<23:19,  1.14it/s] 63%|██████▎   | 2728/4328 [39:48<23:21,  1.14it/s] 63%|██████▎   | 2729/4328 [39:49<23:20,  1.14it/s] 63%|██████▎   | 2730/4328 [39:50<23:17,  1.14it/s] 63%|██████▎   | 2731/4328 [39:51<23:16,  1.14it/s] 63%|██████▎   | 2732/4328 [39:52<23:15,  1.14it/s] 63%|██████▎   | 2733/4328 [39:53<23:13,  1.14it/s] 63%|██████▎   | 2734/4328 [39:53<23:10,  1.15it/s] 63%|██████▎   | 2735/4328 [39:54<23:08,  1.15it/s] 63%|██████▎   | 2736/4328 [39:55<23:14,  1.14it/s] 63%|██████▎   | 2737/4328 [39:56<23:18,  1.14it/s] 63%|██████▎   | 2738/4328 [39:57<23:15,  1.14it/s] 63%|██████▎   | 2739/4328 [39:58<23:11,  1.14it/s] 63%|██████▎   | 2740/4328 [39:59<23:08,  1.14it/s] 63%|██████▎   | 2741/4328 [40:00<23:05,  1.15it/s] 63%|██████▎   | 2742/4328 [40:00<23:17,  1.14it/s] 63%|██████▎   | 2743/4328 [40:01<23:14,  1.14it/s] 63%|██████▎   | 2744/4328 [40:02<23:11,  1.14it/s] 63%|██████▎   | 2745/4328 [40:03<23:08,  1.14it/s] 63%|██████▎   | 2746/4328 [40:04<23:04,  1.14it/s] 63%|██████▎   | 2747/4328 [40:05<23:02,  1.14it/s] 63%|██████▎   | 2748/4328 [40:06<23:04,  1.14it/s] 64%|██████▎   | 2749/4328 [40:07<23:02,  1.14it/s] 64%|██████▎   | 2750/4328 [40:07<22:59,  1.14it/s] 64%|██████▎   | 2751/4328 [40:08<23:01,  1.14it/s] 64%|██████▎   | 2752/4328 [40:09<22:58,  1.14it/s] 64%|██████▎   | 2753/4328 [40:10<22:56,  1.14it/s] 64%|██████▎   | 2754/4328 [40:11<22:53,  1.15it/s] 64%|██████▎   | 2755/4328 [40:12<22:51,  1.15it/s] 64%|██████▎   | 2756/4328 [40:13<23:03,  1.14it/s] 64%|██████▎   | 2757/4328 [40:14<23:01,  1.14it/s] 64%|██████▎   | 2758/4328 [40:14<22:57,  1.14it/s] 64%|██████▎   | 2759/4328 [40:15<22:54,  1.14it/s] 64%|██████▍   | 2760/4328 [40:16<22:50,  1.14it/s] 64%|██████▍   | 2761/4328 [40:17<22:47,  1.15it/s] 64%|██████▍   | 2762/4328 [40:18<22:52,  1.14it/s] 64%|██████▍   | 2763/4328 [40:19<22:49,  1.14it/s] 64%|██████▍   | 2764/4328 [40:20<22:46,  1.14it/s] 64%|██████▍   | 2765/4328 [40:21<22:48,  1.14it/s] 64%|██████▍   | 2766/4328 [40:21<22:45,  1.14it/s] 64%|██████▍   | 2767/4328 [40:22<22:42,  1.15it/s] 64%|██████▍   | 2768/4328 [40:23<22:43,  1.14it/s] 64%|██████▍   | 2769/4328 [40:24<22:40,  1.15it/s] 64%|██████▍   | 2770/4328 [40:25<22:38,  1.15it/s] 64%|██████▍   | 2771/4328 [40:26<22:40,  1.14it/s] 64%|██████▍   | 2772/4328 [40:27<22:39,  1.14it/s] 64%|██████▍   | 2773/4328 [40:28<22:42,  1.14it/s] 64%|██████▍   | 2774/4328 [40:28<22:40,  1.14it/s] 64%|██████▍   | 2775/4328 [40:29<22:37,  1.14it/s] 64%|██████▍   | 2776/4328 [40:30<22:38,  1.14it/s] 64%|██████▍   | 2777/4328 [40:31<22:35,  1.14it/s] 64%|██████▍   | 2778/4328 [40:32<22:32,  1.15it/s] 64%|██████▍   | 2779/4328 [40:33<22:32,  1.15it/s] 64%|██████▍   | 2780/4328 [40:34<22:31,  1.15it/s] 64%|██████▍   | 2781/4328 [40:35<22:30,  1.15it/s] 64%|██████▍   | 2782/4328 [40:35<22:36,  1.14it/s] 64%|██████▍   | 2783/4328 [40:36<22:33,  1.14it/s] 64%|██████▍   | 2784/4328 [40:37<22:31,  1.14it/s] 64%|██████▍   | 2785/4328 [40:38<22:31,  1.14it/s] 64%|██████▍   | 2786/4328 [40:39<22:28,  1.14it/s] 64%|██████▍   | 2787/4328 [40:40<22:25,  1.15it/s] 64%|██████▍   | 2788/4328 [40:41<22:25,  1.14it/s] 64%|██████▍   | 2789/4328 [40:42<22:24,  1.14it/s] 64%|██████▍   | 2790/4328 [40:42<22:29,  1.14it/s] 64%|██████▍   | 2791/4328 [40:43<22:26,  1.14it/s] 65%|██████▍   | 2792/4328 [40:44<22:23,  1.14it/s] 65%|██████▍   | 2793/4328 [40:45<22:23,  1.14it/s] 65%|██████▍   | 2794/4328 [40:46<22:21,  1.14it/s] 65%|██████▍   | 2795/4328 [40:47<22:20,  1.14it/s] 65%|██████▍   | 2796/4328 [40:48<22:25,  1.14it/s] 65%|██████▍   | 2797/4328 [40:49<22:22,  1.14it/s] 65%|██████▍   | 2798/4328 [40:49<22:19,  1.14it/s] 65%|██████▍   | 2799/4328 [40:50<22:20,  1.14it/s] 65%|██████▍   | 2800/4328 [40:51<22:17,  1.14it/s] 65%|██████▍   | 2801/4328 [40:52<22:14,  1.14it/s] 65%|██████▍   | 2802/4328 [40:53<22:12,  1.15it/s] 65%|██████▍   | 2803/4328 [40:54<22:09,  1.15it/s] 65%|██████▍   | 2804/4328 [40:55<22:21,  1.14it/s] 65%|██████▍   | 2805/4328 [40:56<22:17,  1.14it/s] 65%|██████▍   | 2806/4328 [40:56<22:14,  1.14it/s] 65%|██████▍   | 2807/4328 [40:57<22:10,  1.14it/s] 65%|██████▍   | 2808/4328 [40:58<22:07,  1.14it/s] 65%|██████▍   | 2809/4328 [40:59<22:12,  1.14it/s] 65%|██████▍   | 2810/4328 [41:00<22:16,  1.14it/s] 65%|██████▍   | 2811/4328 [41:01<22:11,  1.14it/s] 65%|██████▍   | 2812/4328 [41:02<22:08,  1.14it/s] 65%|██████▍   | 2813/4328 [41:03<22:05,  1.14it/s] 65%|██████▌   | 2814/4328 [41:03<22:02,  1.15it/s] 65%|██████▌   | 2815/4328 [41:04<22:12,  1.14it/s] 65%|██████▌   | 2816/4328 [41:05<22:08,  1.14it/s] 65%|██████▌   | 2817/4328 [41:06<22:05,  1.14it/s] 65%|██████▌   | 2818/4328 [41:07<22:01,  1.14it/s] 65%|██████▌   | 2819/4328 [41:08<21:58,  1.14it/s] 65%|██████▌   | 2820/4328 [41:09<22:02,  1.14it/s] 65%|██████▌   | 2821/4328 [41:10<22:05,  1.14it/s] 65%|██████▌   | 2822/4328 [41:10<22:01,  1.14it/s] 65%|██████▌   | 2823/4328 [41:11<21:58,  1.14it/s] 65%|██████▌   | 2824/4328 [41:12<21:54,  1.14it/s] 65%|██████▌   | 2825/4328 [41:13<21:52,  1.14it/s] 65%|██████▌   | 2826/4328 [41:14<22:02,  1.14it/s] 65%|██████▌   | 2827/4328 [41:15<21:59,  1.14it/s] 65%|██████▌   | 2828/4328 [41:16<21:55,  1.14it/s] 65%|██████▌   | 2829/4328 [41:17<21:54,  1.14it/s] 65%|██████▌   | 2830/4328 [41:17<21:52,  1.14it/s] 65%|██████▌   | 2831/4328 [41:18<21:49,  1.14it/s] 65%|██████▌   | 2832/4328 [41:19<21:54,  1.14it/s] 65%|██████▌   | 2833/4328 [41:20<21:50,  1.14it/s] 65%|██████▌   | 2834/4328 [41:21<21:48,  1.14it/s] 66%|██████▌   | 2835/4328 [41:22<21:48,  1.14it/s] 66%|██████▌   | 2836/4328 [41:23<21:46,  1.14it/s] 66%|██████▌   | 2837/4328 [41:24<21:44,  1.14it/s] 66%|██████▌   | 2838/4328 [41:24<21:41,  1.15it/s] 66%|██████▌   | 2839/4328 [41:25<21:39,  1.15it/s] 66%|██████▌   | 2840/4328 [41:26<21:49,  1.14it/s] 66%|██████▌   | 2841/4328 [41:27<21:46,  1.14it/s] 66%|██████▌   | 2842/4328 [41:28<21:44,  1.14it/s] 66%|██████▌   | 2843/4328 [41:29<21:41,  1.14it/s] 66%|██████▌   | 2844/4328 [41:30<21:37,  1.14it/s] 66%|██████▌   | 2845/4328 [41:31<21:42,  1.14it/s] 66%|██████▌   | 2846/4328 [41:32<21:42,  1.14it/s] 66%|██████▌   | 2847/4328 [41:32<21:38,  1.14it/s] 66%|██████▌   | 2848/4328 [41:33<21:36,  1.14it/s] 66%|██████▌   | 2849/4328 [41:34<21:32,  1.14it/s] 66%|██████▌   | 2850/4328 [41:35<21:30,  1.15it/s] 66%|██████▌   | 2851/4328 [41:36<21:39,  1.14it/s] 66%|██████▌   | 2852/4328 [41:37<21:35,  1.14it/s] 66%|██████▌   | 2853/4328 [41:38<21:32,  1.14it/s] 66%|██████▌   | 2854/4328 [41:39<21:29,  1.14it/s] 66%|██████▌   | 2855/4328 [41:39<21:26,  1.14it/s] 66%|██████▌   | 2856/4328 [41:40<21:30,  1.14it/s] 66%|██████▌   | 2857/4328 [41:41<21:34,  1.14it/s] 66%|██████▌   | 2858/4328 [41:42<21:30,  1.14it/s] 66%|██████▌   | 2859/4328 [41:43<21:27,  1.14it/s] 66%|██████▌   | 2860/4328 [41:44<21:23,  1.14it/s] 66%|██████▌   | 2861/4328 [41:45<21:20,  1.15it/s] 66%|██████▌   | 2862/4328 [41:46<21:24,  1.14it/s] 66%|██████▌   | 2863/4328 [41:46<21:24,  1.14it/s] 66%|██████▌   | 2864/4328 [41:47<21:21,  1.14it/s] 66%|██████▌   | 2865/4328 [41:48<21:22,  1.14it/s] 66%|██████▌   | 2866/4328 [41:49<21:20,  1.14it/s] 66%|██████▌   | 2867/4328 [41:50<21:17,  1.14it/s] 66%|██████▋   | 2868/4328 [41:51<21:14,  1.15it/s] 66%|██████▋   | 2869/4328 [41:52<21:13,  1.15it/s] 66%|██████▋   | 2870/4328 [41:53<21:17,  1.14it/s] 66%|██████▋   | 2871/4328 [41:53<21:20,  1.14it/s] 66%|██████▋   | 2872/4328 [41:54<21:17,  1.14it/s] 66%|██████▋   | 2873/4328 [41:55<21:13,  1.14it/s] 66%|██████▋   | 2874/4328 [41:56<21:10,  1.14it/s] 66%|██████▋   | 2875/4328 [41:57<21:07,  1.15it/s] 66%|██████▋   | 2876/4328 [41:58<21:17,  1.14it/s] 66%|██████▋   | 2877/4328 [41:59<21:14,  1.14it/s] 66%|██████▋   | 2878/4328 [42:00<21:10,  1.14it/s] 67%|██████▋   | 2879/4328 [42:00<21:10,  1.14it/s] 67%|██████▋   | 2880/4328 [42:01<21:07,  1.14it/s] 67%|██████▋   | 2881/4328 [42:02<21:04,  1.14it/s] 67%|██████▋   | 2882/4328 [42:03<21:04,  1.14it/s] 67%|██████▋   | 2883/4328 [42:04<21:02,  1.14it/s] 67%|██████▋   | 2884/4328 [42:05<21:01,  1.14it/s] 67%|██████▋   | 2885/4328 [42:06<21:07,  1.14it/s] 67%|██████▋   | 2886/4328 [42:07<21:03,  1.14it/s] 67%|██████▋   | 2887/4328 [42:07<21:01,  1.14it/s] 67%|██████▋   | 2888/4328 [42:08<20:58,  1.14it/s] 67%|██████▋   | 2889/4328 [42:09<20:56,  1.15it/s] 67%|██████▋   | 2890/4328 [42:10<21:06,  1.14it/s] 67%|██████▋   | 2891/4328 [42:11<21:03,  1.14it/s] 67%|██████▋   | 2892/4328 [42:12<20:59,  1.14it/s] 67%|██████▋   | 2893/4328 [42:13<20:55,  1.14it/s] 67%|██████▋   | 2894/4328 [42:14<20:52,  1.14it/s] 67%|██████▋   | 2895/4328 [42:14<20:56,  1.14it/s] 67%|██████▋   | 2896/4328 [42:15<21:00,  1.14it/s] 67%|██████▋   | 2897/4328 [42:16<20:56,  1.14it/s] 67%|██████▋   | 2898/4328 [42:17<20:52,  1.14it/s] 67%|██████▋   | 2899/4328 [42:18<20:52,  1.14it/s] 67%|██████▋   | 2900/4328 [42:19<20:49,  1.14it/s] 67%|██████▋   | 2901/4328 [42:20<20:52,  1.14it/s] 67%|██████▋   | 2902/4328 [42:21<20:50,  1.14it/s] 67%|██████▋   | 2903/4328 [42:21<20:47,  1.14it/s] 67%|██████▋   | 2904/4328 [42:22<20:46,  1.14it/s] 67%|██████▋   | 2905/4328 [42:23<20:43,  1.14it/s] 67%|██████▋   | 2906/4328 [42:24<20:41,  1.15it/s] 67%|██████▋   | 2907/4328 [42:25<20:41,  1.14it/s] 67%|██████▋   | 2908/4328 [42:26<20:40,  1.14it/s] 67%|██████▋   | 2909/4328 [42:27<20:39,  1.14it/s] 67%|██████▋   | 2910/4328 [42:28<20:45,  1.14it/s] 67%|██████▋   | 2911/4328 [42:28<20:41,  1.14it/s] 67%|██████▋   | 2912/4328 [42:29<20:38,  1.14it/s] 67%|██████▋   | 2913/4328 [42:30<20:38,  1.14it/s] 67%|██████▋   | 2914/4328 [42:31<20:35,  1.14it/s] 67%|██████▋   | 2915/4328 [42:32<20:33,  1.15it/s] 67%|██████▋   | 2916/4328 [42:33<20:33,  1.14it/s] 67%|██████▋   | 2917/4328 [42:34<20:32,  1.14it/s] 67%|██████▋   | 2918/4328 [42:35<20:36,  1.14it/s] 67%|██████▋   | 2919/4328 [42:35<20:35,  1.14it/s] 67%|██████▋   | 2920/4328 [42:36<20:33,  1.14it/s] 67%|██████▋   | 2921/4328 [42:37<20:33,  1.14it/s] 68%|██████▊   | 2922/4328 [42:38<20:30,  1.14it/s] 68%|██████▊   | 2923/4328 [42:39<20:27,  1.14it/s] 68%|██████▊   | 2924/4328 [42:40<20:26,  1.14it/s] 68%|██████▊   | 2925/4328 [42:41<20:25,  1.14it/s] 68%|██████▊   | 2926/4328 [42:42<20:24,  1.15it/s] 68%|██████▊   | 2927/4328 [42:42<20:29,  1.14it/s] 68%|██████▊   | 2928/4328 [42:43<20:26,  1.14it/s] 68%|██████▊   | 2929/4328 [42:44<20:24,  1.14it/s] 68%|██████▊   | 2930/4328 [42:45<20:24,  1.14it/s] 68%|██████▊   | 2931/4328 [42:46<20:21,  1.14it/s] 68%|██████▊   | 2932/4328 [42:47<20:18,  1.15it/s] 68%|██████▊   | 2933/4328 [42:48<20:19,  1.14it/s] 68%|██████▊   | 2934/4328 [42:49<20:17,  1.14it/s] 68%|██████▊   | 2935/4328 [42:49<20:22,  1.14it/s] 68%|██████▊   | 2936/4328 [42:50<20:20,  1.14it/s] 68%|██████▊   | 2937/4328 [42:51<20:17,  1.14it/s] 68%|██████▊   | 2938/4328 [42:52<20:15,  1.14it/s] 68%|██████▊   | 2939/4328 [42:53<20:12,  1.15it/s] 68%|██████▊   | 2940/4328 [42:54<20:16,  1.14it/s] 68%|██████▊   | 2941/4328 [42:55<20:20,  1.14it/s] 68%|██████▊   | 2942/4328 [42:56<20:16,  1.14it/s] 68%|██████▊   | 2943/4328 [42:56<20:13,  1.14it/s] 68%|██████▊   | 2944/4328 [42:57<20:10,  1.14it/s] 68%|██████▊   | 2945/4328 [42:58<20:07,  1.15it/s] 68%|██████▊   | 2946/4328 [42:59<20:16,  1.14it/s] 68%|██████▊   | 2947/4328 [43:00<20:13,  1.14it/s] 68%|██████▊   | 2948/4328 [43:01<20:10,  1.14it/s] 68%|██████▊   | 2949/4328 [43:02<20:09,  1.14it/s] 68%|██████▊   | 2950/4328 [43:03<20:07,  1.14it/s] 68%|██████▊   | 2951/4328 [43:03<20:05,  1.14it/s] 68%|██████▊   | 2952/4328 [43:04<20:09,  1.14it/s] 68%|██████▊   | 2953/4328 [43:05<20:05,  1.14it/s] 68%|██████▊   | 2954/4328 [43:06<20:03,  1.14it/s] 68%|██████▊   | 2955/4328 [43:07<20:00,  1.14it/s] 68%|██████▊   | 2956/4328 [43:08<19:57,  1.15it/s] 68%|██████▊   | 2957/4328 [43:09<20:06,  1.14it/s] 68%|██████▊   | 2958/4328 [43:10<20:03,  1.14it/s] 68%|██████▊   | 2959/4328 [43:10<20:00,  1.14it/s] 68%|██████▊   | 2960/4328 [43:11<19:59,  1.14it/s] 68%|██████▊   | 2961/4328 [43:12<19:57,  1.14it/s] 68%|██████▊   | 2962/4328 [43:13<19:54,  1.14it/s] 68%|██████▊   | 2963/4328 [43:14<19:54,  1.14it/s] 68%|██████▊   | 2964/4328 [43:15<19:52,  1.14it/s] 69%|██████▊   | 2965/4328 [43:16<19:51,  1.14it/s] 69%|██████▊   | 2966/4328 [43:17<19:54,  1.14it/s] 69%|██████▊   | 2967/4328 [43:17<19:50,  1.14it/s] 69%|██████▊   | 2968/4328 [43:18<19:48,  1.14it/s] 69%|██████▊   | 2969/4328 [43:19<19:49,  1.14it/s] 69%|██████▊   | 2970/4328 [43:20<19:46,  1.14it/s] 69%|██████▊   | 2971/4328 [43:21<19:44,  1.15it/s] 69%|██████▊   | 2972/4328 [43:22<19:45,  1.14it/s] 69%|██████▊   | 2973/4328 [43:23<19:43,  1.14it/s] 69%|██████▊   | 2974/4328 [43:24<19:47,  1.14it/s] 69%|██████▊   | 2975/4328 [43:24<19:45,  1.14it/s] 69%|██████▉   | 2976/4328 [43:25<19:43,  1.14it/s] 69%|██████▉   | 2977/4328 [43:26<19:43,  1.14it/s] 69%|██████▉   | 2978/4328 [43:27<19:41,  1.14it/s] 69%|██████▉   | 2979/4328 [43:28<19:38,  1.14it/s] 69%|██████▉   | 2980/4328 [43:29<19:36,  1.15it/s] 69%|██████▉   | 2981/4328 [43:30<19:34,  1.15it/s] 69%|██████▉   | 2982/4328 [43:31<19:38,  1.14it/s] 69%|██████▉   | 2983/4328 [43:31<19:42,  1.14it/s] 69%|██████▉   | 2984/4328 [43:32<19:39,  1.14it/s] 69%|██████▉   | 2985/4328 [43:33<19:36,  1.14it/s] 69%|██████▉   | 2986/4328 [43:34<19:33,  1.14it/s] 69%|██████▉   | 2987/4328 [43:35<19:31,  1.15it/s] 69%|██████▉   | 2988/4328 [43:36<19:40,  1.14it/s] 69%|██████▉   | 2989/4328 [43:37<19:36,  1.14it/s] 69%|██████▉   | 2990/4328 [43:38<19:32,  1.14it/s] 69%|██████▉   | 2991/4328 [43:38<19:31,  1.14it/s] 69%|██████▉   | 2992/4328 [43:39<19:29,  1.14it/s] 69%|██████▉   | 2993/4328 [43:40<19:27,  1.14it/s] 69%|██████▉   | 2994/4328 [43:41<19:31,  1.14it/s] 69%|██████▉   | 2995/4328 [43:42<19:27,  1.14it/s] 69%|██████▉   | 2996/4328 [43:43<19:24,  1.14it/s] 69%|██████▉   | 2997/4328 [43:44<19:24,  1.14it/s] 69%|██████▉   | 2998/4328 [43:45<19:22,  1.14it/s] 69%|██████▉   | 2999/4328 [43:45<19:19,  1.15it/s] 69%|██████▉   | 3000/4328 [43:46<19:20,  1.14it/s] 69%|██████▉   | 3001/4328 [43:47<19:19,  1.14it/s] 69%|██████▉   | 3002/4328 [43:48<19:22,  1.14it/s] 69%|██████▉   | 3003/4328 [43:49<19:20,  1.14it/s] 69%|██████▉   | 3004/4328 [43:50<19:17,  1.14it/s] 69%|██████▉   | 3005/4328 [43:51<19:17,  1.14it/s] 69%|██████▉   | 3006/4328 [43:52<19:15,  1.14it/s] 69%|██████▉   | 3007/4328 [43:52<19:13,  1.15it/s] 70%|██████▉   | 3008/4328 [43:53<19:13,  1.14it/s] 70%|██████▉   | 3009/4328 [43:54<19:11,  1.15it/s] 70%|██████▉   | 3010/4328 [43:55<19:10,  1.15it/s] 70%|██████▉   | 3011/4328 [43:56<19:16,  1.14it/s] 70%|██████▉   | 3012/4328 [43:57<19:13,  1.14it/s] 70%|██████▉   | 3013/4328 [43:58<19:09,  1.14it/s] 70%|██████▉   | 3014/4328 [43:59<19:10,  1.14it/s] 70%|██████▉   | 3015/4328 [43:59<19:07,  1.14it/s] 70%|██████▉   | 3016/4328 [44:00<19:05,  1.15it/s] 70%|██████▉   | 3017/4328 [44:01<19:06,  1.14it/s] 70%|██████▉   | 3018/4328 [44:02<19:05,  1.14it/s] 70%|██████▉   | 3019/4328 [44:03<19:09,  1.14it/s] 70%|██████▉   | 3020/4328 [44:04<19:07,  1.14it/s] 70%|██████▉   | 3021/4328 [44:05<19:04,  1.14it/s] 70%|██████▉   | 3022/4328 [44:06<19:03,  1.14it/s] 70%|██████▉   | 3023/4328 [44:06<19:00,  1.14it/s] 70%|██████▉   | 3024/4328 [44:07<18:58,  1.15it/s] 70%|██████▉   | 3025/4328 [44:08<18:58,  1.14it/s] 70%|██████▉   | 3026/4328 [44:09<18:57,  1.15it/s] 70%|██████▉   | 3027/4328 [44:10<18:56,  1.15it/s] 70%|██████▉   | 3028/4328 [44:11<19:01,  1.14it/s] 70%|██████▉   | 3029/4328 [44:12<18:58,  1.14it/s] 70%|███████   | 3030/4328 [44:13<18:55,  1.14it/s] 70%|███████   | 3031/4328 [44:13<18:53,  1.14it/s] 70%|███████   | 3032/4328 [44:14<18:50,  1.15it/s] 70%|███████   | 3033/4328 [44:15<18:59,  1.14it/s] 70%|███████   | 3034/4328 [44:16<18:56,  1.14it/s] 70%|███████   | 3035/4328 [44:17<18:52,  1.14it/s] 70%|███████   | 3036/4328 [44:18<18:51,  1.14it/s] 70%|███████   | 3037/4328 [44:19<18:49,  1.14it/s] 70%|███████   | 3038/4328 [44:20<18:46,  1.14it/s] 70%|███████   | 3039/4328 [44:20<18:46,  1.14it/s] 70%|███████   | 3040/4328 [44:21<18:45,  1.14it/s] 70%|███████   | 3041/4328 [44:22<18:44,  1.14it/s] 70%|███████   | 3042/4328 [44:23<18:49,  1.14it/s] 70%|███████   | 3043/4328 [44:24<18:46,  1.14it/s] 70%|███████   | 3044/4328 [44:25<18:44,  1.14it/s] 70%|███████   | 3045/4328 [44:26<18:41,  1.14it/s] 70%|███████   | 3046/4328 [44:27<18:38,  1.15it/s] 70%|███████   | 3047/4328 [44:27<18:41,  1.14it/s] 70%|███████   | 3048/4328 [44:28<18:40,  1.14it/s] 70%|███████   | 3049/4328 [44:29<18:38,  1.14it/s] 70%|███████   | 3050/4328 [44:30<18:40,  1.14it/s] 70%|███████   | 3051/4328 [44:31<18:38,  1.14it/s] 71%|███████   | 3052/4328 [44:32<18:36,  1.14it/s] 71%|███████   | 3053/4328 [44:33<18:33,  1.14it/s] 71%|███████   | 3054/4328 [44:34<18:31,  1.15it/s] 71%|███████   | 3055/4328 [44:34<18:34,  1.14it/s] 71%|███████   | 3056/4328 [44:35<18:38,  1.14it/s] 71%|███████   | 3057/4328 [44:36<18:34,  1.14it/s] 71%|███████   | 3058/4328 [44:37<18:31,  1.14it/s] 71%|███████   | 3059/4328 [44:38<18:31,  1.14it/s] 71%|███████   | 3060/4328 [44:39<18:28,  1.14it/s] 71%|███████   | 3061/4328 [44:40<18:25,  1.15it/s] 71%|███████   | 3062/4328 [44:41<18:26,  1.14it/s] 71%|███████   | 3063/4328 [44:41<18:24,  1.15it/s] 71%|███████   | 3064/4328 [44:42<18:28,  1.14it/s] 71%|███████   | 3065/4328 [44:43<18:27,  1.14it/s] 71%|███████   | 3066/4328 [44:44<18:24,  1.14it/s] 71%|███████   | 3067/4328 [44:45<18:23,  1.14it/s] 71%|███████   | 3068/4328 [44:46<18:22,  1.14it/s] 71%|███████   | 3069/4328 [44:47<18:19,  1.14it/s] 71%|███████   | 3070/4328 [44:48<18:17,  1.15it/s] 71%|███████   | 3071/4328 [44:48<18:15,  1.15it/s] 71%|███████   | 3072/4328 [44:49<18:19,  1.14it/s] 71%|███████   | 3073/4328 [44:50<18:22,  1.14it/s] 71%|███████   | 3074/4328 [44:51<18:19,  1.14it/s] 71%|███████   | 3075/4328 [44:52<18:16,  1.14it/s] 71%|███████   | 3076/4328 [44:53<18:16,  1.14it/s] 71%|███████   | 3077/4328 [44:54<18:13,  1.14it/s] 71%|███████   | 3078/4328 [44:55<18:10,  1.15it/s] 71%|███████   | 3079/4328 [44:55<18:11,  1.14it/s] 71%|███████   | 3080/4328 [44:56<18:10,  1.14it/s] 71%|███████   | 3081/4328 [44:57<18:14,  1.14it/s] 71%|███████   | 3082/4328 [44:58<18:12,  1.14it/s] 71%|███████   | 3083/4328 [44:59<18:09,  1.14it/s] 71%|███████▏  | 3084/4328 [45:00<18:07,  1.14it/s] 71%|███████▏  | 3085/4328 [45:01<18:05,  1.15it/s] 71%|███████▏  | 3086/4328 [45:02<18:08,  1.14it/s] 71%|███████▏  | 3087/4328 [45:02<18:11,  1.14it/s] 71%|███████▏  | 3088/4328 [45:03<18:08,  1.14it/s] 71%|███████▏  | 3089/4328 [45:04<18:05,  1.14it/s] 71%|███████▏  | 3090/4328 [45:05<18:02,  1.14it/s] 71%|███████▏  | 3091/4328 [45:06<18:00,  1.14it/s] 71%|███████▏  | 3092/4328 [45:07<18:09,  1.13it/s] 71%|███████▏  | 3093/4328 [45:08<18:06,  1.14it/s] 71%|███████▏  | 3094/4328 [45:09<18:02,  1.14it/s] 72%|███████▏  | 3095/4328 [45:09<17:59,  1.14it/s] 72%|███████▏  | 3096/4328 [45:10<17:56,  1.14it/s] 72%|███████▏  | 3097/4328 [45:11<17:59,  1.14it/s] 72%|███████▏  | 3098/4328 [45:12<18:02,  1.14it/s] 72%|███████▏  | 3099/4328 [45:13<17:58,  1.14it/s] 72%|███████▏  | 3100/4328 [45:14<17:56,  1.14it/s] 72%|███████▏  | 3101/4328 [45:15<17:52,  1.14it/s] 72%|███████▏  | 3102/4328 [45:16<17:50,  1.15it/s] 72%|███████▏  | 3103/4328 [45:17<17:58,  1.14it/s] 72%|███████▏  | 3104/4328 [45:17<17:55,  1.14it/s] 72%|███████▏  | 3105/4328 [45:18<17:51,  1.14it/s] 72%|███████▏  | 3106/4328 [45:19<17:48,  1.14it/s] 72%|███████▏  | 3107/4328 [45:20<17:46,  1.14it/s] 72%|███████▏  | 3108/4328 [45:21<17:49,  1.14it/s] 72%|███████▏  | 3109/4328 [45:22<17:52,  1.14it/s] 72%|███████▏  | 3110/4328 [45:23<17:48,  1.14it/s] 72%|███████▏  | 3111/4328 [45:24<17:46,  1.14it/s] 72%|███████▏  | 3112/4328 [45:24<17:42,  1.14it/s] 72%|███████▏  | 3113/4328 [45:25<17:39,  1.15it/s] 72%|███████▏  | 3114/4328 [45:26<17:48,  1.14it/s] 72%|███████▏  | 3115/4328 [45:27<17:45,  1.14it/s] 72%|███████▏  | 3116/4328 [45:28<17:42,  1.14it/s] 72%|███████▏  | 3117/4328 [45:29<17:39,  1.14it/s] 72%|███████▏  | 3118/4328 [45:30<17:37,  1.14it/s] 72%|███████▏  | 3119/4328 [45:31<17:40,  1.14it/s] 72%|███████▏  | 3120/4328 [45:31<17:42,  1.14it/s] 72%|███████▏  | 3121/4328 [45:32<17:39,  1.14it/s] 72%|███████▏  | 3122/4328 [45:33<17:36,  1.14it/s] 72%|███████▏  | 3123/4328 [45:34<17:36,  1.14it/s] 72%|███████▏  | 3124/4328 [45:35<17:33,  1.14it/s] 72%|███████▏  | 3125/4328 [45:36<17:30,  1.14it/s] 72%|███████▏  | 3126/4328 [45:37<17:30,  1.14it/s] 72%|███████▏  | 3127/4328 [45:38<17:29,  1.14it/s] 72%|███████▏  | 3128/4328 [45:38<17:32,  1.14it/s] 72%|███████▏  | 3129/4328 [45:39<17:31,  1.14it/s] 72%|███████▏  | 3130/4328 [45:40<17:28,  1.14it/s] 72%|███████▏  | 3131/4328 [45:41<17:28,  1.14it/s] 72%|███████▏  | 3132/4328 [45:42<17:26,  1.14it/s] 72%|███████▏  | 3133/4328 [45:43<17:23,  1.15it/s] 72%|███████▏  | 3134/4328 [45:44<17:22,  1.14it/s] 72%|███████▏  | 3135/4328 [45:45<17:21,  1.15it/s] 72%|███████▏  | 3136/4328 [45:45<17:20,  1.15it/s] 72%|███████▏  | 3137/4328 [45:46<17:25,  1.14it/s] 73%|███████▎  | 3138/4328 [45:47<17:22,  1.14it/s] 73%|███████▎  | 3139/4328 [45:48<17:20,  1.14it/s] 73%|███████▎  | 3140/4328 [45:49<17:20,  1.14it/s] 73%|███████▎  | 3141/4328 [45:50<17:18,  1.14it/s] 73%|███████▎  | 3142/4328 [45:51<17:15,  1.14it/s] 73%|███████▎  | 3143/4328 [45:52<17:16,  1.14it/s] 73%|███████▎  | 3144/4328 [45:52<17:15,  1.14it/s] 73%|███████▎  | 3145/4328 [45:53<17:17,  1.14it/s] 73%|███████▎  | 3146/4328 [45:54<17:15,  1.14it/s] 73%|███████▎  | 3147/4328 [45:55<17:12,  1.14it/s] 73%|███████▎  | 3148/4328 [45:56<17:13,  1.14it/s] 73%|███████▎  | 3149/4328 [45:57<17:11,  1.14it/s] 73%|███████▎  | 3150/4328 [45:58<17:09,  1.14it/s] 73%|███████▎  | 3151/4328 [45:59<17:12,  1.14it/s] 73%|███████▎  | 3152/4328 [45:59<17:10,  1.14it/s] 73%|███████▎  | 3153/4328 [46:00<17:08,  1.14it/s] 73%|███████▎  | 3154/4328 [46:01<17:08,  1.14it/s] 73%|███████▎  | 3155/4328 [46:02<17:06,  1.14it/s] 73%|███████▎  | 3156/4328 [46:03<17:03,  1.14it/s] 73%|███████▎  | 3157/4328 [46:04<17:01,  1.15it/s] 73%|███████▎  | 3158/4328 [46:05<16:59,  1.15it/s] 73%|███████▎  | 3159/4328 [46:06<17:08,  1.14it/s] 73%|███████▎  | 3160/4328 [46:06<17:05,  1.14it/s] 73%|███████▎  | 3161/4328 [46:07<17:02,  1.14it/s] 73%|███████▎  | 3162/4328 [46:08<17:00,  1.14it/s] 73%|███████▎  | 3163/4328 [46:09<16:57,  1.15it/s] 73%|███████▎  | 3164/4328 [46:10<17:00,  1.14it/s] 73%|███████▎  | 3165/4328 [46:11<17:03,  1.14it/s] 73%|███████▎  | 3166/4328 [46:12<16:59,  1.14it/s] 73%|███████▎  | 3167/4328 [46:13<16:56,  1.14it/s] 73%|███████▎  | 3168/4328 [46:13<16:53,  1.14it/s] 73%|███████▎  | 3169/4328 [46:14<16:51,  1.15it/s] 73%|███████▎  | 3170/4328 [46:15<16:59,  1.14it/s] 73%|███████▎  | 3171/4328 [46:16<16:56,  1.14it/s] 73%|███████▎  | 3172/4328 [46:17<16:53,  1.14it/s] 73%|███████▎  | 3173/4328 [46:18<16:52,  1.14it/s] 73%|███████▎  | 3174/4328 [46:19<16:50,  1.14it/s] 73%|███████▎  | 3175/4328 [46:20<16:47,  1.14it/s] 73%|███████▎  | 3176/4328 [46:20<16:46,  1.14it/s] 73%|███████▎  | 3177/4328 [46:21<16:45,  1.14it/s] 73%|███████▎  | 3178/4328 [46:22<16:44,  1.14it/s] 73%|███████▎  | 3179/4328 [46:23<16:48,  1.14it/s] 73%|███████▎  | 3180/4328 [46:24<16:46,  1.14it/s] 73%|███████▎  | 3181/4328 [46:25<16:44,  1.14it/s] 74%|███████▎  | 3182/4328 [46:26<16:41,  1.14it/s] 74%|███████▎  | 3183/4328 [46:27<16:39,  1.15it/s] 74%|███████▎  | 3184/4328 [46:27<16:46,  1.14it/s] 74%|███████▎  | 3185/4328 [46:28<16:43,  1.14it/s] 74%|███████▎  | 3186/4328 [46:29<16:40,  1.14it/s] 74%|███████▎  | 3187/4328 [46:30<16:40,  1.14it/s] 74%|███████▎  | 3188/4328 [46:31<16:38,  1.14it/s] 74%|███████▎  | 3189/4328 [46:32<16:36,  1.14it/s] 74%|███████▎  | 3190/4328 [46:33<16:34,  1.14it/s] 74%|███████▎  | 3191/4328 [46:34<16:32,  1.15it/s] 74%|███████▍  | 3192/4328 [46:34<16:35,  1.14it/s] 74%|███████▍  | 3193/4328 [46:35<16:38,  1.14it/s] 74%|███████▍  | 3194/4328 [46:36<16:35,  1.14it/s] 74%|███████▍  | 3195/4328 [46:37<16:32,  1.14it/s] 74%|███████▍  | 3196/4328 [46:38<16:29,  1.14it/s] 74%|███████▍  | 3197/4328 [46:39<16:27,  1.15it/s] 74%|███████▍  | 3198/4328 [46:40<16:35,  1.14it/s] 74%|███████▍  | 3199/4328 [46:41<16:32,  1.14it/s] 74%|███████▍  | 3200/4328 [46:41<16:30,  1.14it/s] 74%|███████▍  | 3201/4328 [46:42<16:27,  1.14it/s] 74%|███████▍  | 3202/4328 [46:43<16:25,  1.14it/s] 74%|███████▍  | 3203/4328 [46:44<16:27,  1.14it/s] 74%|███████▍  | 3204/4328 [46:45<16:27,  1.14it/s] 74%|███████▍  | 3205/4328 [46:46<16:25,  1.14it/s] 74%|███████▍  | 3206/4328 [46:47<16:27,  1.14it/s] 74%|███████▍  | 3207/4328 [46:48<16:24,  1.14it/s] 74%|███████▍  | 3208/4328 [46:48<16:21,  1.14it/s] 74%|███████▍  | 3209/4328 [46:49<16:20,  1.14it/s] 74%|███████▍  | 3210/4328 [46:50<16:18,  1.14it/s] 74%|███████▍  | 3211/4328 [46:51<16:16,  1.14it/s] 74%|███████▍  | 3212/4328 [46:52<16:14,  1.15it/s] 74%|███████▍  | 3213/4328 [46:53<16:12,  1.15it/s] 74%|███████▍  | 3214/4328 [46:54<16:15,  1.14it/s] 74%|███████▍  | 3215/4328 [46:55<16:18,  1.14it/s] 74%|███████▍  | 3216/4328 [46:55<16:14,  1.14it/s] 74%|███████▍  | 3217/4328 [46:56<16:11,  1.14it/s] 74%|███████▍  | 3218/4328 [46:57<16:11,  1.14it/s] 74%|███████▍  | 3219/4328 [46:58<16:08,  1.14it/s] 74%|███████▍  | 3220/4328 [46:59<16:06,  1.15it/s] 74%|███████▍  | 3221/4328 [47:00<16:07,  1.14it/s] 74%|███████▍  | 3222/4328 [47:01<16:05,  1.14it/s] 74%|███████▍  | 3223/4328 [47:02<16:09,  1.14it/s] 74%|███████▍  | 3224/4328 [47:02<16:07,  1.14it/s] 75%|███████▍  | 3225/4328 [47:03<16:05,  1.14it/s] 75%|███████▍  | 3226/4328 [47:04<16:05,  1.14it/s] 75%|███████▍  | 3227/4328 [47:05<16:03,  1.14it/s] 75%|███████▍  | 3228/4328 [47:06<16:00,  1.14it/s] 75%|███████▍  | 3229/4328 [47:07<16:00,  1.14it/s] 75%|███████▍  | 3230/4328 [47:08<15:59,  1.14it/s] 75%|███████▍  | 3231/4328 [47:09<15:57,  1.15it/s] 75%|███████▍  | 3232/4328 [47:09<16:01,  1.14it/s] 75%|███████▍  | 3233/4328 [47:10<15:58,  1.14it/s] 75%|███████▍  | 3234/4328 [47:11<15:56,  1.14it/s] 75%|███████▍  | 3235/4328 [47:12<15:57,  1.14it/s] 75%|███████▍  | 3236/4328 [47:13<15:54,  1.14it/s] 75%|███████▍  | 3237/4328 [47:14<15:52,  1.15it/s] 75%|███████▍  | 3238/4328 [47:15<15:52,  1.14it/s] 75%|███████▍  | 3239/4328 [47:16<15:51,  1.14it/s] 75%|███████▍  | 3240/4328 [47:16<15:54,  1.14it/s] 75%|███████▍  | 3241/4328 [47:17<15:52,  1.14it/s] 75%|███████▍  | 3242/4328 [47:18<15:50,  1.14it/s] 75%|███████▍  | 3243/4328 [47:19<15:50,  1.14it/s] 75%|███████▍  | 3244/4328 [47:20<15:48,  1.14it/s] 75%|███████▍  | 3245/4328 [47:21<15:46,  1.14it/s] 75%|███████▌  | 3246/4328 [47:22<15:45,  1.14it/s] 75%|███████▌  | 3247/4328 [47:23<15:44,  1.14it/s] 75%|███████▌  | 3248/4328 [47:23<15:43,  1.15it/s] 75%|███████▌  | 3249/4328 [47:24<15:46,  1.14it/s] 75%|███████▌  | 3250/4328 [47:25<15:44,  1.14it/s] 75%|███████▌  | 3251/4328 [47:26<15:42,  1.14it/s] 75%|███████▌  | 3252/4328 [47:27<15:42,  1.14it/s] 75%|███████▌  | 3253/4328 [47:28<15:40,  1.14it/s] 75%|███████▌  | 3254/4328 [47:29<15:43,  1.14it/s] 75%|███████▌  | 3255/4328 [47:30<15:41,  1.14it/s] 75%|███████▌  | 3256/4328 [47:30<15:38,  1.14it/s] 75%|███████▌  | 3257/4328 [47:31<15:38,  1.14it/s] 75%|███████▌  | 3258/4328 [47:32<15:36,  1.14it/s] 75%|███████▌  | 3259/4328 [47:33<15:33,  1.15it/s] 75%|███████▌  | 3260/4328 [47:34<15:33,  1.14it/s] 75%|███████▌  | 3261/4328 [47:35<15:32,  1.14it/s] 75%|███████▌  | 3262/4328 [47:36<15:30,  1.15it/s] 75%|███████▌  | 3263/4328 [47:37<15:34,  1.14it/s] 75%|███████▌  | 3264/4328 [47:37<15:32,  1.14it/s] 75%|███████▌  | 3265/4328 [47:38<15:30,  1.14it/s] 75%|███████▌  | 3266/4328 [47:39<15:29,  1.14it/s] 75%|███████▌  | 3267/4328 [47:40<15:27,  1.14it/s] 76%|███████▌  | 3268/4328 [47:41<15:24,  1.15it/s] 76%|███████▌  | 3269/4328 [47:42<15:24,  1.14it/s] 76%|███████▌  | 3270/4328 [47:43<15:23,  1.15it/s] 76%|███████▌  | 3271/4328 [47:44<15:26,  1.14it/s] 76%|███████▌  | 3272/4328 [47:44<15:25,  1.14it/s] 76%|███████▌  | 3273/4328 [47:45<15:23,  1.14it/s] 76%|███████▌  | 3274/4328 [47:46<15:24,  1.14it/s] 76%|███████▌  | 3275/4328 [47:47<15:21,  1.14it/s] 76%|███████▌  | 3276/4328 [47:48<15:19,  1.14it/s] 76%|███████▌  | 3277/4328 [47:49<15:19,  1.14it/s] 76%|███████▌  | 3278/4328 [47:50<15:18,  1.14it/s] 76%|███████▌  | 3279/4328 [47:51<15:17,  1.14it/s] 76%|███████▌  | 3280/4328 [47:51<15:21,  1.14it/s] 76%|███████▌  | 3281/4328 [47:52<15:18,  1.14it/s] 76%|███████▌  | 3282/4328 [47:53<15:15,  1.14it/s] 76%|███████▌  | 3283/4328 [47:54<15:15,  1.14it/s] 76%|███████▌  | 3284/4328 [47:55<15:12,  1.14it/s] 76%|███████▌  | 3285/4328 [47:56<15:10,  1.15it/s] 76%|███████▌  | 3286/4328 [47:57<15:10,  1.14it/s] 76%|███████▌  | 3287/4328 [47:58<15:09,  1.14it/s] 76%|███████▌  | 3288/4328 [47:58<15:11,  1.14it/s] 76%|███████▌  | 3289/4328 [47:59<15:09,  1.14it/s] 76%|███████▌  | 3290/4328 [48:00<15:07,  1.14it/s] 76%|███████▌  | 3291/4328 [48:01<15:07,  1.14it/s] 76%|███████▌  | 3292/4328 [48:02<15:05,  1.14it/s] 76%|███████▌  | 3293/4328 [48:03<15:03,  1.15it/s] 76%|███████▌  | 3294/4328 [48:04<15:03,  1.14it/s] 76%|███████▌  | 3295/4328 [48:05<15:02,  1.14it/s] 76%|███████▌  | 3296/4328 [48:05<15:01,  1.14it/s] 76%|███████▌  | 3297/4328 [48:06<15:04,  1.14it/s] 76%|███████▌  | 3298/4328 [48:07<15:01,  1.14it/s] 76%|███████▌  | 3299/4328 [48:08<14:59,  1.14it/s] 76%|███████▌  | 3300/4328 [48:09<14:59,  1.14it/s] 76%|███████▋  | 3301/4328 [48:10<14:57,  1.14it/s] 76%|███████▋  | 3302/4328 [48:11<14:55,  1.15it/s] 76%|███████▋  | 3303/4328 [48:12<14:56,  1.14it/s] 76%|███████▋  | 3304/4328 [48:12<14:54,  1.14it/s] 76%|███████▋  | 3305/4328 [48:13<14:56,  1.14it/s] 76%|███████▋  | 3306/4328 [48:14<14:54,  1.14it/s] 76%|███████▋  | 3307/4328 [48:15<14:52,  1.14it/s] 76%|███████▋  | 3308/4328 [48:16<14:52,  1.14it/s] 76%|███████▋  | 3309/4328 [48:17<14:50,  1.14it/s] 76%|███████▋  | 3310/4328 [48:18<14:48,  1.15it/s] 77%|███████▋  | 3311/4328 [48:19<14:47,  1.15it/s] 77%|███████▋  | 3312/4328 [48:19<14:46,  1.15it/s] 77%|███████▋  | 3313/4328 [48:20<14:45,  1.15it/s] 77%|███████▋  | 3314/4328 [48:21<14:49,  1.14it/s] 77%|███████▋  | 3315/4328 [48:22<14:46,  1.14it/s] 77%|███████▋  | 3316/4328 [48:23<14:44,  1.14it/s] 77%|███████▋  | 3317/4328 [48:24<14:45,  1.14it/s] 77%|███████▋  | 3318/4328 [48:25<14:42,  1.14it/s] 77%|███████▋  | 3319/4328 [48:26<14:41,  1.15it/s] 77%|███████▋  | 3320/4328 [48:26<14:41,  1.14it/s] 77%|███████▋  | 3321/4328 [48:27<14:39,  1.15it/s] 77%|███████▋  | 3322/4328 [48:28<14:37,  1.15it/s] 77%|███████▋  | 3323/4328 [48:29<14:35,  1.15it/s] 77%|███████▋  | 3324/4328 [48:30<14:34,  1.15it/s] 77%|███████▋  | 3325/4328 [48:31<14:40,  1.14it/s] 77%|███████▋  | 3326/4328 [48:32<14:39,  1.14it/s] 77%|███████▋  | 3327/4328 [48:33<14:36,  1.14it/s] 77%|███████▋  | 3328/4328 [48:33<14:36,  1.14it/s] 77%|███████▋  | 3329/4328 [48:34<14:34,  1.14it/s] 77%|███████▋  | 3330/4328 [48:35<14:32,  1.14it/s] 77%|███████▋  | 3331/4328 [48:36<14:31,  1.14it/s] 77%|███████▋  | 3332/4328 [48:37<14:28,  1.15it/s] 77%|███████▋  | 3333/4328 [48:38<14:31,  1.14it/s] 77%|███████▋  | 3334/4328 [48:39<14:33,  1.14it/s] 77%|███████▋  | 3335/4328 [48:40<14:30,  1.14it/s] 77%|███████▋  | 3336/4328 [48:40<14:27,  1.14it/s] 77%|███████▋  | 3337/4328 [48:41<14:25,  1.14it/s] 77%|███████▋  | 3338/4328 [48:42<14:23,  1.15it/s] 77%|███████▋  | 3339/4328 [48:43<14:30,  1.14it/s] 77%|███████▋  | 3340/4328 [48:44<14:27,  1.14it/s] 77%|███████▋  | 3341/4328 [48:45<14:24,  1.14it/s] 77%|███████▋  | 3342/4328 [48:46<14:23,  1.14it/s] 77%|███████▋  | 3343/4328 [48:47<14:21,  1.14it/s] 77%|███████▋  | 3344/4328 [48:47<14:20,  1.14it/s] 77%|███████▋  | 3345/4328 [48:48<14:23,  1.14it/s] 77%|███████▋  | 3346/4328 [48:49<14:20,  1.14it/s] 77%|███████▋  | 3347/4328 [48:50<14:18,  1.14it/s] 77%|███████▋  | 3348/4328 [48:51<14:18,  1.14it/s] 77%|███████▋  | 3349/4328 [48:52<14:16,  1.14it/s] 77%|███████▋  | 3350/4328 [48:53<14:13,  1.15it/s] 77%|███████▋  | 3351/4328 [48:54<14:14,  1.14it/s] 77%|███████▋  | 3352/4328 [48:54<14:12,  1.14it/s] 77%|███████▋  | 3353/4328 [48:55<14:15,  1.14it/s] 77%|███████▋  | 3354/4328 [48:56<14:13,  1.14it/s] 78%|███████▊  | 3355/4328 [48:57<14:11,  1.14it/s] 78%|███████▊  | 3356/4328 [48:58<14:11,  1.14it/s] 78%|███████▊  | 3357/4328 [48:59<14:09,  1.14it/s] 78%|███████▊  | 3358/4328 [49:00<14:08,  1.14it/s] 78%|███████▊  | 3359/4328 [49:01<14:11,  1.14it/s] 78%|███████▊  | 3360/4328 [49:01<14:08,  1.14it/s] 78%|███████▊  | 3361/4328 [49:02<14:06,  1.14it/s] 78%|███████▊  | 3362/4328 [49:03<14:04,  1.14it/s] 78%|███████▊  | 3363/4328 [49:04<14:02,  1.15it/s] 78%|███████▊  | 3364/4328 [49:05<14:08,  1.14it/s] 78%|███████▊  | 3365/4328 [49:06<14:05,  1.14it/s] 78%|███████▊  | 3366/4328 [49:07<14:02,  1.14it/s] 78%|███████▊  | 3367/4328 [49:08<14:01,  1.14it/s] 78%|███████▊  | 3368/4328 [49:08<13:59,  1.14it/s] 78%|███████▊  | 3369/4328 [49:09<13:57,  1.15it/s] 78%|███████▊  | 3370/4328 [49:10<13:57,  1.14it/s] 78%|███████▊  | 3371/4328 [49:11<13:56,  1.14it/s] 78%|███████▊  | 3372/4328 [49:12<13:54,  1.15it/s] 78%|███████▊  | 3373/4328 [49:13<13:58,  1.14it/s] 78%|███████▊  | 3374/4328 [49:14<13:55,  1.14it/s] 78%|███████▊  | 3375/4328 [49:15<13:53,  1.14it/s] 78%|███████▊  | 3376/4328 [49:15<13:54,  1.14it/s] 78%|███████▊  | 3377/4328 [49:16<13:52,  1.14it/s] 78%|███████▊  | 3378/4328 [49:17<13:50,  1.14it/s] 78%|███████▊  | 3379/4328 [49:18<13:49,  1.14it/s] 78%|███████▊  | 3380/4328 [49:19<13:48,  1.14it/s] 78%|███████▊  | 3381/4328 [49:20<13:50,  1.14it/s] 78%|███████▊  | 3382/4328 [49:21<13:49,  1.14it/s] 78%|███████▊  | 3383/4328 [49:22<13:46,  1.14it/s] 78%|███████▊  | 3384/4328 [49:22<13:46,  1.14it/s] 78%|███████▊  | 3385/4328 [49:23<13:45,  1.14it/s] 78%|███████▊  | 3386/4328 [49:24<13:42,  1.14it/s] 78%|███████▊  | 3387/4328 [49:25<13:42,  1.14it/s] 78%|███████▊  | 3388/4328 [49:26<13:40,  1.15it/s] 78%|███████▊  | 3389/4328 [49:27<13:39,  1.15it/s] 78%|███████▊  | 3390/4328 [49:28<13:42,  1.14it/s] 78%|███████▊  | 3391/4328 [49:29<13:40,  1.14it/s] 78%|███████▊  | 3392/4328 [49:29<13:38,  1.14it/s] 78%|███████▊  | 3393/4328 [49:30<13:39,  1.14it/s] 78%|███████▊  | 3394/4328 [49:31<13:36,  1.14it/s] 78%|███████▊  | 3395/4328 [49:32<13:34,  1.15it/s] 78%|███████▊  | 3396/4328 [49:33<13:34,  1.14it/s] 78%|███████▊  | 3397/4328 [49:34<13:33,  1.14it/s] 79%|███████▊  | 3398/4328 [49:35<13:35,  1.14it/s] 79%|███████▊  | 3399/4328 [49:36<13:33,  1.14it/s] 79%|███████▊  | 3400/4328 [49:36<13:31,  1.14it/s] 79%|███████▊  | 3401/4328 [49:37<13:31,  1.14it/s] 79%|███████▊  | 3402/4328 [49:38<13:30,  1.14it/s] 79%|███████▊  | 3403/4328 [49:39<13:28,  1.14it/s] 79%|███████▊  | 3404/4328 [49:40<13:26,  1.15it/s] 79%|███████▊  | 3405/4328 [49:41<13:25,  1.15it/s] 79%|███████▊  | 3406/4328 [49:42<13:27,  1.14it/s] 79%|███████▊  | 3407/4328 [49:43<13:29,  1.14it/s] 79%|███████▊  | 3408/4328 [49:43<13:26,  1.14it/s] 79%|███████▉  | 3409/4328 [49:44<13:24,  1.14it/s] 79%|███████▉  | 3410/4328 [49:45<13:23,  1.14it/s] 79%|███████▉  | 3411/4328 [49:46<13:22,  1.14it/s] 79%|███████▉  | 3412/4328 [49:47<13:22,  1.14it/s] 79%|███████▉  | 3413/4328 [49:48<13:21,  1.14it/s] 79%|███████▉  | 3414/4328 [49:49<13:19,  1.14it/s] 79%|███████▉  | 3415/4328 [49:50<13:18,  1.14it/s] 79%|███████▉  | 3416/4328 [49:50<13:17,  1.14it/s] 79%|███████▉  | 3417/4328 [49:51<13:15,  1.15it/s] 79%|███████▉  | 3418/4328 [49:52<13:14,  1.15it/s] 79%|███████▉  | 3419/4328 [49:53<13:13,  1.15it/s] 79%|███████▉  | 3420/4328 [49:54<13:11,  1.15it/s] 79%|███████▉  | 3421/4328 [49:55<13:11,  1.15it/s] 79%|███████▉  | 3422/4328 [49:56<13:10,  1.15it/s] 79%|███████▉  | 3423/4328 [49:57<13:09,  1.15it/s] 79%|███████▉  | 3424/4328 [49:57<13:13,  1.14it/s] 79%|███████▉  | 3425/4328 [49:58<13:10,  1.14it/s] 79%|███████▉  | 3426/4328 [49:59<13:08,  1.14it/s] 79%|███████▉  | 3427/4328 [50:00<13:09,  1.14it/s] 79%|███████▉  | 3428/4328 [50:01<13:06,  1.14it/s] 79%|███████▉  | 3429/4328 [50:02<13:05,  1.14it/s] 79%|███████▉  | 3430/4328 [50:03<13:05,  1.14it/s] 79%|███████▉  | 3431/4328 [50:04<13:03,  1.15it/s] 79%|███████▉  | 3432/4328 [50:04<13:01,  1.15it/s] 79%|███████▉  | 3433/4328 [50:05<13:01,  1.14it/s] 79%|███████▉  | 3434/4328 [50:06<13:00,  1.15it/s] 79%|███████▉  | 3435/4328 [50:07<13:03,  1.14it/s] 79%|███████▉  | 3436/4328 [50:08<13:01,  1.14it/s] 79%|███████▉  | 3437/4328 [50:09<12:59,  1.14it/s] 79%|███████▉  | 3438/4328 [50:10<12:59,  1.14it/s] 79%|███████▉  | 3439/4328 [50:11<12:57,  1.14it/s] 79%|███████▉  | 3440/4328 [50:11<12:55,  1.14it/s] 80%|███████▉  | 3441/4328 [50:12<12:54,  1.15it/s] 80%|███████▉  | 3442/4328 [50:13<12:52,  1.15it/s] 80%|███████▉  | 3443/4328 [50:14<12:55,  1.14it/s] 80%|███████▉  | 3444/4328 [50:15<12:57,  1.14it/s] 80%|███████▉  | 3445/4328 [50:16<12:54,  1.14it/s] 80%|███████▉  | 3446/4328 [50:17<12:51,  1.14it/s] 80%|███████▉  | 3447/4328 [50:18<12:51,  1.14it/s] 80%|███████▉  | 3448/4328 [50:18<12:49,  1.14it/s] 80%|███████▉  | 3449/4328 [50:19<12:51,  1.14it/s] 80%|███████▉  | 3450/4328 [50:20<12:49,  1.14it/s] 80%|███████▉  | 3451/4328 [50:21<12:47,  1.14it/s] 80%|███████▉  | 3452/4328 [50:22<12:47,  1.14it/s] 80%|███████▉  | 3453/4328 [50:23<12:45,  1.14it/s] 80%|███████▉  | 3454/4328 [50:24<12:44,  1.14it/s] 80%|███████▉  | 3455/4328 [50:25<12:47,  1.14it/s] 80%|███████▉  | 3456/4328 [50:25<12:44,  1.14it/s] 80%|███████▉  | 3457/4328 [50:26<12:42,  1.14it/s] 80%|███████▉  | 3458/4328 [50:27<12:42,  1.14it/s] 80%|███████▉  | 3459/4328 [50:28<12:40,  1.14it/s] 80%|███████▉  | 3460/4328 [50:29<12:39,  1.14it/s] 80%|███████▉  | 3461/4328 [50:30<12:37,  1.15it/s] 80%|███████▉  | 3462/4328 [50:31<12:35,  1.15it/s] 80%|████████  | 3463/4328 [50:32<12:37,  1.14it/s] 80%|████████  | 3464/4328 [50:32<12:36,  1.14it/s] 80%|████████  | 3465/4328 [50:33<12:34,  1.14it/s] 80%|████████  | 3466/4328 [50:34<12:34,  1.14it/s] 80%|████████  | 3467/4328 [50:35<12:32,  1.14it/s] 80%|████████  | 3468/4328 [50:36<12:30,  1.15it/s] 80%|████████  | 3469/4328 [50:37<12:30,  1.14it/s] 80%|████████  | 3470/4328 [50:38<12:29,  1.15it/s] 80%|████████  | 3471/4328 [50:39<12:27,  1.15it/s] 80%|████████  | 3472/4328 [50:39<12:27,  1.15it/s] 80%|████████  | 3473/4328 [50:40<12:26,  1.15it/s] 80%|████████  | 3474/4328 [50:41<12:25,  1.15it/s] 80%|████████  | 3475/4328 [50:42<12:28,  1.14it/s] 80%|████████  | 3476/4328 [50:43<12:25,  1.14it/s] 80%|████████  | 3477/4328 [50:44<12:24,  1.14it/s] 80%|████████  | 3478/4328 [50:45<12:24,  1.14it/s] 80%|████████  | 3479/4328 [50:46<12:22,  1.14it/s] 80%|████████  | 3480/4328 [50:46<12:20,  1.15it/s] 80%|████████  | 3481/4328 [50:47<12:20,  1.14it/s] 80%|████████  | 3482/4328 [50:48<12:18,  1.15it/s] 80%|████████  | 3483/4328 [50:49<12:16,  1.15it/s] 80%|████████  | 3484/4328 [50:50<12:15,  1.15it/s] 81%|████████  | 3485/4328 [50:51<12:14,  1.15it/s] 81%|████████  | 3486/4328 [50:52<12:20,  1.14it/s] 81%|████████  | 3487/4328 [50:53<12:18,  1.14it/s] 81%|████████  | 3488/4328 [50:53<12:16,  1.14it/s] 81%|████████  | 3489/4328 [50:54<12:13,  1.14it/s] 81%|████████  | 3490/4328 [50:55<12:11,  1.15it/s] 81%|████████  | 3491/4328 [50:56<12:13,  1.14it/s] 81%|████████  | 3492/4328 [50:57<12:15,  1.14it/s] 81%|████████  | 3493/4328 [50:58<12:12,  1.14it/s] 81%|████████  | 3494/4328 [50:59<12:09,  1.14it/s] 81%|████████  | 3495/4328 [51:00<12:09,  1.14it/s] 81%|████████  | 3496/4328 [51:00<12:07,  1.14it/s] 81%|████████  | 3497/4328 [51:01<12:09,  1.14it/s] 81%|████████  | 3498/4328 [51:02<12:07,  1.14it/s] 81%|████████  | 3499/4328 [51:03<12:05,  1.14it/s] 81%|████████  | 3500/4328 [51:04<12:05,  1.14it/s] 81%|████████  | 3501/4328 [51:05<12:03,  1.14it/s] 81%|████████  | 3502/4328 [51:06<12:02,  1.14it/s] 81%|████████  | 3503/4328 [51:07<12:05,  1.14it/s] 81%|████████  | 3504/4328 [51:07<12:02,  1.14it/s] 81%|████████  | 3505/4328 [51:08<12:00,  1.14it/s] 81%|████████  | 3506/4328 [51:09<12:00,  1.14it/s] 81%|████████  | 3507/4328 [51:10<11:58,  1.14it/s] 81%|████████  | 3508/4328 [51:11<11:56,  1.14it/s] 81%|████████  | 3509/4328 [51:12<11:56,  1.14it/s] 81%|████████  | 3510/4328 [51:13<11:55,  1.14it/s] 81%|████████  | 3511/4328 [51:14<11:56,  1.14it/s] 81%|████████  | 3512/4328 [51:14<11:55,  1.14it/s] 81%|████████  | 3513/4328 [51:15<11:53,  1.14it/s] 81%|████████  | 3514/4328 [51:16<11:51,  1.14it/s] 81%|████████  | 3515/4328 [51:17<11:49,  1.15it/s] 81%|████████  | 3516/4328 [51:18<11:50,  1.14it/s] 81%|████████▏ | 3517/4328 [51:19<11:52,  1.14it/s] 81%|████████▏ | 3518/4328 [51:20<11:50,  1.14it/s] 81%|████████▏ | 3519/4328 [51:21<11:47,  1.14it/s] 81%|████████▏ | 3520/4328 [51:21<11:47,  1.14it/s] 81%|████████▏ | 3521/4328 [51:22<11:45,  1.14it/s] 81%|████████▏ | 3522/4328 [51:23<11:44,  1.14it/s] 81%|████████▏ | 3523/4328 [51:24<11:42,  1.15it/s] 81%|████████▏ | 3524/4328 [51:25<11:41,  1.15it/s] 81%|████████▏ | 3525/4328 [51:26<11:46,  1.14it/s] 81%|████████▏ | 3526/4328 [51:27<11:44,  1.14it/s] 81%|████████▏ | 3527/4328 [51:28<11:41,  1.14it/s] 82%|████████▏ | 3528/4328 [51:28<11:39,  1.14it/s] 82%|████████▏ | 3529/4328 [51:29<11:37,  1.15it/s] 82%|████████▏ | 3530/4328 [51:30<11:39,  1.14it/s] 82%|████████▏ | 3531/4328 [51:31<11:40,  1.14it/s] 82%|████████▏ | 3532/4328 [51:32<11:38,  1.14it/s] 82%|████████▏ | 3533/4328 [51:33<11:36,  1.14it/s] 82%|████████▏ | 3534/4328 [51:34<11:35,  1.14it/s] 82%|████████▏ | 3535/4328 [51:35<11:34,  1.14it/s] 82%|████████▏ | 3536/4328 [51:35<11:32,  1.14it/s] 82%|████████▏ | 3537/4328 [51:36<11:30,  1.15it/s] 82%|████████▏ | 3538/4328 [51:37<11:29,  1.15it/s] 82%|████████▏ | 3539/4328 [51:38<11:34,  1.14it/s] 82%|████████▏ | 3540/4328 [51:39<11:32,  1.14it/s] 82%|████████▏ | 3541/4328 [51:40<11:29,  1.14it/s] 82%|████████▏ | 3542/4328 [51:41<11:26,  1.14it/s] 82%|████████▏ | 3543/4328 [51:42<11:24,  1.15it/s] 82%|████████▏ | 3544/4328 [51:42<11:27,  1.14it/s] 82%|████████▏ | 3545/4328 [51:43<11:28,  1.14it/s] 82%|████████▏ | 3546/4328 [51:44<11:26,  1.14it/s] 82%|████████▏ | 3547/4328 [51:45<11:24,  1.14it/s] 82%|████████▏ | 3548/4328 [51:46<11:22,  1.14it/s] 82%|████████▏ | 3549/4328 [51:47<11:20,  1.15it/s] 82%|████████▏ | 3550/4328 [51:48<11:24,  1.14it/s] 82%|████████▏ | 3551/4328 [51:49<11:22,  1.14it/s] 82%|████████▏ | 3552/4328 [51:49<11:20,  1.14it/s] 82%|████████▏ | 3553/4328 [51:50<11:18,  1.14it/s] 82%|████████▏ | 3554/4328 [51:51<11:15,  1.15it/s] 82%|████████▏ | 3555/4328 [51:52<11:17,  1.14it/s] 82%|████████▏ | 3556/4328 [51:53<11:19,  1.14it/s] 82%|████████▏ | 3557/4328 [51:54<11:16,  1.14it/s] 82%|████████▏ | 3558/4328 [51:55<11:14,  1.14it/s] 82%|████████▏ | 3559/4328 [51:56<11:12,  1.14it/s] 82%|████████▏ | 3560/4328 [51:56<11:10,  1.15it/s] 82%|████████▏ | 3561/4328 [51:57<11:12,  1.14it/s] 82%|████████▏ | 3562/4328 [51:58<11:11,  1.14it/s] 82%|████████▏ | 3563/4328 [51:59<11:09,  1.14it/s] 82%|████████▏ | 3564/4328 [52:00<11:09,  1.14it/s] 82%|████████▏ | 3565/4328 [52:01<11:08,  1.14it/s] 82%|████████▏ | 3566/4328 [52:02<11:06,  1.14it/s] 82%|████████▏ | 3567/4328 [52:03<11:04,  1.14it/s] 82%|████████▏ | 3568/4328 [52:03<11:02,  1.15it/s] 82%|████████▏ | 3569/4328 [52:04<11:04,  1.14it/s] 82%|████████▏ | 3570/4328 [52:05<11:06,  1.14it/s] 83%|████████▎ | 3571/4328 [52:06<11:03,  1.14it/s] 83%|████████▎ | 3572/4328 [52:07<11:01,  1.14it/s] 83%|████████▎ | 3573/4328 [52:08<11:01,  1.14it/s] 83%|████████▎ | 3574/4328 [52:09<10:59,  1.14it/s] 83%|████████▎ | 3575/4328 [52:10<10:57,  1.14it/s] 83%|████████▎ | 3576/4328 [52:11<10:57,  1.14it/s] 83%|████████▎ | 3577/4328 [52:11<10:56,  1.14it/s] 83%|████████▎ | 3578/4328 [52:12<10:58,  1.14it/s] 83%|████████▎ | 3579/4328 [52:13<10:56,  1.14it/s] 83%|████████▎ | 3580/4328 [52:14<10:54,  1.14it/s] 83%|████████▎ | 3581/4328 [52:15<10:54,  1.14it/s] 83%|████████▎ | 3582/4328 [52:16<10:52,  1.14it/s] 83%|████████▎ | 3583/4328 [52:17<10:51,  1.14it/s] 83%|████████▎ | 3584/4328 [52:18<10:50,  1.14it/s] 83%|████████▎ | 3585/4328 [52:18<10:49,  1.14it/s] 83%|████████▎ | 3586/4328 [52:19<10:48,  1.14it/s] 83%|████████▎ | 3587/4328 [52:20<10:50,  1.14it/s] 83%|████████▎ | 3588/4328 [52:21<10:48,  1.14it/s] 83%|████████▎ | 3589/4328 [52:22<10:46,  1.14it/s] 83%|████████▎ | 3590/4328 [52:23<10:45,  1.14it/s] 83%|████████▎ | 3591/4328 [52:24<10:44,  1.14it/s] 83%|████████▎ | 3592/4328 [52:25<10:42,  1.15it/s] 83%|████████▎ | 3593/4328 [52:25<10:41,  1.15it/s] 83%|████████▎ | 3594/4328 [52:26<10:39,  1.15it/s] 83%|████████▎ | 3595/4328 [52:27<10:45,  1.14it/s] 83%|████████▎ | 3596/4328 [52:28<10:43,  1.14it/s] 83%|████████▎ | 3597/4328 [52:29<10:41,  1.14it/s] 83%|████████▎ | 3598/4328 [52:30<10:39,  1.14it/s] 83%|████████▎ | 3599/4328 [52:31<10:36,  1.14it/s] 83%|████████▎ | 3600/4328 [52:32<10:38,  1.14it/s] 83%|████████▎ | 3601/4328 [52:32<10:39,  1.14it/s] 83%|████████▎ | 3602/4328 [52:33<10:37,  1.14it/s] 83%|████████▎ | 3603/4328 [52:34<10:35,  1.14it/s] 83%|████████▎ | 3604/4328 [52:35<10:33,  1.14it/s] 83%|████████▎ | 3605/4328 [52:36<10:31,  1.14it/s] 83%|████████▎ | 3606/4328 [52:37<10:35,  1.14it/s] 83%|████████▎ | 3607/4328 [52:38<10:34,  1.14it/s] 83%|████████▎ | 3608/4328 [52:39<10:32,  1.14it/s] 83%|████████▎ | 3609/4328 [52:39<10:33,  1.13it/s] 83%|████████▎ | 3610/4328 [52:40<10:30,  1.14it/s] 83%|████████▎ | 3611/4328 [52:41<10:28,  1.14it/s] 83%|████████▎ | 3612/4328 [52:42<10:26,  1.14it/s] 83%|████████▎ | 3613/4328 [52:43<10:24,  1.14it/s] 84%|████████▎ | 3614/4328 [52:44<10:29,  1.13it/s] 84%|████████▎ | 3615/4328 [52:45<10:26,  1.14it/s] 84%|████████▎ | 3616/4328 [52:46<10:24,  1.14it/s] 84%|████████▎ | 3617/4328 [52:46<10:22,  1.14it/s] 84%|████████▎ | 3618/4328 [52:47<10:20,  1.14it/s] 84%|████████▎ | 3619/4328 [52:48<10:21,  1.14it/s] 84%|████████▎ | 3620/4328 [52:49<10:23,  1.14it/s] 84%|████████▎ | 3621/4328 [52:50<10:20,  1.14it/s] 84%|████████▎ | 3622/4328 [52:51<10:18,  1.14it/s] 84%|████████▎ | 3623/4328 [52:52<10:16,  1.14it/s] 84%|████████▎ | 3624/4328 [52:53<10:14,  1.15it/s] 84%|████████▍ | 3625/4328 [52:53<10:16,  1.14it/s] 84%|████████▍ | 3626/4328 [52:54<10:15,  1.14it/s] 84%|████████▍ | 3627/4328 [52:55<10:13,  1.14it/s] 84%|████████▍ | 3628/4328 [52:56<10:13,  1.14it/s] 84%|████████▍ | 3629/4328 [52:57<10:11,  1.14it/s] 84%|████████▍ | 3630/4328 [52:58<10:10,  1.14it/s] 84%|████████▍ | 3631/4328 [52:59<10:09,  1.14it/s] 84%|████████▍ | 3632/4328 [53:00<10:08,  1.14it/s] 84%|████████▍ | 3633/4328 [53:00<10:07,  1.14it/s] 84%|████████▍ | 3634/4328 [53:01<10:09,  1.14it/s] 84%|████████▍ | 3635/4328 [53:02<10:07,  1.14it/s] 84%|████████▍ | 3636/4328 [53:03<10:05,  1.14it/s] 84%|████████▍ | 3637/4328 [53:04<10:04,  1.14it/s] 84%|████████▍ | 3638/4328 [53:05<10:02,  1.15it/s] 84%|████████▍ | 3639/4328 [53:06<10:06,  1.14it/s] 84%|████████▍ | 3640/4328 [53:07<10:04,  1.14it/s] 84%|████████▍ | 3641/4328 [53:07<10:02,  1.14it/s] 84%|████████▍ | 3642/4328 [53:08<10:01,  1.14it/s] 84%|████████▍ | 3643/4328 [53:09<09:59,  1.14it/s] 84%|████████▍ | 3644/4328 [53:10<09:58,  1.14it/s] 84%|████████▍ | 3645/4328 [53:11<09:59,  1.14it/s] 84%|████████▍ | 3646/4328 [53:12<09:57,  1.14it/s] 84%|████████▍ | 3647/4328 [53:13<09:56,  1.14it/s] 84%|████████▍ | 3648/4328 [53:14<09:56,  1.14it/s] 84%|████████▍ | 3649/4328 [53:14<09:54,  1.14it/s] 84%|████████▍ | 3650/4328 [53:15<09:55,  1.14it/s] 84%|████████▍ | 3651/4328 [53:16<09:54,  1.14it/s] 84%|████████▍ | 3652/4328 [53:17<09:52,  1.14it/s] 84%|████████▍ | 3653/4328 [53:18<09:50,  1.14it/s] 84%|████████▍ | 3654/4328 [53:19<09:48,  1.14it/s] 84%|████████▍ | 3655/4328 [53:20<09:50,  1.14it/s] 84%|████████▍ | 3656/4328 [53:21<09:51,  1.14it/s] 84%|████████▍ | 3657/4328 [53:21<09:49,  1.14it/s] 85%|████████▍ | 3658/4328 [53:22<09:47,  1.14it/s] 85%|████████▍ | 3659/4328 [53:23<09:45,  1.14it/s] 85%|████████▍ | 3660/4328 [53:24<09:43,  1.14it/s] 85%|████████▍ | 3661/4328 [53:25<09:47,  1.14it/s] 85%|████████▍ | 3662/4328 [53:26<09:45,  1.14it/s] 85%|████████▍ | 3663/4328 [53:27<09:42,  1.14it/s] 85%|████████▍ | 3664/4328 [53:28<09:41,  1.14it/s] 85%|████████▍ | 3665/4328 [53:28<09:39,  1.14it/s] 85%|████████▍ | 3666/4328 [53:29<09:40,  1.14it/s] 85%|████████▍ | 3667/4328 [53:30<09:42,  1.14it/s] 85%|████████▍ | 3668/4328 [53:31<09:39,  1.14it/s] 85%|████████▍ | 3669/4328 [53:32<09:37,  1.14it/s] 85%|████████▍ | 3670/4328 [53:33<09:35,  1.14it/s] 85%|████████▍ | 3671/4328 [53:34<09:34,  1.14it/s] 85%|████████▍ | 3672/4328 [53:35<09:38,  1.13it/s] 85%|████████▍ | 3673/4328 [53:36<09:35,  1.14it/s] 85%|████████▍ | 3674/4328 [53:36<09:33,  1.14it/s] 85%|████████▍ | 3675/4328 [53:37<09:32,  1.14it/s] 85%|████████▍ | 3676/4328 [53:38<09:30,  1.14it/s] 85%|████████▍ | 3677/4328 [53:39<09:31,  1.14it/s] 85%|████████▍ | 3678/4328 [53:40<09:32,  1.14it/s] 85%|████████▌ | 3679/4328 [53:41<09:30,  1.14it/s] 85%|████████▌ | 3680/4328 [53:42<09:29,  1.14it/s] 85%|████████▌ | 3681/4328 [53:43<09:27,  1.14it/s] 85%|████████▌ | 3682/4328 [53:43<09:25,  1.14it/s] 85%|████████▌ | 3683/4328 [53:44<09:25,  1.14it/s] 85%|████████▌ | 3684/4328 [53:45<09:24,  1.14it/s] 85%|████████▌ | 3685/4328 [53:46<09:22,  1.14it/s] 85%|████████▌ | 3686/4328 [53:47<09:22,  1.14it/s] 85%|████████▌ | 3687/4328 [53:48<09:20,  1.14it/s] 85%|████████▌ | 3688/4328 [53:49<09:19,  1.14it/s] 85%|████████▌ | 3689/4328 [53:50<09:17,  1.15it/s] 85%|████████▌ | 3690/4328 [53:50<09:16,  1.15it/s] 85%|████████▌ | 3691/4328 [53:51<09:18,  1.14it/s] 85%|████████▌ | 3692/4328 [53:52<09:19,  1.14it/s] 85%|████████▌ | 3693/4328 [53:53<09:17,  1.14it/s] 85%|████████▌ | 3694/4328 [53:54<09:16,  1.14it/s] 85%|████████▌ | 3695/4328 [53:55<09:14,  1.14it/s] 85%|████████▌ | 3696/4328 [53:56<09:12,  1.14it/s] 85%|████████▌ | 3697/4328 [53:57<09:12,  1.14it/s] 85%|████████▌ | 3698/4328 [53:57<09:11,  1.14it/s] 85%|████████▌ | 3699/4328 [53:58<09:10,  1.14it/s] 85%|████████▌ | 3700/4328 [53:59<09:09,  1.14it/s] 86%|████████▌ | 3701/4328 [54:00<09:08,  1.14it/s] 86%|████████▌ | 3702/4328 [54:01<09:07,  1.14it/s] 86%|████████▌ | 3703/4328 [54:02<09:09,  1.14it/s] 86%|████████▌ | 3704/4328 [54:03<09:07,  1.14it/s] 86%|████████▌ | 3705/4328 [54:04<09:05,  1.14it/s] 86%|████████▌ | 3706/4328 [54:04<09:03,  1.14it/s] 86%|████████▌ | 3707/4328 [54:05<09:02,  1.14it/s] 86%|████████▌ | 3708/4328 [54:06<09:06,  1.13it/s] 86%|████████▌ | 3709/4328 [54:07<09:04,  1.14it/s] 86%|████████▌ | 3710/4328 [54:08<09:02,  1.14it/s] 86%|████████▌ | 3711/4328 [54:09<09:00,  1.14it/s] 86%|████████▌ | 3712/4328 [54:10<08:58,  1.14it/s] 86%|████████▌ | 3713/4328 [54:11<08:59,  1.14it/s] 86%|████████▌ | 3714/4328 [54:11<09:00,  1.14it/s] 86%|████████▌ | 3715/4328 [54:12<08:58,  1.14it/s] 86%|████████▌ | 3716/4328 [54:13<08:56,  1.14it/s] 86%|████████▌ | 3717/4328 [54:14<08:54,  1.14it/s] 86%|████████▌ | 3718/4328 [54:15<08:52,  1.14it/s] 86%|████████▌ | 3719/4328 [54:16<08:53,  1.14it/s] 86%|████████▌ | 3720/4328 [54:17<08:52,  1.14it/s] 86%|████████▌ | 3721/4328 [54:18<08:51,  1.14it/s] 86%|████████▌ | 3722/4328 [54:18<08:50,  1.14it/s] 86%|████████▌ | 3723/4328 [54:19<08:49,  1.14it/s] 86%|████████▌ | 3724/4328 [54:20<08:47,  1.14it/s] 86%|████████▌ | 3725/4328 [54:21<08:47,  1.14it/s] 86%|████████▌ | 3726/4328 [54:22<08:46,  1.14it/s] 86%|████████▌ | 3727/4328 [54:23<08:45,  1.14it/s] 86%|████████▌ | 3728/4328 [54:24<08:46,  1.14it/s] 86%|████████▌ | 3729/4328 [54:25<08:44,  1.14it/s] 86%|████████▌ | 3730/4328 [54:25<08:43,  1.14it/s] 86%|████████▌ | 3731/4328 [54:26<08:42,  1.14it/s] 86%|████████▌ | 3732/4328 [54:27<08:41,  1.14it/s] 86%|████████▋ | 3733/4328 [54:28<08:40,  1.14it/s] 86%|████████▋ | 3734/4328 [54:29<08:38,  1.15it/s] 86%|████████▋ | 3735/4328 [54:30<08:37,  1.15it/s] 86%|████████▋ | 3736/4328 [54:31<08:41,  1.14it/s] 86%|████████▋ | 3737/4328 [54:32<08:39,  1.14it/s] 86%|████████▋ | 3738/4328 [54:32<08:38,  1.14it/s] 86%|████████▋ | 3739/4328 [54:33<08:36,  1.14it/s] 86%|████████▋ | 3740/4328 [54:34<08:34,  1.14it/s] 86%|████████▋ | 3741/4328 [54:35<08:32,  1.15it/s] 86%|████████▋ | 3742/4328 [54:36<08:33,  1.14it/s] 86%|████████▋ | 3743/4328 [54:37<08:31,  1.14it/s] 87%|████████▋ | 3744/4328 [54:38<08:30,  1.14it/s] 87%|████████▋ | 3745/4328 [54:39<08:30,  1.14it/s] 87%|████████▋ | 3746/4328 [54:39<08:29,  1.14it/s] 87%|████████▋ | 3747/4328 [54:40<08:27,  1.14it/s] 87%|████████▋ | 3748/4328 [54:41<08:26,  1.15it/s] 87%|████████▋ | 3749/4328 [54:42<08:25,  1.15it/s] 87%|████████▋ | 3750/4328 [54:43<08:28,  1.14it/s] 87%|████████▋ | 3751/4328 [54:44<08:27,  1.14it/s] 87%|████████▋ | 3752/4328 [54:45<08:25,  1.14it/s] 87%|████████▋ | 3753/4328 [54:46<08:23,  1.14it/s] 87%|████████▋ | 3754/4328 [54:46<08:22,  1.14it/s] 87%|████████▋ | 3755/4328 [54:47<08:22,  1.14it/s] 87%|████████▋ | 3756/4328 [54:48<08:22,  1.14it/s] 87%|████████▋ | 3757/4328 [54:49<08:20,  1.14it/s] 87%|████████▋ | 3758/4328 [54:50<08:19,  1.14it/s] 87%|████████▋ | 3759/4328 [54:51<08:17,  1.14it/s] 87%|████████▋ | 3760/4328 [54:52<08:16,  1.14it/s] 87%|████████▋ | 3761/4328 [54:53<08:19,  1.14it/s] 87%|████████▋ | 3762/4328 [54:54<08:17,  1.14it/s] 87%|████████▋ | 3763/4328 [54:54<08:15,  1.14it/s] 87%|████████▋ | 3764/4328 [54:55<08:16,  1.14it/s] 87%|████████▋ | 3765/4328 [54:56<08:14,  1.14it/s] 87%|████████▋ | 3766/4328 [54:57<08:12,  1.14it/s] 87%|████████▋ | 3767/4328 [54:58<08:10,  1.14it/s] 87%|████████▋ | 3768/4328 [54:59<08:09,  1.14it/s] 87%|████████▋ | 3769/4328 [55:00<08:12,  1.13it/s] 87%|████████▋ | 3770/4328 [55:01<08:10,  1.14it/s] 87%|████████▋ | 3771/4328 [55:01<08:08,  1.14it/s] 87%|████████▋ | 3772/4328 [55:02<08:06,  1.14it/s] 87%|████████▋ | 3773/4328 [55:03<08:04,  1.14it/s] 87%|████████▋ | 3774/4328 [55:04<08:05,  1.14it/s] 87%|████████▋ | 3775/4328 [55:05<08:06,  1.14it/s] 87%|████████▋ | 3776/4328 [55:06<08:04,  1.14it/s] 87%|████████▋ | 3777/4328 [55:07<08:02,  1.14it/s] 87%|████████▋ | 3778/4328 [55:08<08:00,  1.14it/s] 87%|████████▋ | 3779/4328 [55:08<07:59,  1.15it/s] 87%|████████▋ | 3780/4328 [55:09<08:02,  1.13it/s] 87%|████████▋ | 3781/4328 [55:10<08:01,  1.14it/s] 87%|████████▋ | 3782/4328 [55:11<07:59,  1.14it/s] 87%|████████▋ | 3783/4328 [55:12<07:57,  1.14it/s] 87%|████████▋ | 3784/4328 [55:13<07:55,  1.14it/s] 87%|████████▋ | 3785/4328 [55:14<07:56,  1.14it/s] 87%|████████▋ | 3786/4328 [55:15<07:57,  1.14it/s] 88%|████████▊ | 3787/4328 [55:15<07:54,  1.14it/s] 88%|████████▊ | 3788/4328 [55:16<07:53,  1.14it/s] 88%|████████▊ | 3789/4328 [55:17<07:52,  1.14it/s] 88%|████████▊ | 3790/4328 [55:18<07:50,  1.14it/s] 88%|████████▊ | 3791/4328 [55:19<07:49,  1.14it/s] 88%|████████▊ | 3792/4328 [55:20<07:48,  1.15it/s] 88%|████████▊ | 3793/4328 [55:21<07:46,  1.15it/s] 88%|████████▊ | 3794/4328 [55:22<07:49,  1.14it/s] 88%|████████▊ | 3795/4328 [55:22<07:48,  1.14it/s] 88%|████████▊ | 3796/4328 [55:23<07:46,  1.14it/s] 88%|████████▊ | 3797/4328 [55:24<07:45,  1.14it/s] 88%|████████▊ | 3798/4328 [55:25<07:43,  1.14it/s] 88%|████████▊ | 3799/4328 [55:26<07:41,  1.15it/s] 88%|████████▊ | 3800/4328 [55:27<07:42,  1.14it/s] 88%|████████▊ | 3801/4328 [55:28<07:41,  1.14it/s] 88%|████████▊ | 3802/4328 [55:29<07:39,  1.14it/s] 88%|████████▊ | 3803/4328 [55:29<07:39,  1.14it/s] 88%|████████▊ | 3804/4328 [55:30<07:38,  1.14it/s] 88%|████████▊ | 3805/4328 [55:31<07:39,  1.14it/s] 88%|████████▊ | 3806/4328 [55:32<07:38,  1.14it/s] 88%|████████▊ | 3807/4328 [55:33<07:36,  1.14it/s] 88%|████████▊ | 3808/4328 [55:34<07:35,  1.14it/s] 88%|████████▊ | 3809/4328 [55:35<07:33,  1.14it/s] 88%|████████▊ | 3810/4328 [55:36<07:34,  1.14it/s] 88%|████████▊ | 3811/4328 [55:36<07:35,  1.14it/s] 88%|████████▊ | 3812/4328 [55:37<07:33,  1.14it/s] 88%|████████▊ | 3813/4328 [55:38<07:31,  1.14it/s] 88%|████████▊ | 3814/4328 [55:39<07:30,  1.14it/s] 88%|████████▊ | 3815/4328 [55:40<07:28,  1.14it/s] 88%|████████▊ | 3816/4328 [55:41<07:30,  1.14it/s] 88%|████████▊ | 3817/4328 [55:42<07:29,  1.14it/s] 88%|████████▊ | 3818/4328 [55:43<07:27,  1.14it/s] 88%|████████▊ | 3819/4328 [55:43<07:26,  1.14it/s] 88%|████████▊ | 3820/4328 [55:44<07:24,  1.14it/s] 88%|████████▊ | 3821/4328 [55:45<07:23,  1.14it/s] 88%|████████▊ | 3822/4328 [55:46<07:24,  1.14it/s] 88%|████████▊ | 3823/4328 [55:47<07:22,  1.14it/s] 88%|████████▊ | 3824/4328 [55:48<07:21,  1.14it/s] 88%|████████▊ | 3825/4328 [55:49<07:19,  1.14it/s] 88%|████████▊ | 3826/4328 [55:50<07:18,  1.15it/s] 88%|████████▊ | 3827/4328 [55:50<07:19,  1.14it/s] 88%|████████▊ | 3828/4328 [55:51<07:18,  1.14it/s] 88%|████████▊ | 3829/4328 [55:52<07:17,  1.14it/s] 88%|████████▊ | 3830/4328 [55:53<07:16,  1.14it/s] 89%|████████▊ | 3831/4328 [55:54<07:15,  1.14it/s] 89%|████████▊ | 3832/4328 [55:55<07:13,  1.14it/s] 89%|████████▊ | 3833/4328 [55:56<07:12,  1.14it/s] 89%|████████▊ | 3834/4328 [55:57<07:11,  1.15it/s] 89%|████████▊ | 3835/4328 [55:57<07:12,  1.14it/s] 89%|████████▊ | 3836/4328 [55:58<07:12,  1.14it/s] 89%|████████▊ | 3837/4328 [55:59<07:10,  1.14it/s] 89%|████████▊ | 3838/4328 [56:00<07:09,  1.14it/s] 89%|████████▊ | 3839/4328 [56:01<07:07,  1.14it/s] 89%|████████▊ | 3840/4328 [56:02<07:06,  1.14it/s] 89%|████████▊ | 3841/4328 [56:03<07:09,  1.13it/s] 89%|████████▉ | 3842/4328 [56:04<07:07,  1.14it/s] 89%|████████▉ | 3843/4328 [56:05<07:06,  1.14it/s] 89%|████████▉ | 3844/4328 [56:05<07:06,  1.13it/s] 89%|████████▉ | 3845/4328 [56:06<07:04,  1.14it/s] 89%|████████▉ | 3846/4328 [56:07<07:02,  1.14it/s] 89%|████████▉ | 3847/4328 [56:08<07:02,  1.14it/s] 89%|████████▉ | 3848/4328 [56:09<07:00,  1.14it/s] 89%|████████▉ | 3849/4328 [56:10<07:00,  1.14it/s] 89%|████████▉ | 3850/4328 [56:11<06:59,  1.14it/s] 89%|████████▉ | 3851/4328 [56:12<06:57,  1.14it/s] 89%|████████▉ | 3852/4328 [56:12<06:57,  1.14it/s] 89%|████████▉ | 3853/4328 [56:13<06:55,  1.14it/s] 89%|████████▉ | 3854/4328 [56:14<06:54,  1.14it/s] 89%|████████▉ | 3855/4328 [56:15<06:56,  1.14it/s] 89%|████████▉ | 3856/4328 [56:16<06:54,  1.14it/s] 89%|████████▉ | 3857/4328 [56:17<06:53,  1.14it/s] 89%|████████▉ | 3858/4328 [56:18<06:51,  1.14it/s] 89%|████████▉ | 3859/4328 [56:19<06:49,  1.14it/s] 89%|████████▉ | 3860/4328 [56:19<06:52,  1.13it/s] 89%|████████▉ | 3861/4328 [56:20<06:50,  1.14it/s] 89%|████████▉ | 3862/4328 [56:21<06:49,  1.14it/s] 89%|████████▉ | 3863/4328 [56:22<06:47,  1.14it/s] 89%|████████▉ | 3864/4328 [56:23<06:46,  1.14it/s] 89%|████████▉ | 3865/4328 [56:24<06:44,  1.14it/s] 89%|████████▉ | 3866/4328 [56:25<06:45,  1.14it/s] 89%|████████▉ | 3867/4328 [56:26<06:43,  1.14it/s] 89%|████████▉ | 3868/4328 [56:26<06:42,  1.14it/s] 89%|████████▉ | 3869/4328 [56:27<06:41,  1.14it/s] 89%|████████▉ | 3870/4328 [56:28<06:40,  1.14it/s] 89%|████████▉ | 3871/4328 [56:29<06:42,  1.13it/s] 89%|████████▉ | 3872/4328 [56:30<06:41,  1.14it/s] 89%|████████▉ | 3873/4328 [56:31<06:39,  1.14it/s] 90%|████████▉ | 3874/4328 [56:32<06:37,  1.14it/s] 90%|████████▉ | 3875/4328 [56:33<06:36,  1.14it/s] 90%|████████▉ | 3876/4328 [56:33<06:36,  1.14it/s] 90%|████████▉ | 3877/4328 [56:34<06:36,  1.14it/s] 90%|████████▉ | 3878/4328 [56:35<06:35,  1.14it/s] 90%|████████▉ | 3879/4328 [56:36<06:33,  1.14it/s] 90%|████████▉ | 3880/4328 [56:37<06:32,  1.14it/s] 90%|████████▉ | 3881/4328 [56:38<06:30,  1.14it/s] 90%|████████▉ | 3882/4328 [56:39<06:30,  1.14it/s] 90%|████████▉ | 3883/4328 [56:40<06:29,  1.14it/s] 90%|████████▉ | 3884/4328 [56:40<06:28,  1.14it/s] 90%|████████▉ | 3885/4328 [56:41<06:27,  1.14it/s] 90%|████████▉ | 3886/4328 [56:42<06:26,  1.14it/s] 90%|████████▉ | 3887/4328 [56:43<06:25,  1.14it/s] 90%|████████▉ | 3888/4328 [56:44<06:24,  1.15it/s] 90%|████████▉ | 3889/4328 [56:45<06:23,  1.15it/s] 90%|████████▉ | 3890/4328 [56:46<06:23,  1.14it/s] 90%|████████▉ | 3891/4328 [56:47<06:24,  1.14it/s] 90%|████████▉ | 3892/4328 [56:47<06:22,  1.14it/s] 90%|████████▉ | 3893/4328 [56:48<06:21,  1.14it/s] 90%|████████▉ | 3894/4328 [56:49<06:19,  1.14it/s] 90%|████████▉ | 3895/4328 [56:50<06:18,  1.14it/s] 90%|█████████ | 3896/4328 [56:51<06:20,  1.14it/s] 90%|█████████ | 3897/4328 [56:52<06:18,  1.14it/s] 90%|█████████ | 3898/4328 [56:53<06:17,  1.14it/s] 90%|█████████ | 3899/4328 [56:54<06:15,  1.14it/s] 90%|█████████ | 3900/4328 [56:54<06:14,  1.14it/s] 90%|█████████ | 3901/4328 [56:55<06:14,  1.14it/s] 90%|█████████ | 3902/4328 [56:56<06:15,  1.14it/s] 90%|█████████ | 3903/4328 [56:57<06:13,  1.14it/s] 90%|█████████ | 3904/4328 [56:58<06:12,  1.14it/s] 90%|█████████ | 3905/4328 [56:59<06:10,  1.14it/s] 90%|█████████ | 3906/4328 [57:00<06:09,  1.14it/s] 90%|█████████ | 3907/4328 [57:01<06:08,  1.14it/s] 90%|█████████ | 3908/4328 [57:01<06:07,  1.14it/s] 90%|█████████ | 3909/4328 [57:02<06:06,  1.14it/s] 90%|█████████ | 3910/4328 [57:03<06:05,  1.14it/s] 90%|█████████ | 3911/4328 [57:04<06:04,  1.14it/s] 90%|█████████ | 3912/4328 [57:05<06:03,  1.14it/s] 90%|█████████ | 3913/4328 [57:06<06:04,  1.14it/s] 90%|█████████ | 3914/4328 [57:07<06:02,  1.14it/s] 90%|█████████ | 3915/4328 [57:08<06:01,  1.14it/s] 90%|█████████ | 3916/4328 [57:08<06:01,  1.14it/s] 91%|█████████ | 3917/4328 [57:09<05:59,  1.14it/s] 91%|█████████ | 3918/4328 [57:10<06:00,  1.14it/s] 91%|█████████ | 3919/4328 [57:11<05:58,  1.14it/s] 91%|█████████ | 3920/4328 [57:12<05:57,  1.14it/s] 91%|█████████ | 3921/4328 [57:13<05:55,  1.14it/s] 91%|█████████ | 3922/4328 [57:14<05:54,  1.15it/s] 91%|█████████ | 3923/4328 [57:15<05:53,  1.15it/s] 91%|█████████ | 3924/4328 [57:16<05:54,  1.14it/s] 91%|█████████ | 3925/4328 [57:16<05:52,  1.14it/s] 91%|█████████ | 3926/4328 [57:17<05:51,  1.14it/s] 91%|█████████ | 3927/4328 [57:18<05:52,  1.14it/s] 91%|█████████ | 3928/4328 [57:19<05:50,  1.14it/s] 91%|█████████ | 3929/4328 [57:20<05:48,  1.14it/s] 91%|█████████ | 3930/4328 [57:21<05:47,  1.15it/s] 91%|█████████ | 3931/4328 [57:22<05:46,  1.15it/s] 91%|█████████ | 3932/4328 [57:23<05:48,  1.14it/s] 91%|█████████ | 3933/4328 [57:23<05:47,  1.14it/s] 91%|█████████ | 3934/4328 [57:24<05:45,  1.14it/s] 91%|█████████ | 3935/4328 [57:25<05:43,  1.14it/s] 91%|█████████ | 3936/4328 [57:26<05:42,  1.14it/s] 91%|█████████ | 3937/4328 [57:27<05:42,  1.14it/s] 91%|█████████ | 3938/4328 [57:28<05:43,  1.14it/s] 91%|█████████ | 3939/4328 [57:29<05:41,  1.14it/s] 91%|█████████ | 3940/4328 [57:30<05:39,  1.14it/s] 91%|█████████ | 3941/4328 [57:30<05:39,  1.14it/s] 91%|█████████ | 3942/4328 [57:31<05:37,  1.14it/s] 91%|█████████ | 3943/4328 [57:32<05:38,  1.14it/s] 91%|█████████ | 3944/4328 [57:33<05:36,  1.14it/s] 91%|█████████ | 3945/4328 [57:34<05:35,  1.14it/s] 91%|█████████ | 3946/4328 [57:35<05:34,  1.14it/s] 91%|█████████ | 3947/4328 [57:36<05:33,  1.14it/s] 91%|█████████ | 3948/4328 [57:37<05:32,  1.14it/s] 91%|█████████ | 3949/4328 [57:37<05:33,  1.14it/s] 91%|█████████▏| 3950/4328 [57:38<05:31,  1.14it/s] 91%|█████████▏| 3951/4328 [57:39<05:30,  1.14it/s] 91%|█████████▏| 3952/4328 [57:40<05:29,  1.14it/s] 91%|█████████▏| 3953/4328 [57:41<05:28,  1.14it/s] 91%|█████████▏| 3954/4328 [57:42<05:28,  1.14it/s] 91%|█████████▏| 3955/4328 [57:43<05:27,  1.14it/s] 91%|█████████▏| 3956/4328 [57:44<05:26,  1.14it/s] 91%|█████████▏| 3957/4328 [57:44<05:24,  1.14it/s] 91%|█████████▏| 3958/4328 [57:45<05:23,  1.14it/s] 91%|█████████▏| 3959/4328 [57:46<05:23,  1.14it/s] 91%|█████████▏| 3960/4328 [57:47<05:23,  1.14it/s] 92%|█████████▏| 3961/4328 [57:48<05:22,  1.14it/s] 92%|█████████▏| 3962/4328 [57:49<05:20,  1.14it/s] 92%|█████████▏| 3963/4328 [57:50<05:19,  1.14it/s] 92%|█████████▏| 3964/4328 [57:51<05:17,  1.15it/s] 92%|█████████▏| 3965/4328 [57:51<05:19,  1.14it/s] 92%|█████████▏| 3966/4328 [57:52<05:18,  1.14it/s] 92%|█████████▏| 3967/4328 [57:53<05:16,  1.14it/s] 92%|█████████▏| 3968/4328 [57:54<05:15,  1.14it/s] 92%|█████████▏| 3969/4328 [57:55<05:13,  1.14it/s] 92%|█████████▏| 3970/4328 [57:56<05:14,  1.14it/s] 92%|█████████▏| 3971/4328 [57:57<05:14,  1.13it/s] 92%|█████████▏| 3972/4328 [57:58<05:12,  1.14it/s] 92%|█████████▏| 3973/4328 [57:58<05:11,  1.14it/s] 92%|█████████▏| 3974/4328 [57:59<05:09,  1.14it/s] 92%|█████████▏| 3975/4328 [58:00<05:08,  1.14it/s] 92%|█████████▏| 3976/4328 [58:01<05:09,  1.14it/s] 92%|█████████▏| 3977/4328 [58:02<05:08,  1.14it/s] 92%|█████████▏| 3978/4328 [58:03<05:06,  1.14it/s] 92%|█████████▏| 3979/4328 [58:04<05:05,  1.14it/s] 92%|█████████▏| 3980/4328 [58:05<05:04,  1.14it/s] 92%|█████████▏| 3981/4328 [58:05<05:04,  1.14it/s] 92%|█████████▏| 3982/4328 [58:06<05:04,  1.14it/s] 92%|█████████▏| 3983/4328 [58:07<05:02,  1.14it/s] 92%|█████████▏| 3984/4328 [58:08<05:01,  1.14it/s] 92%|█████████▏| 3985/4328 [58:09<04:59,  1.14it/s] 92%|█████████▏| 3986/4328 [58:10<04:58,  1.15it/s] 92%|█████████▏| 3987/4328 [58:11<05:00,  1.14it/s] 92%|█████████▏| 3988/4328 [58:12<04:58,  1.14it/s] 92%|█████████▏| 3989/4328 [58:12<04:57,  1.14it/s] 92%|█████████▏| 3990/4328 [58:13<04:55,  1.14it/s] 92%|█████████▏| 3991/4328 [58:14<04:54,  1.14it/s] 92%|█████████▏| 3992/4328 [58:15<04:54,  1.14it/s] 92%|█████████▏| 3993/4328 [58:16<04:54,  1.14it/s] 92%|█████████▏| 3994/4328 [58:17<04:53,  1.14it/s] 92%|█████████▏| 3995/4328 [58:18<04:51,  1.14it/s] 92%|█████████▏| 3996/4328 [58:19<04:50,  1.14it/s] 92%|█████████▏| 3997/4328 [58:19<04:49,  1.15it/s] 92%|█████████▏| 3998/4328 [58:20<04:50,  1.14it/s] 92%|█████████▏| 3999/4328 [58:21<04:49,  1.14it/s] 92%|█████████▏| 4000/4328 [58:22<04:47,  1.14it/s] 92%|█████████▏| 4001/4328 [58:23<04:46,  1.14it/s] 92%|█████████▏| 4002/4328 [58:24<04:44,  1.14it/s] 92%|█████████▏| 4003/4328 [58:25<04:44,  1.14it/s] 93%|█████████▎| 4004/4328 [58:26<04:44,  1.14it/s] 93%|█████████▎| 4005/4328 [58:27<04:43,  1.14it/s] 93%|█████████▎| 4006/4328 [58:27<04:42,  1.14it/s] 93%|█████████▎| 4007/4328 [58:28<04:40,  1.14it/s] 93%|█████████▎| 4008/4328 [58:29<04:39,  1.15it/s] 93%|█████████▎| 4009/4328 [58:30<04:41,  1.14it/s] 93%|█████████▎| 4010/4328 [58:31<04:39,  1.14it/s] 93%|█████████▎| 4011/4328 [58:32<04:38,  1.14it/s] 93%|█████████▎| 4012/4328 [58:33<04:36,  1.14it/s] 93%|█████████▎| 4013/4328 [58:34<04:35,  1.14it/s] 93%|█████████▎| 4014/4328 [58:34<04:35,  1.14it/s] 93%|█████████▎| 4015/4328 [58:35<04:35,  1.14it/s] 93%|█████████▎| 4016/4328 [58:36<04:34,  1.14it/s] 93%|█████████▎| 4017/4328 [58:37<04:32,  1.14it/s] 93%|█████████▎| 4018/4328 [58:38<04:31,  1.14it/s] 93%|█████████▎| 4019/4328 [58:39<04:29,  1.15it/s] 93%|█████████▎| 4020/4328 [58:40<04:31,  1.14it/s] 93%|█████████▎| 4021/4328 [58:41<04:30,  1.14it/s] 93%|█████████▎| 4022/4328 [58:41<04:28,  1.14it/s] 93%|█████████▎| 4023/4328 [58:42<04:28,  1.14it/s] 93%|█████████▎| 4024/4328 [58:43<04:26,  1.14it/s] 93%|█████████▎| 4025/4328 [58:44<04:25,  1.14it/s] 93%|█████████▎| 4026/4328 [58:45<04:24,  1.14it/s] 93%|█████████▎| 4027/4328 [58:46<04:23,  1.14it/s] 93%|█████████▎| 4028/4328 [58:47<04:22,  1.15it/s] 93%|█████████▎| 4029/4328 [58:48<04:21,  1.14it/s] 93%|█████████▎| 4030/4328 [58:48<04:20,  1.14it/s] 93%|█████████▎| 4031/4328 [58:49<04:20,  1.14it/s] 93%|█████████▎| 4032/4328 [58:50<04:19,  1.14it/s] 93%|█████████▎| 4033/4328 [58:51<04:18,  1.14it/s] 93%|█████████▎| 4034/4328 [58:52<04:17,  1.14it/s] 93%|█████████▎| 4035/4328 [58:53<04:16,  1.14it/s] 93%|█████████▎| 4036/4328 [58:54<04:14,  1.15it/s] 93%|█████████▎| 4037/4328 [58:55<04:14,  1.14it/s] 93%|█████████▎| 4038/4328 [58:55<04:13,  1.14it/s] 93%|█████████▎| 4039/4328 [58:56<04:12,  1.14it/s] 93%|█████████▎| 4040/4328 [58:57<04:12,  1.14it/s] 93%|█████████▎| 4041/4328 [58:58<04:11,  1.14it/s] 93%|█████████▎| 4042/4328 [58:59<04:10,  1.14it/s] 93%|█████████▎| 4043/4328 [59:00<04:09,  1.14it/s] 93%|█████████▎| 4044/4328 [59:01<04:08,  1.14it/s] 93%|█████████▎| 4045/4328 [59:02<04:07,  1.14it/s] 93%|█████████▎| 4046/4328 [59:02<04:06,  1.14it/s] 94%|█████████▎| 4047/4328 [59:03<04:05,  1.14it/s] 94%|█████████▎| 4048/4328 [59:04<04:05,  1.14it/s] 94%|█████████▎| 4049/4328 [59:05<04:04,  1.14it/s] 94%|█████████▎| 4050/4328 [59:06<04:03,  1.14it/s] 94%|█████████▎| 4051/4328 [59:07<04:02,  1.14it/s] 94%|█████████▎| 4052/4328 [59:08<04:01,  1.14it/s] 94%|█████████▎| 4053/4328 [59:09<04:00,  1.14it/s] 94%|█████████▎| 4054/4328 [59:09<04:01,  1.14it/s] 94%|█████████▎| 4055/4328 [59:10<03:59,  1.14it/s] 94%|█████████▎| 4056/4328 [59:11<03:58,  1.14it/s] 94%|█████████▎| 4057/4328 [59:12<03:56,  1.14it/s] 94%|█████████▍| 4058/4328 [59:13<03:55,  1.15it/s] 94%|█████████▍| 4059/4328 [59:14<03:55,  1.14it/s] 94%|█████████▍| 4060/4328 [59:15<03:54,  1.14it/s] 94%|█████████▍| 4061/4328 [59:16<03:53,  1.14it/s] 94%|█████████▍| 4062/4328 [59:16<03:53,  1.14it/s] 94%|█████████▍| 4063/4328 [59:17<03:52,  1.14it/s] 94%|█████████▍| 4064/4328 [59:18<03:50,  1.14it/s] 94%|█████████▍| 4065/4328 [59:19<03:49,  1.15it/s] 94%|█████████▍| 4066/4328 [59:20<03:48,  1.15it/s] 94%|█████████▍| 4067/4328 [59:21<03:48,  1.14it/s] 94%|█████████▍| 4068/4328 [59:22<03:48,  1.14it/s] 94%|█████████▍| 4069/4328 [59:23<03:47,  1.14it/s] 94%|█████████▍| 4070/4328 [59:23<03:45,  1.14it/s] 94%|█████████▍| 4071/4328 [59:24<03:44,  1.14it/s] 94%|█████████▍| 4072/4328 [59:25<03:43,  1.15it/s] 94%|█████████▍| 4073/4328 [59:26<03:44,  1.14it/s] 94%|█████████▍| 4074/4328 [59:27<03:43,  1.14it/s] 94%|█████████▍| 4075/4328 [59:28<03:41,  1.14it/s] 94%|█████████▍| 4076/4328 [59:29<03:40,  1.14it/s] 94%|█████████▍| 4077/4328 [59:30<03:39,  1.15it/s] 94%|█████████▍| 4078/4328 [59:30<03:39,  1.14it/s] 94%|█████████▍| 4079/4328 [59:31<03:39,  1.14it/s] 94%|█████████▍| 4080/4328 [59:32<03:37,  1.14it/s] 94%|█████████▍| 4081/4328 [59:33<03:36,  1.14it/s] 94%|█████████▍| 4082/4328 [59:34<03:35,  1.14it/s] 94%|█████████▍| 4083/4328 [59:35<03:33,  1.15it/s] 94%|█████████▍| 4084/4328 [59:36<03:34,  1.14it/s] 94%|█████████▍| 4085/4328 [59:37<03:33,  1.14it/s] 94%|█████████▍| 4086/4328 [59:37<03:31,  1.14it/s] 94%|█████████▍| 4087/4328 [59:38<03:31,  1.14it/s] 94%|█████████▍| 4088/4328 [59:39<03:30,  1.14it/s] 94%|█████████▍| 4089/4328 [59:40<03:28,  1.14it/s] 95%|█████████▍| 4090/4328 [59:41<03:27,  1.14it/s] 95%|█████████▍| 4091/4328 [59:42<03:26,  1.15it/s] 95%|█████████▍| 4092/4328 [59:43<03:26,  1.14it/s] 95%|█████████▍| 4093/4328 [59:44<03:26,  1.14it/s] 95%|█████████▍| 4094/4328 [59:44<03:25,  1.14it/s] 95%|█████████▍| 4095/4328 [59:45<03:24,  1.14it/s] 95%|█████████▍| 4096/4328 [59:46<03:22,  1.14it/s] 95%|█████████▍| 4097/4328 [59:47<03:21,  1.15it/s] 95%|█████████▍| 4098/4328 [59:48<03:21,  1.14it/s] 95%|█████████▍| 4099/4328 [59:49<03:20,  1.14it/s] 95%|█████████▍| 4100/4328 [59:50<03:19,  1.14it/s] 95%|█████████▍| 4101/4328 [59:51<03:18,  1.14it/s] 95%|█████████▍| 4102/4328 [59:51<03:17,  1.14it/s] 95%|█████████▍| 4103/4328 [59:52<03:16,  1.14it/s] 95%|█████████▍| 4104/4328 [59:53<03:15,  1.14it/s] 95%|█████████▍| 4105/4328 [59:54<03:14,  1.14it/s] 95%|█████████▍| 4106/4328 [59:55<03:14,  1.14it/s] 95%|█████████▍| 4107/4328 [59:56<03:14,  1.14it/s] 95%|█████████▍| 4108/4328 [59:57<03:12,  1.14it/s] 95%|█████████▍| 4109/4328 [59:58<03:11,  1.14it/s] 95%|█████████▍| 4110/4328 [59:58<03:10,  1.14it/s] 95%|█████████▍| 4111/4328 [59:59<03:09,  1.14it/s] 95%|█████████▌| 4112/4328 [1:00:00<03:10,  1.13it/s] 95%|█████████▌| 4113/4328 [1:00:01<03:09,  1.14it/s] 95%|█████████▌| 4114/4328 [1:00:02<03:07,  1.14it/s] 95%|█████████▌| 4115/4328 [1:00:03<03:06,  1.14it/s] 95%|█████████▌| 4116/4328 [1:00:04<03:05,  1.14it/s] 95%|█████████▌| 4117/4328 [1:00:05<03:05,  1.14it/s] 95%|█████████▌| 4118/4328 [1:00:06<03:04,  1.14it/s] 95%|█████████▌| 4119/4328 [1:00:06<03:03,  1.14it/s] 95%|█████████▌| 4120/4328 [1:00:07<03:02,  1.14it/s] 95%|█████████▌| 4121/4328 [1:00:08<03:01,  1.14it/s] 95%|█████████▌| 4122/4328 [1:00:09<03:00,  1.14it/s] 95%|█████████▌| 4123/4328 [1:00:10<03:00,  1.14it/s] 95%|█████████▌| 4124/4328 [1:00:11<02:59,  1.14it/s] 95%|█████████▌| 4125/4328 [1:00:12<02:58,  1.14it/s] 95%|█████████▌| 4126/4328 [1:00:13<02:57,  1.14it/s] 95%|█████████▌| 4127/4328 [1:00:13<02:56,  1.14it/s] 95%|█████████▌| 4128/4328 [1:00:14<02:55,  1.14it/s] 95%|█████████▌| 4129/4328 [1:00:15<02:54,  1.14it/s] 95%|█████████▌| 4130/4328 [1:00:16<02:52,  1.15it/s] 95%|█████████▌| 4131/4328 [1:00:17<02:53,  1.14it/s] 95%|█████████▌| 4132/4328 [1:00:18<02:52,  1.14it/s] 95%|█████████▌| 4133/4328 [1:00:19<02:51,  1.14it/s] 96%|█████████▌| 4134/4328 [1:00:20<02:49,  1.14it/s] 96%|█████████▌| 4135/4328 [1:00:20<02:48,  1.14it/s] 96%|█████████▌| 4136/4328 [1:00:21<02:48,  1.14it/s] 96%|█████████▌| 4137/4328 [1:00:22<02:48,  1.14it/s] 96%|█████████▌| 4138/4328 [1:00:23<02:46,  1.14it/s] 96%|█████████▌| 4139/4328 [1:00:24<02:45,  1.14it/s] 96%|█████████▌| 4140/4328 [1:00:25<02:44,  1.14it/s] 96%|█████████▌| 4141/4328 [1:00:26<02:43,  1.14it/s] 96%|█████████▌| 4142/4328 [1:00:27<02:42,  1.14it/s] 96%|█████████▌| 4143/4328 [1:00:27<02:41,  1.14it/s] 96%|█████████▌| 4144/4328 [1:00:28<02:40,  1.14it/s] 96%|█████████▌| 4145/4328 [1:00:29<02:40,  1.14it/s] 96%|█████████▌| 4146/4328 [1:00:30<02:39,  1.14it/s] 96%|█████████▌| 4147/4328 [1:00:31<02:38,  1.14it/s] 96%|█████████▌| 4148/4328 [1:00:32<02:37,  1.14it/s] 96%|█████████▌| 4149/4328 [1:00:33<02:36,  1.14it/s] 96%|█████████▌| 4150/4328 [1:00:34<02:35,  1.14it/s] 96%|█████████▌| 4151/4328 [1:00:34<02:35,  1.14it/s] 96%|█████████▌| 4152/4328 [1:00:35<02:34,  1.14it/s] 96%|█████████▌| 4153/4328 [1:00:36<02:33,  1.14it/s] 96%|█████████▌| 4154/4328 [1:00:37<02:32,  1.14it/s] 96%|█████████▌| 4155/4328 [1:00:38<02:31,  1.14it/s] 96%|█████████▌| 4156/4328 [1:00:39<02:31,  1.13it/s] 96%|█████████▌| 4157/4328 [1:00:40<02:30,  1.14it/s] 96%|█████████▌| 4158/4328 [1:00:41<02:29,  1.14it/s] 96%|█████████▌| 4159/4328 [1:00:41<02:28,  1.14it/s] 96%|█████████▌| 4160/4328 [1:00:42<02:26,  1.14it/s] 96%|█████████▌| 4161/4328 [1:00:43<02:26,  1.14it/s] 96%|█████████▌| 4162/4328 [1:00:44<02:26,  1.14it/s] 96%|█████████▌| 4163/4328 [1:00:45<02:25,  1.14it/s] 96%|█████████▌| 4164/4328 [1:00:46<02:23,  1.14it/s] 96%|█████████▌| 4165/4328 [1:00:47<02:22,  1.14it/s] 96%|█████████▋| 4166/4328 [1:00:48<02:21,  1.14it/s] 96%|█████████▋| 4167/4328 [1:00:48<02:21,  1.14it/s] 96%|█████████▋| 4168/4328 [1:00:49<02:20,  1.14it/s] 96%|█████████▋| 4169/4328 [1:00:50<02:19,  1.14it/s] 96%|█████████▋| 4170/4328 [1:00:51<02:18,  1.14it/s] 96%|█████████▋| 4171/4328 [1:00:52<02:17,  1.14it/s] 96%|█████████▋| 4172/4328 [1:00:53<02:16,  1.14it/s] 96%|█████████▋| 4173/4328 [1:00:54<02:16,  1.14it/s] 96%|█████████▋| 4174/4328 [1:00:55<02:15,  1.14it/s] 96%|█████████▋| 4175/4328 [1:00:55<02:14,  1.14it/s] 96%|█████████▋| 4176/4328 [1:00:56<02:12,  1.14it/s] 97%|█████████▋| 4177/4328 [1:00:57<02:11,  1.14it/s] 97%|█████████▋| 4178/4328 [1:00:58<02:11,  1.14it/s] 97%|█████████▋| 4179/4328 [1:00:59<02:10,  1.14it/s] 97%|█████████▋| 4180/4328 [1:01:00<02:09,  1.14it/s] 97%|█████████▋| 4181/4328 [1:01:01<02:08,  1.14it/s] 97%|█████████▋| 4182/4328 [1:01:02<02:07,  1.14it/s] 97%|█████████▋| 4183/4328 [1:01:02<02:06,  1.14it/s] 97%|█████████▋| 4184/4328 [1:01:03<02:05,  1.14it/s] 97%|█████████▋| 4185/4328 [1:01:04<02:04,  1.15it/s] 97%|█████████▋| 4186/4328 [1:01:05<02:04,  1.14it/s] 97%|█████████▋| 4187/4328 [1:01:06<02:04,  1.14it/s] 97%|█████████▋| 4188/4328 [1:01:07<02:02,  1.14it/s] 97%|█████████▋| 4189/4328 [1:01:08<02:01,  1.14it/s] 97%|█████████▋| 4190/4328 [1:01:09<02:00,  1.14it/s] 97%|█████████▋| 4191/4328 [1:01:09<01:59,  1.15it/s] 97%|█████████▋| 4192/4328 [1:01:10<01:59,  1.14it/s] 97%|█████████▋| 4193/4328 [1:01:11<01:58,  1.14it/s] 97%|█████████▋| 4194/4328 [1:01:12<01:57,  1.14it/s] 97%|█████████▋| 4195/4328 [1:01:13<01:56,  1.14it/s] 97%|█████████▋| 4196/4328 [1:01:14<01:55,  1.14it/s] 97%|█████████▋| 4197/4328 [1:01:15<01:54,  1.14it/s] 97%|█████████▋| 4198/4328 [1:01:16<01:53,  1.14it/s] 97%|█████████▋| 4199/4328 [1:01:16<01:52,  1.15it/s] 97%|█████████▋| 4200/4328 [1:01:17<01:52,  1.14it/s] 97%|█████████▋| 4201/4328 [1:01:18<01:51,  1.14it/s] 97%|█████████▋| 4202/4328 [1:01:19<01:50,  1.14it/s] 97%|█████████▋| 4203/4328 [1:01:20<01:49,  1.14it/s] 97%|█████████▋| 4204/4328 [1:01:21<01:48,  1.14it/s] 97%|█████████▋| 4205/4328 [1:01:22<01:47,  1.14it/s] 97%|█████████▋| 4206/4328 [1:01:23<01:47,  1.13it/s] 97%|█████████▋| 4207/4328 [1:01:24<01:46,  1.14it/s] 97%|█████████▋| 4208/4328 [1:01:24<01:45,  1.14it/s] 97%|█████████▋| 4209/4328 [1:01:25<01:44,  1.14it/s] 97%|█████████▋| 4210/4328 [1:01:26<01:43,  1.14it/s] 97%|█████████▋| 4211/4328 [1:01:27<01:42,  1.14it/s] 97%|█████████▋| 4212/4328 [1:01:28<01:41,  1.14it/s] 97%|█████████▋| 4213/4328 [1:01:29<01:40,  1.14it/s] 97%|█████████▋| 4214/4328 [1:01:30<01:39,  1.14it/s] 97%|█████████▋| 4215/4328 [1:01:31<01:38,  1.15it/s] 97%|█████████▋| 4216/4328 [1:01:31<01:37,  1.15it/s] 97%|█████████▋| 4217/4328 [1:01:32<01:37,  1.14it/s] 97%|█████████▋| 4218/4328 [1:01:33<01:36,  1.14it/s] 97%|█████████▋| 4219/4328 [1:01:34<01:35,  1.14it/s] 98%|█████████▊| 4220/4328 [1:01:35<01:34,  1.14it/s] 98%|█████████▊| 4221/4328 [1:01:36<01:33,  1.14it/s] 98%|█████████▊| 4222/4328 [1:01:37<01:32,  1.14it/s] 98%|█████████▊| 4223/4328 [1:01:38<01:31,  1.14it/s] 98%|█████████▊| 4224/4328 [1:01:38<01:30,  1.14it/s] 98%|█████████▊| 4225/4328 [1:01:39<01:30,  1.14it/s] 98%|█████████▊| 4226/4328 [1:01:40<01:29,  1.14it/s] 98%|█████████▊| 4227/4328 [1:01:41<01:28,  1.14it/s] 98%|█████████▊| 4228/4328 [1:01:42<01:27,  1.14it/s] 98%|█████████▊| 4229/4328 [1:01:43<01:26,  1.14it/s] 98%|█████████▊| 4230/4328 [1:01:44<01:25,  1.15it/s] 98%|█████████▊| 4231/4328 [1:01:45<01:25,  1.14it/s] 98%|█████████▊| 4232/4328 [1:01:45<01:24,  1.14it/s] 98%|█████████▊| 4233/4328 [1:01:46<01:23,  1.14it/s] 98%|█████████▊| 4234/4328 [1:01:47<01:22,  1.14it/s] 98%|█████████▊| 4235/4328 [1:01:48<01:21,  1.14it/s] 98%|█████████▊| 4236/4328 [1:01:49<01:20,  1.14it/s] 98%|█████████▊| 4237/4328 [1:01:50<01:19,  1.14it/s] 98%|█████████▊| 4238/4328 [1:01:51<01:18,  1.15it/s] 98%|█████████▊| 4239/4328 [1:01:52<01:17,  1.14it/s] 98%|█████████▊| 4240/4328 [1:01:52<01:17,  1.14it/s] 98%|█████████▊| 4241/4328 [1:01:53<01:16,  1.14it/s] 98%|█████████▊| 4242/4328 [1:01:54<01:15,  1.14it/s] 98%|█████████▊| 4243/4328 [1:01:55<01:14,  1.14it/s] 98%|█████████▊| 4244/4328 [1:01:56<01:13,  1.14it/s] 98%|█████████▊| 4245/4328 [1:01:57<01:13,  1.14it/s] 98%|█████████▊| 4246/4328 [1:01:58<01:12,  1.14it/s] 98%|█████████▊| 4247/4328 [1:01:59<01:11,  1.14it/s] 98%|█████████▊| 4248/4328 [1:01:59<01:10,  1.14it/s] 98%|█████████▊| 4249/4328 [1:02:00<01:09,  1.14it/s] 98%|█████████▊| 4250/4328 [1:02:01<01:08,  1.14it/s] 98%|█████████▊| 4251/4328 [1:02:02<01:07,  1.14it/s] 98%|█████████▊| 4252/4328 [1:02:03<01:06,  1.15it/s] 98%|█████████▊| 4253/4328 [1:02:04<01:05,  1.14it/s] 98%|█████████▊| 4254/4328 [1:02:05<01:05,  1.14it/s] 98%|█████████▊| 4255/4328 [1:02:06<01:04,  1.14it/s] 98%|█████████▊| 4256/4328 [1:02:06<01:03,  1.14it/s] 98%|█████████▊| 4257/4328 [1:02:07<01:02,  1.14it/s] 98%|█████████▊| 4258/4328 [1:02:08<01:01,  1.14it/s] 98%|█████████▊| 4259/4328 [1:02:09<01:00,  1.14it/s] 98%|█████████▊| 4260/4328 [1:02:10<00:59,  1.14it/s] 98%|█████████▊| 4261/4328 [1:02:11<00:58,  1.14it/s] 98%|█████████▊| 4262/4328 [1:02:12<00:57,  1.14it/s] 98%|█████████▊| 4263/4328 [1:02:13<00:56,  1.14it/s] 99%|█████████▊| 4264/4328 [1:02:13<00:55,  1.15it/s] 99%|█████████▊| 4265/4328 [1:02:14<00:54,  1.15it/s] 99%|█████████▊| 4266/4328 [1:02:15<00:54,  1.15it/s] 99%|█████████▊| 4267/4328 [1:02:16<00:53,  1.14it/s] 99%|█████████▊| 4268/4328 [1:02:17<00:52,  1.14it/s] 99%|█████████▊| 4269/4328 [1:02:18<00:51,  1.14it/s] 99%|█████████▊| 4270/4328 [1:02:19<00:50,  1.14it/s] 99%|█████████▊| 4271/4328 [1:02:20<00:49,  1.14it/s] 99%|█████████▊| 4272/4328 [1:02:20<00:48,  1.14it/s] 99%|█████████▊| 4273/4328 [1:02:21<00:48,  1.14it/s] 99%|█████████▉| 4274/4328 [1:02:22<00:47,  1.14it/s] 99%|█████████▉| 4275/4328 [1:02:23<00:46,  1.14it/s] 99%|█████████▉| 4276/4328 [1:02:24<00:45,  1.14it/s] 99%|█████████▉| 4277/4328 [1:02:25<00:44,  1.14it/s] 99%|█████████▉| 4278/4328 [1:02:26<00:43,  1.14it/s] 99%|█████████▉| 4279/4328 [1:02:27<00:43,  1.14it/s] 99%|█████████▉| 4280/4328 [1:02:27<00:42,  1.14it/s] 99%|█████████▉| 4281/4328 [1:02:28<00:41,  1.14it/s] 99%|█████████▉| 4282/4328 [1:02:29<00:40,  1.14it/s] 99%|█████████▉| 4283/4328 [1:02:30<00:39,  1.14it/s] 99%|█████████▉| 4284/4328 [1:02:31<00:38,  1.15it/s] 99%|█████████▉| 4285/4328 [1:02:32<00:37,  1.14it/s] 99%|█████████▉| 4286/4328 [1:02:33<00:36,  1.14it/s] 99%|█████████▉| 4287/4328 [1:02:34<00:35,  1.14it/s] 99%|█████████▉| 4288/4328 [1:02:34<00:35,  1.14it/s] 99%|█████████▉| 4289/4328 [1:02:35<00:34,  1.14it/s] 99%|█████████▉| 4290/4328 [1:02:36<00:33,  1.14it/s] 99%|█████████▉| 4291/4328 [1:02:37<00:32,  1.14it/s] 99%|█████████▉| 4292/4328 [1:02:38<00:31,  1.14it/s] 99%|█████████▉| 4293/4328 [1:02:39<00:30,  1.14it/s] 99%|█████████▉| 4294/4328 [1:02:40<00:29,  1.14it/s] 99%|█████████▉| 4295/4328 [1:02:41<00:28,  1.14it/s] 99%|█████████▉| 4296/4328 [1:02:41<00:28,  1.14it/s] 99%|█████████▉| 4297/4328 [1:02:42<00:27,  1.14it/s] 99%|█████████▉| 4298/4328 [1:02:43<00:26,  1.14it/s] 99%|█████████▉| 4299/4328 [1:02:44<00:25,  1.14it/s] 99%|█████████▉| 4300/4328 [1:02:45<00:24,  1.14it/s] 99%|█████████▉| 4301/4328 [1:02:46<00:23,  1.14it/s] 99%|█████████▉| 4302/4328 [1:02:47<00:22,  1.14it/s] 99%|█████████▉| 4303/4328 [1:02:48<00:21,  1.14it/s] 99%|█████████▉| 4304/4328 [1:02:48<00:21,  1.14it/s] 99%|█████████▉| 4305/4328 [1:02:49<00:20,  1.14it/s] 99%|█████████▉| 4306/4328 [1:02:50<00:19,  1.14it/s]100%|█████████▉| 4307/4328 [1:02:51<00:18,  1.14it/s]100%|█████████▉| 4308/4328 [1:02:52<00:17,  1.14it/s]100%|█████████▉| 4309/4328 [1:02:53<00:16,  1.14it/s]100%|█████████▉| 4310/4328 [1:02:54<00:15,  1.14it/s]100%|█████████▉| 4311/4328 [1:02:55<00:14,  1.15it/s]100%|█████████▉| 4312/4328 [1:02:55<00:14,  1.14it/s]100%|█████████▉| 4313/4328 [1:02:56<00:13,  1.14it/s]100%|█████████▉| 4314/4328 [1:02:57<00:12,  1.14it/s]100%|█████████▉| 4315/4328 [1:02:58<00:11,  1.14it/s]100%|█████████▉| 4316/4328 [1:02:59<00:10,  1.14it/s]100%|█████████▉| 4317/4328 [1:03:00<00:09,  1.14it/s]100%|█████████▉| 4318/4328 [1:03:01<00:08,  1.14it/s]100%|█████████▉| 4319/4328 [1:03:02<00:07,  1.14it/s]100%|█████████▉| 4320/4328 [1:03:02<00:07,  1.14it/s]100%|█████████▉| 4321/4328 [1:03:03<00:06,  1.14it/s]100%|█████████▉| 4322/4328 [1:03:04<00:05,  1.15it/s]100%|█████████▉| 4323/4328 [1:03:05<00:04,  1.14it/s]100%|█████████▉| 4324/4328 [1:03:06<00:03,  1.14it/s]100%|█████████▉| 4325/4328 [1:03:07<00:02,  1.14it/s]100%|█████████▉| 4326/4328 [1:03:08<00:01,  1.14it/s]100%|█████████▉| 4327/4328 [1:03:09<00:00,  1.14it/s]100%|██████████| 4328/4328 [1:03:09<00:00,  1.14it/s]100%|██████████| 4328/4328 [1:03:10<00:00,  1.14it/s]
Total: 4328
Accuracy: 0.8891
Precision: 0.0000
Recall: 0.0000
F1 Score: 0.0000
