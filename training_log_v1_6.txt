/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-31 18:03:21,651] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-31 18:03:42,033] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5: setting --include=localhost:0,1,2,3,4,5
[2026-01-31 18:03:45,105] [INFO] [runner.py:571:main] cmd = /data3/jisu/miniconda3/envs/mfm-new/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path liuhaotian/llava-v1.6-vicuna-7b --version v1 --data_path /data3/jisu/LLaVA/visa_llava_instruct.json --image_folder /data3/jisu/MFM/datasets/ViSA --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 False --fp16 True --output_dir ./checkpoints/llava-v1.6-vicuna-7b-mfm-lora --num_train_epochs 3 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 False --model_max_length 1024 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to none
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-31 18:03:50,890] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-31 18:04:03,814] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2026-01-31 18:04:03,815] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2026-01-31 18:04:03,815] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2026-01-31 18:04:03,815] [INFO] [launch.py:163:main] dist_world_size=6
[2026-01-31 18:04:03,815] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-31 18:04:12,099] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-31 18:04:12,117] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-31 18:04:12,127] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-31 18:04:12,129] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-31 18:04:12,141] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-31 18:04:12,153] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-31 18:05:07,336] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-31 18:05:07,336] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2026-01-31 18:05:07,565] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-31 18:05:07,567] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-31 18:05:07,569] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-31 18:05:07,574] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-31 18:05:07,589] [INFO] [comm.py:637:init_distributed] cdb=None
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2026-01-31 18:07:00,623] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 687, num_elems = 7.06B
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:32<01:05, 32.89s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:32<01:05, 32.89s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:32<01:05, 32.89s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:32<01:05, 32.89s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:32<01:05, 32.90s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [01:30<03:00, 90.32s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:00<01:05, 65.14s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:00<01:05, 65.14s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:00<01:05, 65.14s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:00<01:05, 65.14s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:00<01:05, 65.14s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:55<01:27, 87.36s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:58<00:00, 89.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:58<00:00, 89.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:58<00:00, 79.39s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [03:58<00:00, 79.39s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [03:58<00:00, 89.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:58<00:00, 79.39s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [03:58<00:00, 89.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:58<00:00, 79.39s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [03:58<00:00, 89.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:58<00:00, 79.39s/it]
[META ALL] 0 tensors are on meta (params+buffers)
model class: LlavaLlamaForCausalLM
model name_or_path: liuhaotian/llava-v1.6-vicuna-7b
[META ALL] 0 tensors are on meta (params+buffers)
model class: LlavaLlamaForCausalLM
model name_or_path: liuhaotian/llava-v1.6-vicuna-7b
trainable param groups: 296
top trainables: [('model.image_newline', 0), ('model.embed_tokens.weight', 0), ('model.layers.0.self_attn.q_proj.weight', 0), ('model.layers.0.self_attn.k_proj.weight', 0), ('model.layers.0.self_attn.v_proj.weight', 0), ('model.layers.0.self_attn.o_proj.weight', 0), ('model.layers.0.mlp.gate_proj.weight', 0), ('model.layers.0.mlp.up_proj.weight', 0), ('model.layers.0.mlp.down_proj.weight', 0), ('model.layers.0.input_layernorm.weight', 0), ('model.layers.0.post_attention_layernorm.weight', 0), ('model.layers.1.self_attn.q_proj.weight', 0), ('model.layers.1.self_attn.k_proj.weight', 0), ('model.layers.1.self_attn.v_proj.weight', 0), ('model.layers.1.self_attn.o_proj.weight', 0), ('model.layers.1.mlp.gate_proj.weight', 0), ('model.layers.1.mlp.up_proj.weight', 0), ('model.layers.1.mlp.down_proj.weight', 0), ('model.layers.1.input_layernorm.weight', 0), ('model.layers.1.post_attention_layernorm.weight', 0)]
total trainable params: 0
trainable param groups: 296
top trainables: [('model.image_newline', 0), ('model.embed_tokens.weight', 0), ('model.layers.0.self_attn.q_proj.weight', 0), ('model.layers.0.self_attn.k_proj.weight', 0), ('model.layers.0.self_attn.v_proj.weight', 0), ('model.layers.0.self_attn.o_proj.weight', 0), ('model.layers.0.mlp.gate_proj.weight', 0), ('model.layers.0.mlp.up_proj.weight', 0), ('model.layers.0.mlp.down_proj.weight', 0), ('model.layers.0.input_layernorm.weight', 0), ('model.layers.0.post_attention_layernorm.weight', 0), ('model.layers.1.self_attn.q_proj.weight', 0), ('model.layers.1.self_attn.k_proj.weight', 0), ('model.layers.1.self_attn.v_proj.weight', 0), ('model.layers.1.self_attn.o_proj.weight', 0), ('model.layers.1.mlp.gate_proj.weight', 0), ('model.layers.1.mlp.up_proj.weight', 0), ('model.layers.1.mlp.down_proj.weight', 0), ('model.layers.1.input_layernorm.weight', 0), ('model.layers.1.post_attention_layernorm.weight', 0)]
total trainable params: 0
[META ALL] 0 tensors are on meta (params+buffers)
model class: LlavaLlamaForCausalLM
model name_or_path: liuhaotian/llava-v1.6-vicuna-7b
trainable param groups: 296
top trainables: [('model.image_newline', 0), ('model.embed_tokens.weight', 0), ('model.layers.0.self_attn.q_proj.weight', 0), ('model.layers.0.self_attn.k_proj.weight', 0), ('model.layers.0.self_attn.v_proj.weight', 0), ('model.layers.0.self_attn.o_proj.weight', 0), ('model.layers.0.mlp.gate_proj.weight', 0), ('model.layers.0.mlp.up_proj.weight', 0), ('model.layers.0.mlp.down_proj.weight', 0), ('model.layers.0.input_layernorm.weight', 0), ('model.layers.0.post_attention_layernorm.weight', 0), ('model.layers.1.self_attn.q_proj.weight', 0), ('model.layers.1.self_attn.k_proj.weight', 0), ('model.layers.1.self_attn.v_proj.weight', 0), ('model.layers.1.self_attn.o_proj.weight', 0), ('model.layers.1.mlp.gate_proj.weight', 0), ('model.layers.1.mlp.up_proj.weight', 0), ('model.layers.1.mlp.down_proj.weight', 0), ('model.layers.1.input_layernorm.weight', 0), ('model.layers.1.post_attention_layernorm.weight', 0)]
total trainable params: 0
[META ALL] 0 tensors are on meta (params+buffers)
model class: LlavaLlamaForCausalLM
model name_or_path: liuhaotian/llava-v1.6-vicuna-7b
trainable param groups: 296
top trainables: [('model.image_newline', 0), ('model.embed_tokens.weight', 0), ('model.layers.0.self_attn.q_proj.weight', 0), ('model.layers.0.self_attn.k_proj.weight', 0), ('model.layers.0.self_attn.v_proj.weight', 0), ('model.layers.0.self_attn.o_proj.weight', 0), ('model.layers.0.mlp.gate_proj.weight', 0), ('model.layers.0.mlp.up_proj.weight', 0), ('model.layers.0.mlp.down_proj.weight', 0), ('model.layers.0.input_layernorm.weight', 0), ('model.layers.0.post_attention_layernorm.weight', 0), ('model.layers.1.self_attn.q_proj.weight', 0), ('model.layers.1.self_attn.k_proj.weight', 0), ('model.layers.1.self_attn.v_proj.weight', 0), ('model.layers.1.self_attn.o_proj.weight', 0), ('model.layers.1.mlp.gate_proj.weight', 0), ('model.layers.1.mlp.up_proj.weight', 0), ('model.layers.1.mlp.down_proj.weight', 0), ('model.layers.1.input_layernorm.weight', 0), ('model.layers.1.post_attention_layernorm.weight', 0)]
total trainable params: 0
[META ALL] 0 tensors are on meta (params+buffers)
model class: LlavaLlamaForCausalLM
model name_or_path: liuhaotian/llava-v1.6-vicuna-7b
trainable param groups: 296
top trainables: [('model.image_newline', 0), ('model.embed_tokens.weight', 0), ('model.layers.0.self_attn.q_proj.weight', 0), ('model.layers.0.self_attn.k_proj.weight', 0), ('model.layers.0.self_attn.v_proj.weight', 0), ('model.layers.0.self_attn.o_proj.weight', 0), ('model.layers.0.mlp.gate_proj.weight', 0), ('model.layers.0.mlp.up_proj.weight', 0), ('model.layers.0.mlp.down_proj.weight', 0), ('model.layers.0.input_layernorm.weight', 0), ('model.layers.0.post_attention_layernorm.weight', 0), ('model.layers.1.self_attn.q_proj.weight', 0), ('model.layers.1.self_attn.k_proj.weight', 0), ('model.layers.1.self_attn.v_proj.weight', 0), ('model.layers.1.self_attn.o_proj.weight', 0), ('model.layers.1.mlp.gate_proj.weight', 0), ('model.layers.1.mlp.up_proj.weight', 0), ('model.layers.1.mlp.down_proj.weight', 0), ('model.layers.1.input_layernorm.weight', 0), ('model.layers.1.post_attention_layernorm.weight', 0)]
total trainable params: 0
Loading checkpoint shards: 100%|██████████| 3/3 [04:04<00:00, 78.86s/it]Loading checkpoint shards: 100%|██████████| 3/3 [04:04<00:00, 81.45s/it]
[META ALL] 0 tensors are on meta (params+buffers)
model class: LlavaLlamaForCausalLM
model name_or_path: liuhaotian/llava-v1.6-vicuna-7b
trainable param groups: 296
top trainables: [('model.image_newline', 0), ('model.embed_tokens.weight', 0), ('model.layers.0.self_attn.q_proj.weight', 0), ('model.layers.0.self_attn.k_proj.weight', 0), ('model.layers.0.self_attn.v_proj.weight', 0), ('model.layers.0.self_attn.o_proj.weight', 0), ('model.layers.0.mlp.gate_proj.weight', 0), ('model.layers.0.mlp.up_proj.weight', 0), ('model.layers.0.mlp.down_proj.weight', 0), ('model.layers.0.input_layernorm.weight', 0), ('model.layers.0.post_attention_layernorm.weight', 0), ('model.layers.1.self_attn.q_proj.weight', 0), ('model.layers.1.self_attn.k_proj.weight', 0), ('model.layers.1.self_attn.v_proj.weight', 0), ('model.layers.1.self_attn.o_proj.weight', 0), ('model.layers.1.mlp.gate_proj.weight', 0), ('model.layers.1.mlp.up_proj.weight', 0), ('model.layers.1.mlp.down_proj.weight', 0), ('model.layers.1.input_layernorm.weight', 0), ('model.layers.1.post_attention_layernorm.weight', 0)]
total trainable params: 0
Adding LoRA adapters...
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
Formatting inputs...Skip in lazy mode
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Parameter Offload: Total persistent parameters: 603136 in 313 params
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
  0%|          | 0/336 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[2026-01-31 18:15:12,308] [WARNING] [stage3.py:1991:step] 32 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|          | 1/336 [02:59<16:43:37, 179.75s/it]                                                   {'loss': 3.4042, 'learning_rate': 1.8181818181818182e-05, 'epoch': 0.01}
  0%|          | 1/336 [02:59<16:43:37, 179.75s/it][2026-01-31 18:18:24,527] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 2/336 [06:11<17:21:26, 187.08s/it]                                                   {'loss': 3.3698, 'learning_rate': 3.6363636363636364e-05, 'epoch': 0.02}
  1%|          | 2/336 [06:11<17:21:26, 187.08s/it][2026-01-31 18:21:38,430] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 3/336 [09:25<17:35:36, 190.20s/it]                                                   {'loss': 2.3686, 'learning_rate': 5.4545454545454546e-05, 'epoch': 0.03}
  1%|          | 3/336 [09:25<17:35:36, 190.20s/it][2026-01-31 18:24:47,965] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 4/336 [12:35<17:30:58, 189.94s/it]                                                   {'loss': 0.9578, 'learning_rate': 7.272727272727273e-05, 'epoch': 0.04}
  1%|          | 4/336 [12:35<17:30:58, 189.94s/it][2026-01-31 18:28:00,683] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|▏         | 5/336 [15:48<17:33:20, 190.94s/it]                                                   {'loss': 0.3862, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.04}
  1%|▏         | 5/336 [15:48<17:33:20, 190.94s/it][2026-01-31 18:31:14,369] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 6/336 [19:01<17:35:18, 191.87s/it]                                                   {'loss': 0.3068, 'learning_rate': 0.00010909090909090909, 'epoch': 0.05}
  2%|▏         | 6/336 [19:01<17:35:18, 191.87s/it][2026-01-31 18:34:25,669] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 7/336 [22:13<17:31:04, 191.69s/it]                                                   {'loss': 0.2803, 'learning_rate': 0.00012727272727272728, 'epoch': 0.06}
  2%|▏         | 7/336 [22:13<17:31:04, 191.69s/it][2026-01-31 18:37:37,232] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 8/336 [25:24<17:27:40, 191.65s/it]                                                   {'loss': 0.1934, 'learning_rate': 0.00014545454545454546, 'epoch': 0.07}
  2%|▏         | 8/336 [25:24<17:27:40, 191.65s/it][2026-01-31 18:40:49,831] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 9/336 [28:37<17:26:05, 191.94s/it]                                                   {'loss': 0.2632, 'learning_rate': 0.00016363636363636366, 'epoch': 0.08}
  3%|▎         | 9/336 [28:37<17:26:05, 191.94s/it][2026-01-31 18:44:02,009] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 10/336 [31:49<17:23:17, 192.02s/it]                                                    {'loss': 0.2144, 'learning_rate': 0.00018181818181818183, 'epoch': 0.09}
  3%|▎         | 10/336 [31:49<17:23:17, 192.02s/it][2026-01-31 18:47:12,909] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 11/336 [35:00<17:18:14, 191.67s/it]                                                    {'loss': 0.2128, 'learning_rate': 0.0002, 'epoch': 0.1}
  3%|▎         | 11/336 [35:00<17:18:14, 191.67s/it][2026-01-31 18:50:25,817] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▎         | 12/336 [38:13<17:17:04, 192.05s/it]                                                    {'loss': 0.1202, 'learning_rate': 0.0001999953280342959, 'epoch': 0.11}
  4%|▎         | 12/336 [38:13<17:17:04, 192.05s/it][2026-01-31 18:53:37,967] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 13/336 [41:25<17:14:01, 192.08s/it]                                                    {'loss': 0.1359, 'learning_rate': 0.00019998131257372876, 'epoch': 0.12}
  4%|▍         | 13/336 [41:25<17:14:01, 192.08s/it][2026-01-31 18:56:51,127] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 14/336 [44:38<17:12:34, 192.41s/it]                                                    {'loss': 0.1391, 'learning_rate': 0.0001999579549278937, 'epoch': 0.12}
  4%|▍         | 14/336 [44:38<17:12:34, 192.41s/it][2026-01-31 19:00:02,155] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 15/336 [47:49<17:07:09, 191.99s/it]                                                    {'loss': 0.1677, 'learning_rate': 0.00019992525727931303, 'epoch': 0.13}
  4%|▍         | 15/336 [47:49<17:07:09, 191.99s/it][2026-01-31 19:03:15,900] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▍         | 16/336 [51:03<17:06:46, 192.52s/it]                                                    {'loss': 0.1508, 'learning_rate': 0.00019988322268323268, 'epoch': 0.14}
  5%|▍         | 16/336 [51:03<17:06:46, 192.52s/it][2026-01-31 19:06:27,077] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▌         | 17/336 [54:14<17:01:24, 192.12s/it]                                                    {'loss': 0.1013, 'learning_rate': 0.0001998318550673364, 'epoch': 0.15}
  5%|▌         | 17/336 [54:14<17:01:24, 192.12s/it][2026-01-31 19:09:38,197] [WARNING] [stage3.py:1991:step] 45 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▌         | 18/336 [57:25<16:56:37, 191.82s/it]                                                    {'loss': 0.0815, 'learning_rate': 0.00019977115923137912, 'epoch': 0.16}
  5%|▌         | 18/336 [57:25<16:56:37, 191.82s/it][2026-01-31 19:12:50,838] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 19/336 [1:00:38<16:54:44, 192.06s/it]                                                      {'loss': 0.0974, 'learning_rate': 0.00019970114084673796, 'epoch': 0.17}
  6%|▌         | 19/336 [1:00:38<16:54:44, 192.06s/it][2026-01-31 19:16:02,458] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 20/336 [1:03:49<16:50:50, 191.93s/it]                                                      {'loss': 0.0725, 'learning_rate': 0.0001996218064558829, 'epoch': 0.18}
  6%|▌         | 20/336 [1:03:49<16:50:50, 191.93s/it][2026-01-31 19:19:16,185] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 21/336 [1:07:03<16:50:28, 192.47s/it]                                                      {'loss': 0.0751, 'learning_rate': 0.00019953316347176488, 'epoch': 0.19}
  6%|▋         | 21/336 [1:07:03<16:50:28, 192.47s/it][2026-01-31 19:22:27,704] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 22/336 [1:10:15<16:45:45, 192.18s/it]                                                      {'loss': 0.0732, 'learning_rate': 0.00019943522017712358, 'epoch': 0.2}
  7%|▋         | 22/336 [1:10:15<16:45:45, 192.18s/it][2026-01-31 19:25:40,128] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 23/336 [1:13:27<16:42:56, 192.26s/it]                                                      {'loss': 0.0994, 'learning_rate': 0.0001993279857237133, 'epoch': 0.2}
  7%|▋         | 23/336 [1:13:27<16:42:56, 192.26s/it][2026-01-31 19:28:50,423] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 24/336 [1:16:37<16:36:40, 191.67s/it]                                                      {'loss': 0.0582, 'learning_rate': 0.0001992114701314478, 'epoch': 0.21}
  7%|▋         | 24/336 [1:16:37<16:36:40, 191.67s/it][2026-01-31 19:32:04,186] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 25/336 [1:19:51<16:36:44, 192.30s/it]                                                      {'loss': 0.0495, 'learning_rate': 0.0001990856842874641, 'epoch': 0.22}
  7%|▋         | 25/336 [1:19:51<16:36:44, 192.30s/it][2026-01-31 19:35:15,846] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 26/336 [1:23:03<16:32:32, 192.11s/it]                                                      {'loss': 0.0474, 'learning_rate': 0.0001989506399451051, 'epoch': 0.23}
  8%|▊         | 26/336 [1:23:03<16:32:32, 192.11s/it][2026-01-31 19:38:38,512] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 27/336 [1:26:25<16:45:39, 195.27s/it]                                                      {'loss': 0.048, 'learning_rate': 0.00019880634972282166, 'epoch': 0.24}
  8%|▊         | 27/336 [1:26:25<16:45:39, 195.27s/it][2026-01-31 19:41:51,671] [WARNING] [stage3.py:1991:step] 43 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 28/336 [1:29:39<16:39:08, 194.64s/it]                                                      {'loss': 0.0755, 'learning_rate': 0.0001986528271029931, 'epoch': 0.25}
  8%|▊         | 28/336 [1:29:39<16:39:08, 194.64s/it][2026-01-31 19:45:03,075] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▊         | 29/336 [1:32:50<16:30:56, 193.67s/it]                                                      {'loss': 0.0543, 'learning_rate': 0.00019849008643066772, 'epoch': 0.26}
  9%|▊         | 29/336 [1:32:50<16:30:56, 193.67s/it][2026-01-31 19:48:14,976] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 30/336 [1:36:02<16:25:00, 193.14s/it]                                                      {'loss': 0.0681, 'learning_rate': 0.00019831814291222232, 'epoch': 0.27}
  9%|▉         | 30/336 [1:36:02<16:25:00, 193.14s/it][2026-01-31 19:51:27,766] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 31/336 [1:39:15<16:21:15, 193.03s/it]                                                      {'loss': 0.0759, 'learning_rate': 0.00019813701261394136, 'epoch': 0.27}
  9%|▉         | 31/336 [1:39:15<16:21:15, 193.03s/it][2026-01-31 19:54:39,514] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|▉         | 32/336 [1:42:26<16:16:05, 192.65s/it]                                                      {'loss': 0.0564, 'learning_rate': 0.0001979467124605156, 'epoch': 0.28}
 10%|▉         | 32/336 [1:42:26<16:16:05, 192.65s/it][2026-01-31 19:57:51,313] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|▉         | 33/336 [1:45:38<16:11:35, 192.39s/it]                                                      {'loss': 0.0736, 'learning_rate': 0.0001977472602334609, 'epoch': 0.29}
 10%|▉         | 33/336 [1:45:38<16:11:35, 192.39s/it][2026-01-31 20:01:03,552] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|█         | 34/336 [1:48:51<16:08:11, 192.36s/it]                                                      {'loss': 0.0411, 'learning_rate': 0.0001975386745694565, 'epoch': 0.3}
 10%|█         | 34/336 [1:48:51<16:08:11, 192.36s/it][2026-01-31 20:04:16,616] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|█         | 35/336 [1:52:04<16:06:00, 192.56s/it]                                                      {'loss': 0.0667, 'learning_rate': 0.00019732097495860386, 'epoch': 0.31}
 10%|█         | 35/336 [1:52:04<16:06:00, 192.56s/it][2026-01-31 20:07:27,357] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 36/336 [1:55:14<16:00:04, 192.01s/it]                                                      {'loss': 0.0573, 'learning_rate': 0.0001970941817426052, 'epoch': 0.32}
 11%|█         | 36/336 [1:55:14<16:00:04, 192.01s/it][2026-01-31 20:10:41,219] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 37/336 [1:58:28<15:59:37, 192.57s/it]                                                      {'loss': 0.0419, 'learning_rate': 0.0001968583161128631, 'epoch': 0.33}
 11%|█         | 37/336 [1:58:28<15:59:37, 192.57s/it][2026-01-31 20:13:52,450] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█▏        | 38/336 [2:01:39<15:54:25, 192.17s/it]                                                      {'loss': 0.07, 'learning_rate': 0.00019661340010850026, 'epoch': 0.34}
 11%|█▏        | 38/336 [2:01:39<15:54:25, 192.17s/it][2026-01-31 20:17:05,094] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 39/336 [2:04:52<15:51:56, 192.31s/it]                                                      {'loss': 0.0524, 'learning_rate': 0.00019635945661430006, 'epoch': 0.35}
 12%|█▏        | 39/336 [2:04:52<15:51:56, 192.31s/it][2026-01-31 20:20:16,660] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 40/336 [2:08:04<15:47:37, 192.09s/it]                                                      {'loss': 0.0684, 'learning_rate': 0.00019609650935856844, 'epoch': 0.35}
 12%|█▏        | 40/336 [2:08:04<15:47:37, 192.09s/it][2026-01-31 20:23:30,267] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 41/336 [2:11:17<15:46:40, 192.54s/it]                                                      {'loss': 0.0624, 'learning_rate': 0.00019582458291091663, 'epoch': 0.36}
 12%|█▏        | 41/336 [2:11:17<15:46:40, 192.54s/it][2026-01-31 20:26:52,083] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▎        | 42/336 [2:14:39<15:57:05, 195.32s/it]                                                      {'loss': 0.0686, 'learning_rate': 0.00019554370267996538, 'epoch': 0.37}
 12%|█▎        | 42/336 [2:14:39<15:57:05, 195.32s/it][2026-01-31 20:30:17,337] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 43/336 [2:18:04<16:08:22, 198.30s/it]                                                      {'loss': 0.0531, 'learning_rate': 0.0001952538949109708, 'epoch': 0.38}
 13%|█▎        | 43/336 [2:18:04<16:08:22, 198.30s/it][2026-01-31 20:33:29,753] [WARNING] [stage3.py:1991:step] 45 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 44/336 [2:21:17<15:56:28, 196.54s/it]                                                      {'loss': 0.0525, 'learning_rate': 0.00019495518668337201, 'epoch': 0.39}
 13%|█▎        | 44/336 [2:21:17<15:56:28, 196.54s/it][2026-01-31 20:36:41,704] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 45/336 [2:24:29<15:46:32, 195.16s/it]                                                      {'loss': 0.0285, 'learning_rate': 0.00019464760590826098, 'epoch': 0.4}
 13%|█▎        | 45/336 [2:24:29<15:46:32, 195.16s/it][2026-01-31 20:39:53,994] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▎        | 46/336 [2:27:41<15:39:06, 194.30s/it]                                                      {'loss': 0.0563, 'learning_rate': 0.0001943311813257743, 'epoch': 0.41}
 14%|█▎        | 46/336 [2:27:41<15:39:06, 194.30s/it][2026-01-31 20:43:05,698] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 47/336 [2:30:53<15:32:07, 193.52s/it]                                                      {'loss': 0.0448, 'learning_rate': 0.00019400594250240798, 'epoch': 0.42}
 14%|█▍        | 47/336 [2:30:53<15:32:07, 193.52s/it][2026-01-31 20:46:16,481] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 48/336 [2:34:03<15:24:57, 192.70s/it]                                                      {'loss': 0.0403, 'learning_rate': 0.0001936719198282545, 'epoch': 0.43}
 14%|█▍        | 48/336 [2:34:03<15:24:57, 192.70s/it][2026-01-31 20:49:29,560] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▍        | 49/336 [2:37:17<15:22:17, 192.81s/it]                                                      {'loss': 0.1145, 'learning_rate': 0.00019332914451416347, 'epoch': 0.43}
 15%|█▍        | 49/336 [2:37:17<15:22:17, 192.81s/it][2026-01-31 20:52:41,131] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▍        | 50/336 [2:40:28<15:17:18, 192.44s/it]                                                      {'loss': 0.0487, 'learning_rate': 0.00019297764858882514, 'epoch': 0.44}
 15%|█▍        | 50/336 [2:40:28<15:17:18, 192.44s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[2026-01-31 20:56:17,407] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▌        | 51/336 [2:44:04<15:48:03, 199.59s/it]                                                      {'loss': 0.0551, 'learning_rate': 0.00019261746489577765, 'epoch': 0.45}
 15%|█▌        | 51/336 [2:44:04<15:48:03, 199.59s/it][2026-01-31 20:59:30,801] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▌        | 52/336 [2:47:18<15:35:56, 197.73s/it]                                                      {'loss': 0.0635, 'learning_rate': 0.00019224862709033824, 'epoch': 0.46}
 15%|█▌        | 52/336 [2:47:18<15:35:56, 197.73s/it][2026-01-31 21:02:41,804] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 53/336 [2:50:29<15:23:06, 195.71s/it]                                                      {'loss': 0.0574, 'learning_rate': 0.00019187116963645842, 'epoch': 0.47}
 16%|█▌        | 53/336 [2:50:29<15:23:06, 195.71s/it][2026-01-31 21:05:53,684] [WARNING] [stage3.py:1991:step] 45 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 54/336 [2:53:41<15:14:26, 194.56s/it]                                                      {'loss': 0.0404, 'learning_rate': 0.00019148512780350384, 'epoch': 0.48}
 16%|█▌        | 54/336 [2:53:41<15:14:26, 194.56s/it][2026-01-31 21:09:07,080] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▋        | 55/336 [2:56:54<15:09:33, 194.21s/it]                                                      {'loss': 0.0396, 'learning_rate': 0.0001910905376629585, 'epoch': 0.49}
 16%|█▋        | 55/336 [2:56:54<15:09:33, 194.21s/it][2026-01-31 21:12:19,953] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 56/336 [3:00:07<15:04:27, 193.81s/it]                                                      {'loss': 0.0329, 'learning_rate': 0.00019068743608505455, 'epoch': 0.5}
 17%|█▋        | 56/336 [3:00:07<15:04:27, 193.81s/it][2026-01-31 21:15:30,123] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 57/336 [3:03:17<14:56:08, 192.72s/it]                                                      {'loss': 0.0499, 'learning_rate': 0.0001902758607353269, 'epoch': 0.51}
 17%|█▋        | 57/336 [3:03:17<14:56:08, 192.72s/it][2026-01-31 21:18:42,688] [WARNING] [stage3.py:1991:step] 52 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 58/336 [3:06:30<14:52:42, 192.67s/it]                                                      {'loss': 0.0589, 'learning_rate': 0.0001898558500710939, 'epoch': 0.51}
 17%|█▋        | 58/336 [3:06:30<14:52:42, 192.67s/it][2026-01-31 21:21:56,096] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 59/336 [3:09:43<14:50:31, 192.89s/it]                                                      {'loss': 0.0275, 'learning_rate': 0.00018942744333786397, 'epoch': 0.52}
 18%|█▊        | 59/336 [3:09:43<14:50:31, 192.89s/it][2026-01-31 21:25:07,972] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 60/336 [3:12:55<14:45:54, 192.59s/it]                                                      {'loss': 0.0599, 'learning_rate': 0.0001889906805656684, 'epoch': 0.53}
 18%|█▊        | 60/336 [3:12:55<14:45:54, 192.59s/it][2026-01-31 21:28:30,902] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 61/336 [3:16:18<14:56:54, 195.69s/it]                                                      {'loss': 0.036, 'learning_rate': 0.000188545602565321, 'epoch': 0.54}
 18%|█▊        | 61/336 [3:16:18<14:56:54, 195.69s/it][2026-01-31 21:31:44,937] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 62/336 [3:19:32<14:51:23, 195.19s/it]                                                      {'loss': 0.0444, 'learning_rate': 0.00018809225092460488, 'epoch': 0.55}
 18%|█▊        | 62/336 [3:19:32<14:51:23, 195.19s/it][2026-01-31 21:34:57,094] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 63/336 [3:22:44<14:43:59, 194.28s/it]                                                      {'loss': 0.0359, 'learning_rate': 0.00018763066800438636, 'epoch': 0.56}
 19%|█▉        | 63/336 [3:22:44<14:43:59, 194.28s/it][2026-01-31 21:38:09,704] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 64/336 [3:25:57<14:38:28, 193.78s/it]                                                      {'loss': 0.0513, 'learning_rate': 0.00018716089693465696, 'epoch': 0.57}
 19%|█▉        | 64/336 [3:25:57<14:38:28, 193.78s/it][2026-01-31 21:41:21,413] [WARNING] [stage3.py:1991:step] 45 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 65/336 [3:29:08<14:32:26, 193.16s/it]                                                      {'loss': 0.0442, 'learning_rate': 0.00018668298161050309, 'epoch': 0.58}
 19%|█▉        | 65/336 [3:29:08<14:32:26, 193.16s/it][2026-01-31 21:44:34,660] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|█▉        | 66/336 [3:32:22<14:29:20, 193.19s/it]                                                      {'loss': 0.0342, 'learning_rate': 0.00018619696668800492, 'epoch': 0.59}
 20%|█▉        | 66/336 [3:32:22<14:29:20, 193.19s/it][2026-01-31 21:47:46,034] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|█▉        | 67/336 [3:35:33<14:23:40, 192.64s/it]                                                      {'loss': 0.0436, 'learning_rate': 0.00018570289758006346, 'epoch': 0.59}
 20%|█▉        | 67/336 [3:35:33<14:23:40, 192.64s/it][2026-01-31 21:50:58,900] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|██        | 68/336 [3:38:46<14:20:46, 192.71s/it]                                                      {'loss': 0.0434, 'learning_rate': 0.0001852008204521572, 'epoch': 0.6}
 20%|██        | 68/336 [3:38:46<14:20:46, 192.71s/it][2026-01-31 21:54:09,850] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 69/336 [3:41:57<14:15:12, 192.18s/it]                                                      {'loss': 0.0357, 'learning_rate': 0.0001846907822180286, 'epoch': 0.61}
 21%|██        | 69/336 [3:41:57<14:15:12, 192.18s/it][2026-01-31 21:57:22,801] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 70/336 [3:45:10<14:13:01, 192.41s/it]                                                      {'loss': 0.0587, 'learning_rate': 0.00018417283053530044, 'epoch': 0.62}
 21%|██        | 70/336 [3:45:10<14:13:01, 192.41s/it][2026-01-31 22:00:35,732] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 71/336 [3:48:23<14:10:32, 192.58s/it]                                                      {'loss': 0.0462, 'learning_rate': 0.00018364701380102266, 'epoch': 0.63}
 21%|██        | 71/336 [3:48:23<14:10:32, 192.58s/it][2026-01-31 22:03:47,992] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██▏       | 72/336 [3:51:35<14:06:52, 192.47s/it]                                                      {'loss': 0.0231, 'learning_rate': 0.0001831133811471503, 'epoch': 0.64}
 21%|██▏       | 72/336 [3:51:35<14:06:52, 192.47s/it][2026-01-31 22:06:58,506] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 73/336 [3:54:45<14:01:05, 191.89s/it]                                                      {'loss': 0.0224, 'learning_rate': 0.0001825719824359524, 'epoch': 0.65}
 22%|██▏       | 73/336 [3:54:45<14:01:05, 191.89s/it][2026-01-31 22:10:12,456] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 74/336 [3:57:59<14:00:36, 192.51s/it]                                                      {'loss': 0.0288, 'learning_rate': 0.0001820228682553533, 'epoch': 0.66}
 22%|██▏       | 74/336 [3:57:59<14:00:36, 192.51s/it][2026-01-31 22:13:25,255] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 75/336 [4:01:12<13:57:46, 192.59s/it]                                                      {'loss': 0.0211, 'learning_rate': 0.00018146608991420534, 'epoch': 0.67}
 22%|██▏       | 75/336 [4:01:12<13:57:46, 192.59s/it][2026-01-31 22:16:35,168] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 76/336 [4:04:22<13:51:05, 191.79s/it]                                                      {'loss': 0.0352, 'learning_rate': 0.00018090169943749476, 'epoch': 0.67}
 23%|██▎       | 76/336 [4:04:22<13:51:05, 191.79s/it][2026-01-31 22:19:47,215] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 77/336 [4:07:34<13:48:13, 191.87s/it]                                                      {'loss': 0.03, 'learning_rate': 0.00018032974956148063, 'epoch': 0.68}
 23%|██▎       | 77/336 [4:07:34<13:48:13, 191.87s/it][2026-01-31 22:22:48,325] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 78/336 [4:10:35<13:31:09, 188.64s/it]                                                      {'loss': 0.0376, 'learning_rate': 0.00017975029372876706, 'epoch': 0.69}
 23%|██▎       | 78/336 [4:10:35<13:31:09, 188.64s/it][2026-01-31 22:25:59,992] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▎       | 79/336 [4:13:47<13:31:53, 189.55s/it]                                                      {'loss': 0.0368, 'learning_rate': 0.0001791633860833096, 'epoch': 0.7}
 24%|██▎       | 79/336 [4:13:47<13:31:53, 189.55s/it][2026-01-31 22:29:11,534] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 80/336 [4:16:58<13:31:17, 190.15s/it]                                                      {'loss': 0.0308, 'learning_rate': 0.00017856908146535603, 'epoch': 0.71}
 24%|██▍       | 80/336 [4:16:58<13:31:17, 190.15s/it][2026-01-31 22:32:25,508] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 81/336 [4:20:12<13:33:00, 191.29s/it]                                                      {'loss': 0.0442, 'learning_rate': 0.00017796743540632223, 'epoch': 0.72}
 24%|██▍       | 81/336 [4:20:12<13:33:00, 191.29s/it][2026-01-31 22:35:36,715] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 82/336 [4:23:24<13:29:42, 191.27s/it]                                                      {'loss': 0.0607, 'learning_rate': 0.00017735850412360331, 'epoch': 0.73}
 24%|██▍       | 82/336 [4:23:24<13:29:42, 191.27s/it][2026-01-31 22:38:48,899] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▍       | 83/336 [4:26:36<13:27:40, 191.54s/it]                                                      {'loss': 0.0489, 'learning_rate': 0.00017674234451532065, 'epoch': 0.74}
 25%|██▍       | 83/336 [4:26:36<13:27:40, 191.54s/it][2026-01-31 22:42:12,318] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▌       | 84/336 [4:29:59<13:39:26, 195.11s/it]                                                      {'loss': 0.0337, 'learning_rate': 0.00017611901415500535, 'epoch': 0.75}
 25%|██▌       | 84/336 [4:29:59<13:39:26, 195.11s/it][2026-01-31 22:45:23,367] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▌       | 85/336 [4:33:10<13:31:06, 193.89s/it]                                                      {'loss': 0.0339, 'learning_rate': 0.00017548857128621875, 'epoch': 0.75}
 25%|██▌       | 85/336 [4:33:10<13:31:06, 193.89s/it][2026-01-31 22:48:36,298] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 86/336 [4:36:23<13:26:40, 193.60s/it]                                                      {'loss': 0.0414, 'learning_rate': 0.00017485107481711012, 'epoch': 0.76}
 26%|██▌       | 86/336 [4:36:23<13:26:40, 193.60s/it][2026-01-31 22:51:48,805] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 87/336 [4:39:36<13:22:05, 193.27s/it]                                                      {'loss': 0.0319, 'learning_rate': 0.00017420658431491223, 'epoch': 0.77}
 26%|██▌       | 87/336 [4:39:36<13:22:05, 193.27s/it][2026-01-31 22:55:01,021] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 88/336 [4:42:48<13:17:33, 192.96s/it]                                                      {'loss': 0.0375, 'learning_rate': 0.00017355516000037554, 'epoch': 0.78}
 26%|██▌       | 88/336 [4:42:48<13:17:33, 192.96s/it][2026-01-31 22:58:22,158] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▋       | 89/336 [4:46:09<13:24:26, 195.41s/it]                                                      {'loss': 0.0259, 'learning_rate': 0.00017289686274214118, 'epoch': 0.79}
 26%|██▋       | 89/336 [4:46:09<13:24:26, 195.41s/it][2026-01-31 23:01:26,949] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 90/336 [4:49:14<13:08:09, 192.23s/it]                                                      {'loss': 0.0585, 'learning_rate': 0.0001722317540510534, 'epoch': 0.8}
 27%|██▋       | 90/336 [4:49:14<13:08:09, 192.23s/it][2026-01-31 23:04:49,072] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 91/336 [4:52:36<13:17:01, 195.19s/it]                                                      {'loss': 0.0575, 'learning_rate': 0.00017155989607441213, 'epoch': 0.81}
 27%|██▋       | 91/336 [4:52:36<13:17:01, 195.19s/it][2026-01-31 23:08:02,933] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 92/336 [4:55:50<13:12:09, 194.79s/it]                                                      {'loss': 0.0309, 'learning_rate': 0.00017088135159016584, 'epoch': 0.82}
 27%|██▋       | 92/336 [4:55:50<13:12:09, 194.79s/it][2026-01-31 23:11:13,354] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 93/336 [4:59:00<13:03:35, 193.48s/it]                                                      {'loss': 0.0386, 'learning_rate': 0.00017019618400104572, 'epoch': 0.82}
 28%|██▊       | 93/336 [4:59:00<13:03:35, 193.48s/it][2026-01-31 23:14:26,463] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 94/336 [5:02:13<12:59:55, 193.37s/it]                                                      {'loss': 0.0472, 'learning_rate': 0.00016950445732864127, 'epoch': 0.83}
 28%|██▊       | 94/336 [5:02:13<12:59:55, 193.37s/it][2026-01-31 23:17:38,646] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 95/336 [5:05:26<12:55:16, 193.01s/it]                                                      {'loss': 0.0149, 'learning_rate': 0.00016880623620741842, 'epoch': 0.84}
 28%|██▊       | 95/336 [5:05:26<12:55:16, 193.01s/it][2026-01-31 23:20:49,900] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▊       | 96/336 [5:08:37<12:49:56, 192.49s/it]                                                      {'loss': 0.048, 'learning_rate': 0.00016810158587867973, 'epoch': 0.85}
 29%|██▊       | 96/336 [5:08:37<12:49:56, 192.49s/it][2026-01-31 23:24:02,438] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 97/336 [5:11:49<12:46:47, 192.50s/it]                                                      {'loss': 0.0206, 'learning_rate': 0.0001673905721844686, 'epoch': 0.86}
 29%|██▉       | 97/336 [5:11:49<12:46:47, 192.50s/it][2026-01-31 23:27:24,893] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 98/336 [5:15:12<12:55:25, 195.49s/it]                                                      {'loss': 0.0248, 'learning_rate': 0.00016667326156141692, 'epoch': 0.87}
 29%|██▉       | 98/336 [5:15:12<12:55:25, 195.49s/it][2026-01-31 23:30:36,362] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 99/336 [5:18:23<12:47:24, 194.28s/it]                                                      {'loss': 0.0271, 'learning_rate': 0.00016594972103453726, 'epoch': 0.88}
 29%|██▉       | 99/336 [5:18:23<12:47:24, 194.28s/it][2026-01-31 23:33:49,525] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|██▉       | 100/336 [5:21:36<12:42:51, 193.95s/it]                                                       {'loss': 0.0187, 'learning_rate': 0.0001652200182109602, 'epoch': 0.89}
 30%|██▉       | 100/336 [5:21:36<12:42:51, 193.95s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[2026-01-31 23:37:37,912] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|███       | 101/336 [5:25:25<13:20:05, 204.28s/it]                                                       {'loss': 0.0409, 'learning_rate': 0.00016448422127361706, 'epoch': 0.9}
 30%|███       | 101/336 [5:25:25<13:20:05, 204.28s/it][2026-01-31 23:40:48,817] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|███       | 102/336 [5:28:36<13:01:02, 200.27s/it]                                                       {'loss': 0.0245, 'learning_rate': 0.000163742398974869, 'epoch': 0.9}
 30%|███       | 102/336 [5:28:36<13:01:02, 200.27s/it][2026-01-31 23:44:01,979] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 103/336 [5:31:49<12:49:25, 198.14s/it]                                                       {'loss': 0.0462, 'learning_rate': 0.00016299462063008272, 'epoch': 0.91}
 31%|███       | 103/336 [5:31:49<12:49:25, 198.14s/it][2026-01-31 23:47:12,632] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 104/336 [5:35:00<12:37:26, 195.89s/it]                                                       {'loss': 0.0439, 'learning_rate': 0.00016224095611115384, 'epoch': 0.92}
 31%|███       | 104/336 [5:35:00<12:37:26, 195.89s/it][2026-01-31 23:50:24,893] [WARNING] [stage3.py:1991:step] 45 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███▏      | 105/336 [5:38:12<12:29:59, 194.80s/it]                                                       {'loss': 0.0289, 'learning_rate': 0.00016148147583997812, 'epoch': 0.93}
 31%|███▏      | 105/336 [5:38:12<12:29:59, 194.80s/it][2026-01-31 23:53:38,667] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 106/336 [5:41:26<12:25:33, 194.49s/it]                                                       {'loss': 0.0246, 'learning_rate': 0.00016071625078187114, 'epoch': 0.94}
 32%|███▏      | 106/336 [5:41:26<12:25:33, 194.49s/it][2026-01-31 23:56:50,085] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 107/336 [5:44:37<12:18:47, 193.57s/it]                                                       {'loss': 0.0368, 'learning_rate': 0.0001599453524389374, 'epoch': 0.95}
 32%|███▏      | 107/336 [5:44:37<12:18:47, 193.57s/it][2026-02-01 00:00:01,588] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 108/336 [5:47:49<12:13:13, 192.95s/it]                                                       {'loss': 0.0334, 'learning_rate': 0.00015916885284338937, 'epoch': 0.96}
 32%|███▏      | 108/336 [5:47:49<12:13:13, 192.95s/it][2026-02-01 00:03:13,833] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 109/336 [5:51:01<12:09:11, 192.74s/it]                                                       {'loss': 0.0522, 'learning_rate': 0.00015838682455081657, 'epoch': 0.97}
 32%|███▏      | 109/336 [5:51:01<12:09:11, 192.74s/it][2026-02-01 00:06:27,575] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 33%|███▎      | 110/336 [5:54:15<12:07:06, 193.04s/it]                                                       {'loss': 0.0238, 'learning_rate': 0.00015759934063340627, 'epoch': 0.98}
 33%|███▎      | 110/336 [5:54:15<12:07:06, 193.04s/it][2026-02-01 00:09:38,846] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 33%|███▎      | 111/336 [5:57:26<12:01:54, 192.51s/it]                                                       {'loss': 0.0375, 'learning_rate': 0.00015680647467311557, 'epoch': 0.98}
 33%|███▎      | 111/336 [5:57:26<12:01:54, 192.51s/it][2026-02-01 00:12:50,935] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 33%|███▎      | 112/336 [6:00:38<11:58:13, 192.38s/it]                                                       {'loss': 0.0222, 'learning_rate': 0.00015600830075479603, 'epoch': 0.99}
 33%|███▎      | 112/336 [6:00:38<11:58:13, 192.38s/it][2026-02-01 00:16:04,119] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 34%|███▎      | 113/336 [6:03:51<11:55:54, 192.62s/it]                                                       {'loss': 0.0262, 'learning_rate': 0.00015520489345927096, 'epoch': 1.0}
 34%|███▎      | 113/336 [6:03:51<11:55:54, 192.62s/it][2026-02-01 00:19:14,963] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 34%|███▍      | 114/336 [6:07:02<11:50:43, 192.09s/it]                                                       {'loss': 0.0349, 'learning_rate': 0.00015439632785636706, 'epoch': 1.01}
 34%|███▍      | 114/336 [6:07:02<11:50:43, 192.09s/it][2026-02-01 00:22:27,591] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 34%|███▍      | 115/336 [6:10:15<11:48:07, 192.25s/it]                                                       {'loss': 0.0364, 'learning_rate': 0.00015358267949789966, 'epoch': 1.02}
 34%|███▍      | 115/336 [6:10:15<11:48:07, 192.25s/it][2026-02-01 00:25:39,921] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 35%|███▍      | 116/336 [6:13:27<11:45:00, 192.27s/it]                                                       {'loss': 0.0247, 'learning_rate': 0.0001527640244106133, 'epoch': 1.03}
 35%|███▍      | 116/336 [6:13:27<11:45:00, 192.27s/it][2026-02-01 00:28:51,587] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 35%|███▍      | 117/336 [6:16:39<11:41:08, 192.09s/it]                                                       {'loss': 0.0241, 'learning_rate': 0.00015194043908907775, 'epoch': 1.04}
 35%|███▍      | 117/336 [6:16:39<11:41:08, 192.09s/it][2026-02-01 00:32:03,320] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 35%|███▌      | 118/336 [6:19:50<11:37:32, 191.98s/it]                                                       {'loss': 0.037, 'learning_rate': 0.00015111200048854056, 'epoch': 1.05}
 35%|███▌      | 118/336 [6:19:50<11:37:32, 191.98s/it][2026-02-01 00:35:15,198] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 35%|███▌      | 119/336 [6:23:02<11:34:13, 191.95s/it]                                                       {'loss': 0.0081, 'learning_rate': 0.00015027878601773633, 'epoch': 1.06}
 35%|███▌      | 119/336 [6:23:02<11:34:13, 191.95s/it][2026-02-01 00:38:27,544] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 36%|███▌      | 120/336 [6:26:14<11:31:27, 192.07s/it]                                                       {'loss': 0.0331, 'learning_rate': 0.0001494408735316537, 'epoch': 1.06}
 36%|███▌      | 120/336 [6:26:14<11:31:27, 192.07s/it][2026-02-01 00:41:40,119] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 36%|███▌      | 121/336 [6:29:27<11:28:47, 192.22s/it]                                                       {'loss': 0.0325, 'learning_rate': 0.0001485983413242606, 'epoch': 1.07}
 36%|███▌      | 121/336 [6:29:27<11:28:47, 192.22s/it][2026-02-01 00:44:52,554] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 36%|███▋      | 122/336 [6:32:39<11:25:49, 192.29s/it]                                                       {'loss': 0.0101, 'learning_rate': 0.00014775126812118864, 'epoch': 1.08}
 36%|███▋      | 122/336 [6:32:40<11:25:49, 192.29s/it][2026-02-01 00:48:04,196] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 37%|███▋      | 123/336 [6:35:51<11:21:55, 192.09s/it]                                                       {'loss': 0.032, 'learning_rate': 0.00014689973307237687, 'epoch': 1.09}
 37%|███▋      | 123/336 [6:35:51<11:21:55, 192.09s/it][2026-02-01 00:51:16,722] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 37%|███▋      | 124/336 [6:39:04<11:19:11, 192.22s/it]                                                       {'loss': 0.0263, 'learning_rate': 0.00014604381574467615, 'epoch': 1.1}
 37%|███▋      | 124/336 [6:39:04<11:19:11, 192.22s/it][2026-02-01 00:54:27,364] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 37%|███▋      | 125/336 [6:42:14<11:14:18, 191.75s/it]                                                       {'loss': 0.0093, 'learning_rate': 0.0001451835961144145, 'epoch': 1.11}
 37%|███▋      | 125/336 [6:42:14<11:14:18, 191.75s/it][2026-02-01 00:57:39,639] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 38%|███▊      | 126/336 [6:45:27<11:11:40, 191.91s/it]                                                       {'loss': 0.0341, 'learning_rate': 0.00014431915455992414, 'epoch': 1.12}
 38%|███▊      | 126/336 [6:45:27<11:11:40, 191.91s/it][2026-02-01 01:00:52,088] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 38%|███▊      | 127/336 [6:48:39<11:09:04, 192.08s/it]                                                       {'loss': 0.018, 'learning_rate': 0.000143450571854031, 'epoch': 1.13}
 38%|███▊      | 127/336 [6:48:39<11:09:04, 192.08s/it][2026-02-01 01:04:06,223] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 38%|███▊      | 128/336 [6:51:53<11:07:58, 192.69s/it]                                                       {'loss': 0.021, 'learning_rate': 0.00014257792915650728, 'epoch': 1.14}
 38%|███▊      | 128/336 [6:51:53<11:07:58, 192.69s/it][2026-02-01 01:07:17,820] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 38%|███▊      | 129/336 [6:55:05<11:03:38, 192.36s/it]                                                       {'loss': 0.0262, 'learning_rate': 0.00014170130800648814, 'epoch': 1.14}
 38%|███▊      | 129/336 [6:55:05<11:03:38, 192.36s/it][2026-02-01 01:10:29,531] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 39%|███▊      | 130/336 [6:58:16<10:59:45, 192.16s/it]                                                       {'loss': 0.0226, 'learning_rate': 0.0001408207903148525, 'epoch': 1.15}
 39%|███▊      | 130/336 [6:58:16<10:59:45, 192.16s/it][2026-02-01 01:13:41,941] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 39%|███▉      | 131/336 [7:01:29<10:56:48, 192.24s/it]                                                       {'loss': 0.0111, 'learning_rate': 0.00013993645835656953, 'epoch': 1.16}
 39%|███▉      | 131/336 [7:01:29<10:56:48, 192.24s/it][2026-02-01 01:16:53,325] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 39%|███▉      | 132/336 [7:04:40<10:52:44, 191.98s/it]                                                       {'loss': 0.0164, 'learning_rate': 0.0001390483947630109, 'epoch': 1.17}
 39%|███▉      | 132/336 [7:04:40<10:52:44, 191.98s/it][2026-02-01 01:20:05,942] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 40%|███▉      | 133/336 [7:07:53<10:50:11, 192.17s/it]                                                       {'loss': 0.0067, 'learning_rate': 0.00013815668251422952, 'epoch': 1.18}
 40%|███▉      | 133/336 [7:07:53<10:50:11, 192.17s/it][2026-02-01 01:23:17,520] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 40%|███▉      | 134/336 [7:11:04<10:46:22, 191.99s/it]                                                       {'loss': 0.0327, 'learning_rate': 0.0001372614049312064, 'epoch': 1.19}
 40%|███▉      | 134/336 [7:11:04<10:46:22, 191.99s/it][2026-02-01 01:26:30,850] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 40%|████      | 135/336 [7:14:18<10:44:31, 192.40s/it]                                                       {'loss': 0.0517, 'learning_rate': 0.0001363626456680647, 'epoch': 1.2}
 40%|████      | 135/336 [7:14:18<10:44:31, 192.40s/it][2026-02-01 01:29:42,662] [WARNING] [stage3.py:1991:step] 45 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 40%|████      | 136/336 [7:17:30<10:40:44, 192.22s/it]                                                       {'loss': 0.023, 'learning_rate': 0.00013546048870425356, 'epoch': 1.21}
 40%|████      | 136/336 [7:17:30<10:40:44, 192.22s/it][2026-02-01 01:32:54,594] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 41%|████      | 137/336 [7:20:42<10:37:14, 192.13s/it]                                                       {'loss': 0.0102, 'learning_rate': 0.00013455501833670088, 'epoch': 1.22}
 41%|████      | 137/336 [7:20:42<10:37:14, 192.13s/it][2026-02-01 01:36:06,247] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 41%|████      | 138/336 [7:23:53<10:33:33, 191.99s/it]                                                       {'loss': 0.0297, 'learning_rate': 0.0001336463191719367, 'epoch': 1.22}
 41%|████      | 138/336 [7:23:53<10:33:33, 191.99s/it][2026-02-01 01:39:18,623] [WARNING] [stage3.py:1991:step] 45 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 41%|████▏     | 139/336 [7:27:06<10:30:44, 192.11s/it]                                                       {'loss': 0.0136, 'learning_rate': 0.00013273447611818767, 'epoch': 1.23}
 41%|████▏     | 139/336 [7:27:06<10:30:44, 192.11s/it][2026-02-01 01:42:31,584] [WARNING] [stage3.py:1991:step] 45 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 42%|████▏     | 140/336 [7:30:19<10:28:22, 192.36s/it]                                                       {'loss': 0.0209, 'learning_rate': 0.00013181957437744332, 'epoch': 1.24}
 42%|████▏     | 140/336 [7:30:19<10:28:22, 192.36s/it][2026-02-01 01:45:53,182] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 42%|████▏     | 141/336 [7:33:40<10:34:10, 195.13s/it]                                                       {'loss': 0.018, 'learning_rate': 0.00013090169943749476, 'epoch': 1.25}
 42%|████▏     | 141/336 [7:33:40<10:34:10, 195.13s/it][2026-02-01 01:49:05,124] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 42%|████▏     | 142/336 [7:36:52<10:27:50, 194.18s/it]                                                       {'loss': 0.0092, 'learning_rate': 0.00012998093706394675, 'epoch': 1.26}
 42%|████▏     | 142/336 [7:36:52<10:27:50, 194.18s/it][2026-02-01 01:52:18,288] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 43%|████▎     | 143/336 [7:40:05<10:23:37, 193.87s/it]                                                       {'loss': 0.0441, 'learning_rate': 0.00012905737329220392, 'epoch': 1.27}
 43%|████▎     | 143/336 [7:40:05<10:23:37, 193.87s/it][2026-02-01 01:55:31,311] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 43%|████▎     | 144/336 [7:43:18<10:19:34, 193.62s/it]                                                       {'loss': 0.0247, 'learning_rate': 0.00012813109441943166, 'epoch': 1.28}
 43%|████▎     | 144/336 [7:43:18<10:19:34, 193.62s/it][2026-02-01 01:58:43,390] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 43%|████▎     | 145/336 [7:46:30<10:14:52, 193.16s/it]                                                       {'loss': 0.0159, 'learning_rate': 0.00012720218699649243, 'epoch': 1.29}
 43%|████▎     | 145/336 [7:46:30<10:14:52, 193.16s/it][2026-02-01 02:01:55,650] [WARNING] [stage3.py:1991:step] 43 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 43%|████▎     | 146/336 [7:49:43<10:10:49, 192.89s/it]                                                       {'loss': 0.0115, 'learning_rate': 0.0001262707378198587, 'epoch': 1.29}
 43%|████▎     | 146/336 [7:49:43<10:10:49, 192.89s/it][2026-02-01 02:05:08,173] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 44%|████▍     | 147/336 [7:52:55<10:07:14, 192.78s/it]                                                       {'loss': 0.031, 'learning_rate': 0.00012533683392350263, 'epoch': 1.3}
 44%|████▍     | 147/336 [7:52:55<10:07:14, 192.78s/it][2026-02-01 02:08:24,355] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 44%|████▍     | 148/336 [7:56:11<10:07:14, 193.80s/it]                                                       {'loss': 0.0379, 'learning_rate': 0.00012440056257076375, 'epoch': 1.31}
 44%|████▍     | 148/336 [7:56:11<10:07:14, 193.80s/it][2026-02-01 02:11:44,271] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 44%|████▍     | 149/336 [7:59:31<10:09:43, 195.63s/it]                                                       {'loss': 0.0226, 'learning_rate': 0.00012346201124619502, 'epoch': 1.32}
 44%|████▍     | 149/336 [7:59:31<10:09:43, 195.63s/it][2026-02-01 02:14:55,036] [WARNING] [stage3.py:1991:step] 52 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 45%|████▍     | 150/336 [8:02:42<10:01:56, 194.17s/it]                                                       {'loss': 0.0374, 'learning_rate': 0.00012252126764738844, 'epoch': 1.33}
 45%|████▍     | 150/336 [8:02:42<10:01:56, 194.17s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[2026-02-01 02:18:55,576] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 45%|████▍     | 151/336 [8:06:43<10:41:35, 208.08s/it]                                                       {'loss': 0.0217, 'learning_rate': 0.00012157841967678063, 'epoch': 1.34}
 45%|████▍     | 151/336 [8:06:43<10:41:35, 208.08s/it][2026-02-01 02:22:09,141] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 45%|████▌     | 152/336 [8:09:56<10:24:45, 203.73s/it]                                                       {'loss': 0.0367, 'learning_rate': 0.00012063355543343924, 'epoch': 1.35}
 45%|████▌     | 152/336 [8:09:56<10:24:45, 203.73s/it][2026-02-01 02:25:21,580] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 46%|████▌     | 153/336 [8:13:09<10:11:02, 200.34s/it]                                                       {'loss': 0.019, 'learning_rate': 0.00011968676320483103, 'epoch': 1.36}
 46%|████▌     | 153/336 [8:13:09<10:11:02, 200.34s/it][2026-02-01 02:28:33,684] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 46%|████▌     | 154/336 [8:16:21<10:00:12, 197.87s/it]                                                       {'loss': 0.0175, 'learning_rate': 0.00011873813145857249, 'epoch': 1.37}
 46%|████▌     | 154/336 [8:16:21<10:00:12, 197.87s/it][2026-02-01 02:31:45,575] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 46%|████▌     | 155/336 [8:19:33<9:51:29, 196.08s/it]                                                       {'loss': 0.0176, 'learning_rate': 0.00011778774883416323, 'epoch': 1.37}
 46%|████▌     | 155/336 [8:19:33<9:51:29, 196.08s/it][2026-02-01 02:34:56,634] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 46%|████▋     | 156/336 [8:22:44<9:43:42, 194.57s/it]                                                      {'loss': 0.0182, 'learning_rate': 0.00011683570413470383, 'epoch': 1.38}
 46%|████▋     | 156/336 [8:22:44<9:43:42, 194.57s/it][2026-02-01 02:38:07,538] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 47%|████▋     | 157/336 [8:25:54<9:37:11, 193.47s/it]                                                      {'loss': 0.023, 'learning_rate': 0.00011588208631859807, 'epoch': 1.39}
 47%|████▋     | 157/336 [8:25:54<9:37:11, 193.47s/it][2026-02-01 02:41:21,551] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 47%|████▋     | 158/336 [8:29:08<9:34:26, 193.63s/it]                                                      {'loss': 0.0318, 'learning_rate': 0.00011492698449124042, 'epoch': 1.4}
 47%|████▋     | 158/336 [8:29:08<9:34:26, 193.63s/it][2026-02-01 02:44:32,822] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 47%|████▋     | 159/336 [8:32:20<9:29:07, 192.92s/it]                                                      {'loss': 0.0174, 'learning_rate': 0.0001139704878966906, 'epoch': 1.41}
 47%|████▋     | 159/336 [8:32:20<9:29:07, 192.92s/it][2026-02-01 02:47:44,176] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 48%|████▊     | 160/336 [8:35:31<9:24:31, 192.45s/it]                                                      {'loss': 0.0098, 'learning_rate': 0.00011301268590933434, 'epoch': 1.42}
 48%|████▊     | 160/336 [8:35:31<9:24:31, 192.45s/it][2026-02-01 02:50:57,030] [WARNING] [stage3.py:1991:step] 52 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 48%|████▊     | 161/336 [8:38:44<9:21:40, 192.57s/it]                                                      {'loss': 0.0118, 'learning_rate': 0.0001120536680255323, 'epoch': 1.43}
 48%|████▊     | 161/336 [8:38:44<9:21:40, 192.57s/it][2026-02-01 02:54:09,449] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 48%|████▊     | 162/336 [8:41:56<9:18:19, 192.53s/it]                                                      {'loss': 0.0361, 'learning_rate': 0.00011109352385525783, 'epoch': 1.44}
 48%|████▊     | 162/336 [8:41:56<9:18:19, 192.53s/it][2026-02-01 02:57:20,997] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 49%|████▊     | 163/336 [8:45:08<9:14:16, 192.23s/it]                                                      {'loss': 0.0388, 'learning_rate': 0.00011013234311372353, 'epoch': 1.45}
 49%|████▊     | 163/336 [8:45:08<9:14:16, 192.23s/it][2026-02-01 03:00:33,091] [WARNING] [stage3.py:1991:step] 52 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 49%|████▉     | 164/336 [8:48:20<9:10:57, 192.19s/it]                                                      {'loss': 0.0254, 'learning_rate': 0.00010917021561299863, 'epoch': 1.45}
 49%|████▉     | 164/336 [8:48:20<9:10:57, 192.19s/it][2026-02-01 03:03:46,732] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 49%|████▉     | 165/336 [8:51:34<9:08:59, 192.63s/it]                                                      {'loss': 0.0369, 'learning_rate': 0.00010820723125361684, 'epoch': 1.46}
 49%|████▉     | 165/336 [8:51:34<9:08:59, 192.63s/it][2026-02-01 03:06:59,171] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 49%|████▉     | 166/336 [8:54:46<9:05:36, 192.57s/it]                                                      {'loss': 0.0266, 'learning_rate': 0.00010724348001617625, 'epoch': 1.47}
 49%|████▉     | 166/336 [8:54:46<9:05:36, 192.57s/it][2026-02-01 03:10:09,704] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 50%|████▉     | 167/336 [8:57:57<9:00:41, 191.96s/it]                                                      {'loss': 0.0036, 'learning_rate': 0.00010627905195293135, 'epoch': 1.48}
 50%|████▉     | 167/336 [8:57:57<9:00:41, 191.96s/it][2026-02-01 03:13:23,216] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 50%|█████     | 168/336 [9:01:10<8:58:47, 192.42s/it]                                                      {'loss': 0.0398, 'learning_rate': 0.00010531403717937887, 'epoch': 1.49}
 50%|█████     | 168/336 [9:01:10<8:58:47, 192.42s/it][2026-02-01 03:16:34,989] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 50%|█████     | 169/336 [9:04:22<8:55:02, 192.23s/it]                                                      {'loss': 0.0123, 'learning_rate': 0.00010434852586583736, 'epoch': 1.5}
 50%|█████     | 169/336 [9:04:22<8:55:02, 192.23s/it][2026-02-01 03:19:47,264] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 51%|█████     | 170/336 [9:07:34<8:51:52, 192.24s/it]                                                      {'loss': 0.0395, 'learning_rate': 0.00010338260822902167, 'epoch': 1.51}
 51%|█████     | 170/336 [9:07:34<8:51:52, 192.24s/it][2026-02-01 03:22:58,667] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 51%|█████     | 171/336 [9:10:46<8:47:58, 191.99s/it]                                                      {'loss': 0.0231, 'learning_rate': 0.00010241637452361323, 'epoch': 1.52}
 51%|█████     | 171/336 [9:10:46<8:47:58, 191.99s/it][2026-02-01 03:26:11,972] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 51%|█████     | 172/336 [9:13:59<8:45:51, 192.39s/it]                                                      {'loss': 0.0205, 'learning_rate': 0.00010144991503382674, 'epoch': 1.53}
 51%|█████     | 172/336 [9:13:59<8:45:51, 192.39s/it][2026-02-01 03:29:22,836] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 51%|█████▏    | 173/336 [9:17:10<8:41:24, 191.93s/it]                                                      {'loss': 0.0248, 'learning_rate': 0.00010048332006497406, 'epoch': 1.53}
 51%|█████▏    | 173/336 [9:17:10<8:41:24, 191.93s/it][2026-02-01 03:32:35,502] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 52%|█████▏    | 174/336 [9:20:22<8:38:48, 192.15s/it]                                                      {'loss': 0.032, 'learning_rate': 9.9516679935026e-05, 'epoch': 1.54}
 52%|█████▏    | 174/336 [9:20:22<8:38:48, 192.15s/it][2026-02-01 03:35:58,598] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 52%|█████▏    | 175/336 [9:23:46<8:44:24, 195.43s/it]                                                      {'loss': 0.0131, 'learning_rate': 9.855008496617327e-05, 'epoch': 1.55}
 52%|█████▏    | 175/336 [9:23:46<8:44:24, 195.43s/it][2026-02-01 03:39:11,577] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 52%|█████▏    | 176/336 [9:26:59<8:39:11, 194.70s/it]                                                      {'loss': 0.0278, 'learning_rate': 9.75836254763868e-05, 'epoch': 1.56}
 52%|█████▏    | 176/336 [9:26:59<8:39:11, 194.70s/it][2026-02-01 03:42:23,411] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 53%|█████▎    | 177/336 [9:30:10<8:33:40, 193.84s/it]                                                      {'loss': 0.0224, 'learning_rate': 9.661739177097836e-05, 'epoch': 1.57}
 53%|█████▎    | 177/336 [9:30:10<8:33:40, 193.84s/it][2026-02-01 03:45:36,940] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 53%|█████▎    | 178/336 [9:33:24<8:30:11, 193.75s/it]                                                      {'loss': 0.0285, 'learning_rate': 9.565147413416266e-05, 'epoch': 1.58}
 53%|█████▎    | 178/336 [9:33:24<8:30:11, 193.75s/it][2026-02-01 03:48:46,640] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 53%|█████▎    | 179/336 [9:36:34<8:23:47, 192.53s/it]                                                      {'loss': 0.0189, 'learning_rate': 9.468596282062114e-05, 'epoch': 1.59}
 53%|█████▎    | 179/336 [9:36:34<8:23:47, 192.53s/it][2026-02-01 03:52:00,416] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 54%|█████▎    | 180/336 [9:39:47<8:21:33, 192.90s/it]                                                      {'loss': 0.0171, 'learning_rate': 9.372094804706867e-05, 'epoch': 1.6}
 54%|█████▎    | 180/336 [9:39:47<8:21:33, 192.90s/it][2026-02-01 03:55:10,945] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 54%|█████▍    | 181/336 [9:42:58<8:16:29, 192.19s/it]                                                      {'loss': 0.0119, 'learning_rate': 9.275651998382377e-05, 'epoch': 1.61}
 54%|█████▍    | 181/336 [9:42:58<8:16:29, 192.19s/it][2026-02-01 03:58:21,972] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 54%|█████▍    | 182/336 [9:46:09<8:12:23, 191.84s/it]                                                      {'loss': 0.0128, 'learning_rate': 9.179276874638315e-05, 'epoch': 1.61}
 54%|█████▍    | 182/336 [9:46:09<8:12:23, 191.84s/it][2026-02-01 04:01:36,441] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 54%|█████▍    | 183/336 [9:49:23<8:11:13, 192.64s/it]                                                      {'loss': 0.0311, 'learning_rate': 9.082978438700138e-05, 'epoch': 1.62}
 54%|█████▍    | 183/336 [9:49:23<8:11:13, 192.64s/it][2026-02-01 04:04:47,553] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 55%|█████▍    | 184/336 [9:52:34<8:06:50, 192.17s/it]                                                      {'loss': 0.0131, 'learning_rate': 8.986765688627652e-05, 'epoch': 1.63}
 55%|█████▍    | 184/336 [9:52:35<8:06:50, 192.17s/it][2026-02-01 04:08:00,569] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 55%|█████▌    | 185/336 [9:55:48<8:04:16, 192.43s/it]                                                      {'loss': 0.0248, 'learning_rate': 8.890647614474224e-05, 'epoch': 1.64}
 55%|█████▌    | 185/336 [9:55:48<8:04:16, 192.43s/it][2026-02-01 04:11:13,159] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 55%|█████▌    | 186/336 [9:59:00<8:01:11, 192.47s/it]                                                      {'loss': 0.0285, 'learning_rate': 8.79463319744677e-05, 'epoch': 1.65}
 55%|█████▌    | 186/336 [9:59:00<8:01:11, 192.47s/it][2026-02-01 04:14:26,047] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 56%|█████▌    | 187/336 [10:02:13<7:58:17, 192.60s/it]                                                       {'loss': 0.0317, 'learning_rate': 8.698731409066568e-05, 'epoch': 1.66}
 56%|█████▌    | 187/336 [10:02:13<7:58:17, 192.60s/it][2026-02-01 04:17:38,396] [WARNING] [stage3.py:1991:step] 45 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 56%|█████▌    | 188/336 [10:05:25<7:54:53, 192.52s/it]                                                       {'loss': 0.022, 'learning_rate': 8.602951210330942e-05, 'epoch': 1.67}
 56%|█████▌    | 188/336 [10:05:25<7:54:53, 192.52s/it][2026-02-01 04:20:51,455] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 56%|█████▋    | 189/336 [10:08:38<7:52:04, 192.68s/it]                                                       {'loss': 0.0118, 'learning_rate': 8.50730155087596e-05, 'epoch': 1.68}
 56%|█████▋    | 189/336 [10:08:38<7:52:04, 192.68s/it][2026-02-01 04:24:01,615] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 57%|█████▋    | 190/336 [10:11:49<7:47:01, 191.93s/it]                                                       {'loss': 0.0159, 'learning_rate': 8.411791368140196e-05, 'epoch': 1.69}
 57%|█████▋    | 190/336 [10:11:49<7:47:01, 191.93s/it][2026-02-01 04:27:14,128] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 57%|█████▋    | 191/336 [10:15:01<7:44:14, 192.10s/it]                                                       {'loss': 0.0317, 'learning_rate': 8.316429586529615e-05, 'epoch': 1.69}
 57%|█████▋    | 191/336 [10:15:01<7:44:14, 192.10s/it][2026-02-01 04:30:24,884] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 57%|█████▋    | 192/336 [10:18:12<7:40:04, 191.70s/it]                                                       {'loss': 0.0356, 'learning_rate': 8.221225116583678e-05, 'epoch': 1.7}
 57%|█████▋    | 192/336 [10:18:12<7:40:04, 191.70s/it][2026-02-01 04:33:37,988] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 57%|█████▋    | 193/336 [10:21:25<7:37:53, 192.12s/it]                                                       {'loss': 0.017, 'learning_rate': 8.126186854142752e-05, 'epoch': 1.71}
 57%|█████▋    | 193/336 [10:21:25<7:37:53, 192.12s/it][2026-02-01 04:36:50,774] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 58%|█████▊    | 194/336 [10:24:38<7:35:09, 192.32s/it]                                                       {'loss': 0.0179, 'learning_rate': 8.0313236795169e-05, 'epoch': 1.72}
 58%|█████▊    | 194/336 [10:24:38<7:35:09, 192.32s/it][2026-02-01 04:40:03,389] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 58%|█████▊    | 195/336 [10:27:50<7:32:09, 192.41s/it]                                                       {'loss': 0.0258, 'learning_rate': 7.936644456656081e-05, 'epoch': 1.73}
 58%|█████▊    | 195/336 [10:27:50<7:32:09, 192.41s/it][2026-02-01 04:43:15,890] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 58%|█████▊    | 196/336 [10:31:03<7:29:01, 192.44s/it]                                                       {'loss': 0.0219, 'learning_rate': 7.84215803232194e-05, 'epoch': 1.74}
 58%|█████▊    | 196/336 [10:31:03<7:29:01, 192.44s/it][2026-02-01 04:46:27,146] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 59%|█████▊    | 197/336 [10:34:14<7:24:59, 192.08s/it]                                                       {'loss': 0.0175, 'learning_rate': 7.747873235261157e-05, 'epoch': 1.75}
 59%|█████▊    | 197/336 [10:34:14<7:24:59, 192.08s/it][2026-02-01 04:49:38,738] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 59%|█████▉    | 198/336 [10:37:26<7:21:27, 191.94s/it]                                                       {'loss': 0.0216, 'learning_rate': 7.6537988753805e-05, 'epoch': 1.76}
 59%|█████▉    | 198/336 [10:37:26<7:21:27, 191.94s/it][2026-02-01 04:52:51,625] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 59%|█████▉    | 199/336 [10:40:39<7:18:54, 192.22s/it]                                                       {'loss': 0.0223, 'learning_rate': 7.559943742923626e-05, 'epoch': 1.76}
 59%|█████▉    | 199/336 [10:40:39<7:18:54, 192.22s/it][2026-02-01 04:56:03,025] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 60%|█████▉    | 200/336 [10:43:50<7:15:08, 191.97s/it]                                                       {'loss': 0.0242, 'learning_rate': 7.466316607649738e-05, 'epoch': 1.77}
 60%|█████▉    | 200/336 [10:43:50<7:15:08, 191.97s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[2026-02-01 05:00:03,474] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 60%|█████▉    | 201/336 [10:47:50<7:44:41, 206.53s/it]                                                       {'loss': 0.0376, 'learning_rate': 7.372926218014131e-05, 'epoch': 1.78}
 60%|█████▉    | 201/336 [10:47:50<7:44:41, 206.53s/it][2026-02-01 05:03:15,618] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 60%|██████    | 202/336 [10:51:03<7:31:35, 202.20s/it]                                                       {'loss': 0.0169, 'learning_rate': 7.279781300350758e-05, 'epoch': 1.79}
 60%|██████    | 202/336 [10:51:03<7:31:35, 202.20s/it][2026-02-01 05:06:27,283] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 60%|██████    | 203/336 [10:54:14<7:21:12, 199.04s/it]                                                       {'loss': 0.014, 'learning_rate': 7.186890558056836e-05, 'epoch': 1.8}
 60%|██████    | 203/336 [10:54:14<7:21:12, 199.04s/it][2026-02-01 05:09:40,431] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 61%|██████    | 204/336 [10:57:27<7:14:00, 197.27s/it]                                                       {'loss': 0.0238, 'learning_rate': 7.094262670779612e-05, 'epoch': 1.81}
 61%|██████    | 204/336 [10:57:27<7:14:00, 197.27s/it][2026-02-01 05:12:53,427] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 61%|██████    | 205/336 [11:00:40<7:07:54, 195.99s/it]                                                       {'loss': 0.0174, 'learning_rate': 7.00190629360533e-05, 'epoch': 1.82}
 61%|██████    | 205/336 [11:00:40<7:07:54, 195.99s/it][2026-02-01 05:16:05,531] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 61%|██████▏   | 206/336 [11:03:52<7:02:07, 194.82s/it]                                                       {'loss': 0.0196, 'learning_rate': 6.909830056250527e-05, 'epoch': 1.83}
 61%|██████▏   | 206/336 [11:03:52<7:02:07, 194.82s/it][2026-02-01 05:19:16,228] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 62%|██████▏   | 207/336 [11:07:03<6:56:12, 193.59s/it]                                                       {'loss': 0.0172, 'learning_rate': 6.81804256225567e-05, 'epoch': 1.84}
 62%|██████▏   | 207/336 [11:07:03<6:56:12, 193.59s/it][2026-02-01 05:22:29,821] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 62%|██████▏   | 208/336 [11:10:17<6:52:59, 193.59s/it]                                                       {'loss': 0.0156, 'learning_rate': 6.726552388181233e-05, 'epoch': 1.84}
 62%|██████▏   | 208/336 [11:10:17<6:52:59, 193.59s/it][2026-02-01 05:25:42,123] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 62%|██████▏   | 209/336 [11:13:29<6:48:56, 193.20s/it]                                                       {'loss': 0.0357, 'learning_rate': 6.63536808280633e-05, 'epoch': 1.85}
 62%|██████▏   | 209/336 [11:13:29<6:48:56, 193.20s/it][2026-02-01 05:28:53,974] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 62%|██████▎   | 210/336 [11:16:41<6:44:52, 192.80s/it]                                                       {'loss': 0.025, 'learning_rate': 6.544498166329913e-05, 'epoch': 1.86}
 62%|██████▎   | 210/336 [11:16:41<6:44:52, 192.80s/it][2026-02-01 05:32:03,275] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 63%|██████▎   | 211/336 [11:19:50<6:39:28, 191.75s/it]                                                       {'loss': 0.0141, 'learning_rate': 6.453951129574644e-05, 'epoch': 1.87}
 63%|██████▎   | 211/336 [11:19:50<6:39:28, 191.75s/it][2026-02-01 05:35:16,125] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 63%|██████▎   | 212/336 [11:23:03<6:36:57, 192.08s/it]                                                       {'loss': 0.0247, 'learning_rate': 6.36373543319353e-05, 'epoch': 1.88}
 63%|██████▎   | 212/336 [11:23:03<6:36:57, 192.08s/it][2026-02-01 05:38:16,894] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 63%|██████▎   | 213/336 [11:26:04<6:26:48, 188.69s/it]                                                       {'loss': 0.0373, 'learning_rate': 6.273859506879365e-05, 'epoch': 1.89}
 63%|██████▎   | 213/336 [11:26:04<6:26:48, 188.69s/it][2026-02-01 05:41:29,463] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 64%|██████▎   | 214/336 [11:29:16<6:26:01, 189.85s/it]                                                       {'loss': 0.0283, 'learning_rate': 6.18433174857705e-05, 'epoch': 1.9}
 64%|██████▎   | 214/336 [11:29:16<6:26:01, 189.85s/it][2026-02-01 05:44:41,396] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 64%|██████▍   | 215/336 [11:32:28<6:24:07, 190.48s/it]                                                       {'loss': 0.0251, 'learning_rate': 6.095160523698913e-05, 'epoch': 1.91}
 64%|██████▍   | 215/336 [11:32:28<6:24:07, 190.48s/it][2026-02-01 05:47:53,837] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 64%|██████▍   | 216/336 [11:35:41<6:22:07, 191.06s/it]                                                       {'loss': 0.0232, 'learning_rate': 6.006354164343046e-05, 'epoch': 1.92}
 64%|██████▍   | 216/336 [11:35:41<6:22:07, 191.06s/it][2026-02-01 05:51:05,580] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 65%|██████▍   | 217/336 [11:38:53<6:19:20, 191.27s/it]                                                       {'loss': 0.0284, 'learning_rate': 5.917920968514752e-05, 'epoch': 1.92}
 65%|██████▍   | 217/336 [11:38:53<6:19:20, 191.27s/it][2026-02-01 05:54:17,256] [WARNING] [stage3.py:1991:step] 45 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 65%|██████▍   | 218/336 [11:42:04<6:16:24, 191.39s/it]                                                       {'loss': 0.0242, 'learning_rate': 5.829869199351188e-05, 'epoch': 1.93}
 65%|██████▍   | 218/336 [11:42:04<6:16:24, 191.39s/it][2026-02-01 05:57:30,060] [WARNING] [stage3.py:1991:step] 53 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 65%|██████▌   | 219/336 [11:45:17<6:14:02, 191.81s/it]                                                       {'loss': 0.0386, 'learning_rate': 5.7422070843492734e-05, 'epoch': 1.94}
 65%|██████▌   | 219/336 [11:45:17<6:14:02, 191.81s/it][2026-02-01 06:00:41,974] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 65%|██████▌   | 220/336 [11:48:29<6:10:54, 191.85s/it]                                                       {'loss': 0.0153, 'learning_rate': 5.654942814596902e-05, 'epoch': 1.95}
 65%|██████▌   | 220/336 [11:48:29<6:10:54, 191.85s/it][2026-02-01 06:03:54,446] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 66%|██████▌   | 221/336 [11:51:41<6:08:03, 192.03s/it]                                                       {'loss': 0.0273, 'learning_rate': 5.568084544007588e-05, 'epoch': 1.96}
 66%|██████▌   | 221/336 [11:51:41<6:08:03, 192.03s/it][2026-02-01 06:07:07,652] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 66%|██████▌   | 222/336 [11:54:55<6:05:31, 192.38s/it]                                                       {'loss': 0.0189, 'learning_rate': 5.481640388558551e-05, 'epoch': 1.97}
 66%|██████▌   | 222/336 [11:54:55<6:05:31, 192.38s/it][2026-02-01 06:10:20,288] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 66%|██████▋   | 223/336 [11:58:07<6:02:27, 192.46s/it]                                                       {'loss': 0.0257, 'learning_rate': 5.395618425532389e-05, 'epoch': 1.98}
 66%|██████▋   | 223/336 [11:58:07<6:02:27, 192.46s/it][2026-02-01 06:13:30,853] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 67%|██████▋   | 224/336 [12:01:18<5:58:11, 191.89s/it]                                                       {'loss': 0.0229, 'learning_rate': 5.3100266927623156e-05, 'epoch': 1.99}
 67%|██████▋   | 224/336 [12:01:18<5:58:11, 191.89s/it][2026-02-01 06:16:43,995] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 67%|██████▋   | 225/336 [12:04:31<5:55:41, 192.27s/it]                                                       {'loss': 0.005, 'learning_rate': 5.2248731878811365e-05, 'epoch': 2.0}
 67%|██████▋   | 225/336 [12:04:31<5:55:41, 192.27s/it][2026-02-01 06:19:54,617] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 67%|██████▋   | 226/336 [12:07:42<5:51:35, 191.77s/it]                                                       {'loss': 0.0205, 'learning_rate': 5.14016586757394e-05, 'epoch': 2.0}
 67%|██████▋   | 226/336 [12:07:42<5:51:35, 191.77s/it][2026-02-01 06:23:07,777] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 68%|██████▊   | 227/336 [12:10:55<5:49:08, 192.19s/it]                                                       {'loss': 0.0119, 'learning_rate': 5.055912646834635e-05, 'epoch': 2.01}
 68%|██████▊   | 227/336 [12:10:55<5:49:08, 192.19s/it][2026-02-01 06:26:18,493] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 68%|██████▊   | 228/336 [12:14:05<5:45:08, 191.75s/it]                                                       {'loss': 0.0287, 'learning_rate': 4.972121398226371e-05, 'epoch': 2.02}
 68%|██████▊   | 228/336 [12:14:05<5:45:08, 191.75s/it][2026-02-01 06:29:31,947] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 68%|██████▊   | 229/336 [12:17:19<5:42:51, 192.26s/it]                                                       {'loss': 0.016, 'learning_rate': 4.888799951145948e-05, 'epoch': 2.03}
 68%|██████▊   | 229/336 [12:17:19<5:42:51, 192.26s/it][2026-02-01 06:32:44,100] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 68%|██████▊   | 230/336 [12:20:31<5:39:36, 192.23s/it]                                                       {'loss': 0.0112, 'learning_rate': 4.805956091092227e-05, 'epoch': 2.04}
 68%|██████▊   | 230/336 [12:20:31<5:39:36, 192.23s/it][2026-02-01 06:35:55,290] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 69%|██████▉   | 231/336 [12:23:42<5:35:51, 191.92s/it]                                                       {'loss': 0.0212, 'learning_rate': 4.723597558938672e-05, 'epoch': 2.05}
 69%|██████▉   | 231/336 [12:23:42<5:35:51, 191.92s/it][2026-02-01 06:39:07,723] [WARNING] [stage3.py:1991:step] 52 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 69%|██████▉   | 232/336 [12:26:55<5:32:55, 192.07s/it]                                                       {'loss': 0.0117, 'learning_rate': 4.6417320502100316e-05, 'epoch': 2.06}
 69%|██████▉   | 232/336 [12:26:55<5:32:55, 192.07s/it][2026-02-01 06:42:20,871] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 69%|██████▉   | 233/336 [12:30:08<5:30:16, 192.39s/it]                                                       {'loss': 0.0084, 'learning_rate': 4.5603672143632944e-05, 'epoch': 2.07}
 69%|██████▉   | 233/336 [12:30:08<5:30:16, 192.39s/it][2026-02-01 06:45:32,514] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 70%|██████▉   | 234/336 [12:33:19<5:26:41, 192.17s/it]                                                       {'loss': 0.0248, 'learning_rate': 4.479510654072909e-05, 'epoch': 2.08}
 70%|██████▉   | 234/336 [12:33:19<5:26:41, 192.17s/it][2026-02-01 06:48:44,088] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 70%|██████▉   | 235/336 [12:36:31<5:23:11, 191.99s/it]                                                       {'loss': 0.0252, 'learning_rate': 4.399169924520403e-05, 'epoch': 2.08}
 70%|██████▉   | 235/336 [12:36:31<5:23:11, 191.99s/it][2026-02-01 06:51:54,942] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 70%|███████   | 236/336 [12:39:42<5:19:24, 191.65s/it]                                                       {'loss': 0.0221, 'learning_rate': 4.3193525326884435e-05, 'epoch': 2.09}
 70%|███████   | 236/336 [12:39:42<5:19:24, 191.65s/it][2026-02-01 06:55:07,175] [WARNING] [stage3.py:1991:step] 45 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 71%|███████   | 237/336 [12:42:54<5:16:30, 191.82s/it]                                                       {'loss': 0.0088, 'learning_rate': 4.240065936659374e-05, 'epoch': 2.1}
 71%|███████   | 237/336 [12:42:54<5:16:30, 191.82s/it][2026-02-01 06:58:19,579] [WARNING] [stage3.py:1991:step] 44 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 71%|███████   | 238/336 [12:46:07<5:13:35, 192.00s/it]                                                       {'loss': 0.0155, 'learning_rate': 4.161317544918345e-05, 'epoch': 2.11}
 71%|███████   | 238/336 [12:46:07<5:13:35, 192.00s/it][2026-02-01 07:01:32,674] [WARNING] [stage3.py:1991:step] 44 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 71%|███████   | 239/336 [12:49:20<5:10:56, 192.34s/it]                                                       {'loss': 0.0239, 'learning_rate': 4.0831147156610684e-05, 'epoch': 2.12}
 71%|███████   | 239/336 [12:49:20<5:10:56, 192.34s/it][2026-02-01 07:04:46,742] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 71%|███████▏  | 240/336 [12:52:34<5:08:33, 192.85s/it]                                                       {'loss': 0.0159, 'learning_rate': 4.005464756106262e-05, 'epoch': 2.13}
 71%|███████▏  | 240/336 [12:52:34<5:08:33, 192.85s/it][2026-02-01 07:07:57,775] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 72%|███████▏  | 241/336 [12:55:45<5:04:28, 192.30s/it]                                                       {'loss': 0.011, 'learning_rate': 3.9283749218128885e-05, 'epoch': 2.14}
 72%|███████▏  | 241/336 [12:55:45<5:04:28, 192.30s/it][2026-02-01 07:11:08,671] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 72%|███████▏  | 242/336 [12:58:56<5:00:36, 191.88s/it]                                                       {'loss': 0.0123, 'learning_rate': 3.851852416002187e-05, 'epoch': 2.15}
 72%|███████▏  | 242/336 [12:58:56<5:00:36, 191.88s/it][2026-02-01 07:14:21,855] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 72%|███████▏  | 243/336 [13:02:09<4:58:01, 192.27s/it]                                                       {'loss': 0.0252, 'learning_rate': 3.775904388884618e-05, 'epoch': 2.16}
 72%|███████▏  | 243/336 [13:02:09<4:58:01, 192.27s/it][2026-02-01 07:17:33,822] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 73%|███████▎  | 244/336 [13:05:21<4:54:40, 192.18s/it]                                                       {'loss': 0.0187, 'learning_rate': 3.7005379369917325e-05, 'epoch': 2.16}
 73%|███████▎  | 244/336 [13:05:21<4:54:40, 192.18s/it][2026-02-01 07:20:46,919] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 73%|███████▎  | 245/336 [13:08:34<4:51:53, 192.46s/it]                                                       {'loss': 0.0142, 'learning_rate': 3.6257601025131026e-05, 'epoch': 2.17}
 73%|███████▎  | 245/336 [13:08:34<4:51:53, 192.46s/it][2026-02-01 07:23:58,909] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 73%|███████▎  | 246/336 [13:11:46<4:48:28, 192.32s/it]                                                       {'loss': 0.005, 'learning_rate': 3.5515778726382966e-05, 'epoch': 2.18}
 73%|███████▎  | 246/336 [13:11:46<4:48:28, 192.32s/it][2026-02-01 07:27:11,034] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 74%|███████▎  | 247/336 [13:14:58<4:45:11, 192.26s/it]                                                       {'loss': 0.0081, 'learning_rate': 3.477998178903982e-05, 'epoch': 2.19}
 74%|███████▎  | 247/336 [13:14:58<4:45:11, 192.26s/it][2026-02-01 07:30:22,266] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 74%|███████▍  | 248/336 [13:18:09<4:41:31, 191.95s/it]                                                       {'loss': 0.0078, 'learning_rate': 3.4050278965462764e-05, 'epoch': 2.2}
 74%|███████▍  | 248/336 [13:18:09<4:41:31, 191.95s/it][2026-02-01 07:33:35,539] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 74%|███████▍  | 249/336 [13:21:22<4:38:54, 192.35s/it]                                                       {'loss': 0.0295, 'learning_rate': 3.3326738438583114e-05, 'epoch': 2.21}
 74%|███████▍  | 249/336 [13:21:22<4:38:54, 192.35s/it][2026-02-01 07:36:47,089] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 74%|███████▍  | 250/336 [13:24:34<4:35:21, 192.11s/it]                                                       {'loss': 0.0151, 'learning_rate': 3.2609427815531426e-05, 'epoch': 2.22}
 74%|███████▍  | 250/336 [13:24:34<4:35:21, 192.11s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[2026-02-01 07:40:33,590] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 75%|███████▍  | 251/336 [13:28:21<4:46:46, 202.43s/it]                                                       {'loss': 0.0132, 'learning_rate': 3.1898414121320276e-05, 'epoch': 2.23}
 75%|███████▍  | 251/336 [13:28:21<4:46:46, 202.43s/it][2026-02-01 07:43:47,263] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 75%|███████▌  | 252/336 [13:31:34<4:39:43, 199.80s/it]                                                       {'loss': 0.0222, 'learning_rate': 3.11937637925816e-05, 'epoch': 2.24}
 75%|███████▌  | 252/336 [13:31:34<4:39:43, 199.80s/it][2026-02-01 07:46:59,622] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 75%|███████▌  | 253/336 [13:34:47<4:33:18, 197.57s/it]                                                       {'loss': 0.0156, 'learning_rate': 3.0495542671358746e-05, 'epoch': 2.24}
 75%|███████▌  | 253/336 [13:34:47<4:33:18, 197.57s/it][2026-02-01 07:50:10,430] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 76%|███████▌  | 254/336 [13:37:57<4:27:14, 195.54s/it]                                                       {'loss': 0.0076, 'learning_rate': 2.9803815998954332e-05, 'epoch': 2.25}
 76%|███████▌  | 254/336 [13:37:57<4:27:14, 195.54s/it][2026-02-01 07:53:22,877] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 76%|███████▌  | 255/336 [13:41:10<4:22:43, 194.61s/it]                                                       {'loss': 0.0189, 'learning_rate': 2.9118648409834205e-05, 'epoch': 2.26}
 76%|███████▌  | 255/336 [13:41:10<4:22:43, 194.61s/it][2026-02-01 07:56:35,534] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 76%|███████▌  | 256/336 [13:44:22<4:18:42, 194.03s/it]                                                       {'loss': 0.0196, 'learning_rate': 2.84401039255879e-05, 'epoch': 2.27}
 76%|███████▌  | 256/336 [13:44:22<4:18:42, 194.03s/it][2026-02-01 07:59:48,546] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 76%|███████▋  | 257/336 [13:47:35<4:15:03, 193.72s/it]                                                       {'loss': 0.0151, 'learning_rate': 2.7768245948946612e-05, 'epoch': 2.28}
 76%|███████▋  | 257/336 [13:47:35<4:15:03, 193.72s/it][2026-02-01 08:03:00,071] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 77%|███████▋  | 258/336 [13:50:47<4:10:58, 193.06s/it]                                                       {'loss': 0.0063, 'learning_rate': 2.7103137257858868e-05, 'epoch': 2.29}
 77%|███████▋  | 258/336 [13:50:47<4:10:58, 193.06s/it][2026-02-01 08:06:12,677] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 77%|███████▋  | 259/336 [13:54:00<4:07:35, 192.93s/it]                                                       {'loss': 0.019, 'learning_rate': 2.6444839999624494e-05, 'epoch': 2.3}
 77%|███████▋  | 259/336 [13:54:00<4:07:35, 192.93s/it][2026-02-01 08:09:23,915] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 77%|███████▋  | 260/336 [13:57:11<4:03:43, 192.42s/it]                                                       {'loss': 0.0158, 'learning_rate': 2.5793415685087797e-05, 'epoch': 2.31}
 77%|███████▋  | 260/336 [13:57:11<4:03:43, 192.42s/it][2026-02-01 08:12:37,060] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 78%|███████▊  | 261/336 [14:00:24<4:00:47, 192.64s/it]                                                       {'loss': 0.0113, 'learning_rate': 2.514892518288988e-05, 'epoch': 2.31}
 78%|███████▊  | 261/336 [14:00:24<4:00:47, 192.64s/it][2026-02-01 08:15:48,925] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 78%|███████▊  | 262/336 [14:03:36<3:57:18, 192.41s/it]                                                       {'loss': 0.0191, 'learning_rate': 2.4511428713781238e-05, 'epoch': 2.32}
 78%|███████▊  | 262/336 [14:03:36<3:57:18, 192.41s/it][2026-02-01 08:18:59,717] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 78%|███████▊  | 263/336 [14:06:47<3:53:30, 191.92s/it]                                                       {'loss': 0.0137, 'learning_rate': 2.3880985844994674e-05, 'epoch': 2.33}
 78%|███████▊  | 263/336 [14:06:47<3:53:30, 191.92s/it][2026-02-01 08:22:11,881] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 79%|███████▊  | 264/336 [14:09:59<3:50:23, 191.99s/it]                                                       {'loss': 0.0018, 'learning_rate': 2.3257655484679374e-05, 'epoch': 2.34}
 79%|███████▊  | 264/336 [14:09:59<3:50:23, 191.99s/it][2026-02-01 08:25:25,643] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 79%|███████▉  | 265/336 [14:13:13<3:47:49, 192.52s/it]                                                       {'loss': 0.0095, 'learning_rate': 2.2641495876396713e-05, 'epoch': 2.35}
 79%|███████▉  | 265/336 [14:13:13<3:47:49, 192.52s/it][2026-02-01 08:28:48,167] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 79%|███████▉  | 266/336 [14:16:35<3:48:06, 195.52s/it]                                                       {'loss': 0.0144, 'learning_rate': 2.2032564593677774e-05, 'epoch': 2.36}
 79%|███████▉  | 266/336 [14:16:35<3:48:06, 195.52s/it][2026-02-01 08:32:00,258] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 79%|███████▉  | 267/336 [14:19:47<3:43:40, 194.49s/it]                                                       {'loss': 0.0033, 'learning_rate': 2.1430918534643996e-05, 'epoch': 2.37}
 79%|███████▉  | 267/336 [14:19:47<3:43:40, 194.49s/it][2026-02-01 08:35:13,982] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 80%|███████▉  | 268/336 [14:23:01<3:40:09, 194.26s/it]                                                       {'loss': 0.0084, 'learning_rate': 2.0836613916690428e-05, 'epoch': 2.38}
 80%|███████▉  | 268/336 [14:23:01<3:40:09, 194.26s/it][2026-02-01 08:38:24,371] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 80%|████████  | 269/336 [14:26:11<3:35:37, 193.10s/it]                                                       {'loss': 0.0067, 'learning_rate': 2.024970627123295e-05, 'epoch': 2.39}
 80%|████████  | 269/336 [14:26:11<3:35:37, 193.10s/it][2026-02-01 08:41:35,441] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 80%|████████  | 270/336 [14:29:22<3:31:44, 192.49s/it]                                                       {'loss': 0.0154, 'learning_rate': 1.967025043851939e-05, 'epoch': 2.39}
 80%|████████  | 270/336 [14:29:22<3:31:44, 192.49s/it][2026-02-01 08:44:48,821] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 81%|████████  | 271/336 [14:32:36<3:28:49, 192.76s/it]                                                       {'loss': 0.0106, 'learning_rate': 1.9098300562505266e-05, 'epoch': 2.4}
 81%|████████  | 271/336 [14:32:36<3:28:49, 192.76s/it][2026-02-01 08:48:01,718] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 81%|████████  | 272/336 [14:35:49<3:25:39, 192.80s/it]                                                       {'loss': 0.0091, 'learning_rate': 1.8533910085794713e-05, 'epoch': 2.41}
 81%|████████  | 272/336 [14:35:49<3:25:39, 192.80s/it][2026-02-01 08:51:13,302] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 81%|████████▏ | 273/336 [14:39:00<3:22:03, 192.44s/it]                                                       {'loss': 0.0198, 'learning_rate': 1.7977131744646724e-05, 'epoch': 2.42}
 81%|████████▏ | 273/336 [14:39:00<3:22:03, 192.44s/it][2026-02-01 08:54:25,490] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 82%|████████▏ | 274/336 [14:42:12<3:18:46, 192.36s/it]                                                       {'loss': 0.017, 'learning_rate': 1.7428017564047594e-05, 'epoch': 2.43}
 82%|████████▏ | 274/336 [14:42:12<3:18:46, 192.36s/it][2026-02-01 08:57:38,411] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 82%|████████▏ | 275/336 [14:45:25<3:15:44, 192.53s/it]                                                       {'loss': 0.0079, 'learning_rate': 1.6886618852849724e-05, 'epoch': 2.44}
 82%|████████▏ | 275/336 [14:45:25<3:15:44, 192.53s/it][2026-02-01 09:00:50,159] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 82%|████████▏ | 276/336 [14:48:37<3:12:18, 192.30s/it]                                                       {'loss': 0.0391, 'learning_rate': 1.6352986198977325e-05, 'epoch': 2.45}
 82%|████████▏ | 276/336 [14:48:37<3:12:18, 192.30s/it][2026-02-01 09:04:02,209] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 82%|████████▏ | 277/336 [14:51:49<3:09:00, 192.22s/it]                                                       {'loss': 0.0086, 'learning_rate': 1.5827169464699576e-05, 'epoch': 2.46}
 82%|████████▏ | 277/336 [14:51:49<3:09:00, 192.22s/it][2026-02-01 09:07:14,229] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 83%|████████▎ | 278/336 [14:55:01<3:05:45, 192.16s/it]                                                       {'loss': 0.0205, 'learning_rate': 1.530921778197142e-05, 'epoch': 2.47}
 83%|████████▎ | 278/336 [14:55:01<3:05:45, 192.16s/it][2026-02-01 09:10:25,956] [WARNING] [stage3.py:1991:step] 53 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 83%|████████▎ | 279/336 [14:58:13<3:02:25, 192.03s/it]                                                       {'loss': 0.009, 'learning_rate': 1.4799179547842822e-05, 'epoch': 2.47}
 83%|████████▎ | 279/336 [14:58:13<3:02:25, 192.03s/it][2026-02-01 09:13:38,835] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 83%|████████▎ | 280/336 [15:01:26<2:59:27, 192.28s/it]                                                       {'loss': 0.0116, 'learning_rate': 1.429710241993656e-05, 'epoch': 2.48}
 83%|████████▎ | 280/336 [15:01:26<2:59:27, 192.28s/it][2026-02-01 09:16:50,797] [WARNING] [stage3.py:1991:step] 45 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 84%|████████▎ | 281/336 [15:04:38<2:56:10, 192.19s/it]                                                       {'loss': 0.0104, 'learning_rate': 1.3803033311995072e-05, 'epoch': 2.49}
 84%|████████▎ | 281/336 [15:04:38<2:56:10, 192.19s/it][2026-02-01 09:20:02,102] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 84%|████████▍ | 282/336 [15:07:49<2:52:43, 191.92s/it]                                                       {'loss': 0.0106, 'learning_rate': 1.3317018389496927e-05, 'epoch': 2.5}
 84%|████████▍ | 282/336 [15:07:49<2:52:43, 191.92s/it][2026-02-01 09:23:15,165] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 84%|████████▍ | 283/336 [15:11:02<2:49:50, 192.26s/it]                                                       {'loss': 0.0152, 'learning_rate': 1.2839103065343083e-05, 'epoch': 2.51}
 84%|████████▍ | 283/336 [15:11:02<2:49:50, 192.26s/it][2026-02-01 09:26:27,319] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 85%|████████▍ | 284/336 [15:14:14<2:46:36, 192.23s/it]                                                       {'loss': 0.0147, 'learning_rate': 1.2369331995613665e-05, 'epoch': 2.52}
 85%|████████▍ | 284/336 [15:14:14<2:46:36, 192.23s/it][2026-02-01 09:29:39,916] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 85%|████████▍ | 285/336 [15:17:27<2:43:29, 192.34s/it]                                                       {'loss': 0.0078, 'learning_rate': 1.1907749075395147e-05, 'epoch': 2.53}
 85%|████████▍ | 285/336 [15:17:27<2:43:29, 192.34s/it][2026-02-01 09:33:03,611] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 85%|████████▌ | 286/336 [15:20:51<2:43:07, 195.75s/it]                                                       {'loss': 0.0078, 'learning_rate': 1.1454397434679021e-05, 'epoch': 2.54}
 85%|████████▌ | 286/336 [15:20:51<2:43:07, 195.75s/it][2026-02-01 09:36:15,412] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 85%|████████▌ | 287/336 [15:24:02<2:38:53, 194.56s/it]                                                       {'loss': 0.0102, 'learning_rate': 1.1009319434331622e-05, 'epoch': 2.55}
 85%|████████▌ | 287/336 [15:24:02<2:38:53, 194.56s/it][2026-02-01 09:39:27,992] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 86%|████████▌ | 288/336 [15:27:15<2:35:10, 193.97s/it]                                                       {'loss': 0.0077, 'learning_rate': 1.0572556662136035e-05, 'epoch': 2.55}
 86%|████████▌ | 288/336 [15:27:15<2:35:10, 193.97s/it][2026-02-01 09:42:39,724] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 86%|████████▌ | 289/336 [15:30:27<2:31:24, 193.30s/it]                                                       {'loss': 0.0146, 'learning_rate': 1.014414992890611e-05, 'epoch': 2.56}
 86%|████████▌ | 289/336 [15:30:27<2:31:24, 193.30s/it][2026-02-01 09:45:51,201] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 86%|████████▋ | 290/336 [15:33:38<2:27:46, 192.75s/it]                                                       {'loss': 0.018, 'learning_rate': 9.724139264673116e-06, 'epoch': 2.57}
 86%|████████▋ | 290/336 [15:33:38<2:27:46, 192.75s/it][2026-02-01 09:49:03,549] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 87%|████████▋ | 291/336 [15:36:50<2:24:28, 192.63s/it]                                                       {'loss': 0.0198, 'learning_rate': 9.31256391494546e-06, 'epoch': 2.58}
 87%|████████▋ | 291/336 [15:36:50<2:24:28, 192.63s/it][2026-02-01 09:52:15,652] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 87%|████████▋ | 292/336 [15:40:03<2:21:08, 192.47s/it]                                                       {'loss': 0.0089, 'learning_rate': 8.909462337041507e-06, 'epoch': 2.59}
 87%|████████▋ | 292/336 [15:40:03<2:21:08, 192.47s/it][2026-02-01 09:55:27,220] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 87%|████████▋ | 293/336 [15:43:14<2:17:44, 192.20s/it]                                                       {'loss': 0.0053, 'learning_rate': 8.514872196496183e-06, 'epoch': 2.6}
 87%|████████▋ | 293/336 [15:43:14<2:17:44, 192.20s/it][2026-02-01 09:58:40,192] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 88%|████████▊ | 294/336 [15:46:27<2:14:42, 192.43s/it]                                                       {'loss': 0.0154, 'learning_rate': 8.128830363541574e-06, 'epoch': 2.61}
 88%|████████▊ | 294/336 [15:46:27<2:14:42, 192.43s/it][2026-02-01 10:01:53,346] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 88%|████████▊ | 295/336 [15:49:40<2:11:39, 192.66s/it]                                                       {'loss': 0.0076, 'learning_rate': 7.751372909661769e-06, 'epoch': 2.62}
 88%|████████▊ | 295/336 [15:49:40<2:11:39, 192.66s/it][2026-02-01 10:05:03,923] [WARNING] [stage3.py:1991:step] 45 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 88%|████████▊ | 296/336 [15:52:51<2:08:00, 192.02s/it]                                                       {'loss': 0.0039, 'learning_rate': 7.382535104222366e-06, 'epoch': 2.63}
 88%|████████▊ | 296/336 [15:52:51<2:08:00, 192.02s/it][2026-02-01 10:08:18,386] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 88%|████████▊ | 297/336 [15:56:05<2:05:17, 192.76s/it]                                                       {'loss': 0.0212, 'learning_rate': 7.022351411174866e-06, 'epoch': 2.63}
 88%|████████▊ | 297/336 [15:56:05<2:05:17, 192.76s/it][2026-02-01 10:11:30,065] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 89%|████████▊ | 298/336 [15:59:17<2:01:52, 192.43s/it]                                                       {'loss': 0.0185, 'learning_rate': 6.670855485836525e-06, 'epoch': 2.64}
 89%|████████▊ | 298/336 [15:59:17<2:01:52, 192.43s/it][2026-02-01 10:14:40,859] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 89%|████████▉ | 299/336 [16:02:28<1:58:21, 191.94s/it]                                                       {'loss': 0.0103, 'learning_rate': 6.32808017174551e-06, 'epoch': 2.65}
 89%|████████▉ | 299/336 [16:02:28<1:58:21, 191.94s/it][2026-02-01 10:17:53,469] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 89%|████████▉ | 300/336 [16:05:40<1:55:17, 192.14s/it]                                                       {'loss': 0.02, 'learning_rate': 5.994057497592031e-06, 'epoch': 2.66}
 89%|████████▉ | 300/336 [16:05:40<1:55:17, 192.14s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[2026-02-01 10:21:53,181] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 90%|████████▉ | 301/336 [16:09:40<2:00:24, 206.41s/it]                                                       {'loss': 0.0153, 'learning_rate': 5.668818674225685e-06, 'epoch': 2.67}
 90%|████████▉ | 301/336 [16:09:40<2:00:24, 206.41s/it][2026-02-01 10:25:05,648] [WARNING] [stage3.py:1991:step] 46 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 90%|████████▉ | 302/336 [16:12:53<1:54:35, 202.23s/it]                                                       {'loss': 0.0284, 'learning_rate': 5.3523940917390215e-06, 'epoch': 2.68}
 90%|████████▉ | 302/336 [16:12:53<1:54:35, 202.23s/it][2026-02-01 10:28:18,430] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 90%|█████████ | 303/336 [16:16:05<1:49:40, 199.39s/it]                                                       {'loss': 0.0062, 'learning_rate': 5.0448133166279944e-06, 'epoch': 2.69}
 90%|█████████ | 303/336 [16:16:05<1:49:40, 199.39s/it][2026-02-01 10:31:28,585] [WARNING] [stage3.py:1991:step] 45 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 90%|█████████ | 304/336 [16:19:16<1:44:51, 196.62s/it]                                                       {'loss': 0.0041, 'learning_rate': 4.746105089029229e-06, 'epoch': 2.7}
 90%|█████████ | 304/336 [16:19:16<1:44:51, 196.62s/it][2026-02-01 10:34:41,727] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 91%|█████████ | 305/336 [16:22:29<1:41:02, 195.58s/it]                                                       {'loss': 0.0242, 'learning_rate': 4.4562973200346416e-06, 'epoch': 2.71}
 91%|█████████ | 305/336 [16:22:29<1:41:02, 195.58s/it][2026-02-01 10:37:52,857] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 91%|█████████ | 306/336 [16:25:40<1:37:07, 194.24s/it]                                                       {'loss': 0.0147, 'learning_rate': 4.175417089083378e-06, 'epoch': 2.71}
 91%|█████████ | 306/336 [16:25:40<1:37:07, 194.24s/it][2026-02-01 10:41:04,823] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 91%|█████████▏| 307/336 [16:28:52<1:33:33, 193.56s/it]                                                       {'loss': 0.0199, 'learning_rate': 3.903490641431573e-06, 'epoch': 2.72}
 91%|█████████▏| 307/336 [16:28:52<1:33:33, 193.56s/it][2026-02-01 10:44:18,614] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 92%|█████████▏| 308/336 [16:32:06<1:30:21, 193.63s/it]                                                       {'loss': 0.0202, 'learning_rate': 3.6405433856999684e-06, 'epoch': 2.73}
 92%|█████████▏| 308/336 [16:32:06<1:30:21, 193.63s/it][2026-02-01 10:47:30,041] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 92%|█████████▏| 309/336 [16:35:17<1:26:50, 192.97s/it]                                                       {'loss': 0.0193, 'learning_rate': 3.3865998914997643e-06, 'epoch': 2.74}
 92%|█████████▏| 309/336 [16:35:17<1:26:50, 192.97s/it][2026-02-01 10:50:42,194] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 92%|█████████▏| 310/336 [16:38:29<1:23:30, 192.72s/it]                                                       {'loss': 0.0053, 'learning_rate': 3.1416838871368924e-06, 'epoch': 2.75}
 92%|█████████▏| 310/336 [16:38:29<1:23:30, 192.72s/it][2026-02-01 10:53:52,768] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 93%|█████████▎| 311/336 [16:41:40<1:20:01, 192.08s/it]                                                       {'loss': 0.0164, 'learning_rate': 2.905818257394799e-06, 'epoch': 2.76}
 93%|█████████▎| 311/336 [16:41:40<1:20:01, 192.08s/it][2026-02-01 10:57:05,362] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 93%|█████████▎| 312/336 [16:44:52<1:16:53, 192.23s/it]                                                       {'loss': 0.0109, 'learning_rate': 2.679025041396155e-06, 'epoch': 2.77}
 93%|█████████▎| 312/336 [16:44:52<1:16:53, 192.23s/it][2026-02-01 11:00:18,507] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 93%|█████████▎| 313/336 [16:48:05<1:13:47, 192.51s/it]                                                       {'loss': 0.0218, 'learning_rate': 2.461325430543482e-06, 'epoch': 2.78}
 93%|█████████▎| 313/336 [16:48:05<1:13:47, 192.51s/it][2026-02-01 11:03:32,964] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 93%|█████████▎| 314/336 [16:51:20<1:10:47, 193.09s/it]                                                       {'loss': 0.011, 'learning_rate': 2.2527397665391027e-06, 'epoch': 2.78}
 93%|█████████▎| 314/336 [16:51:20<1:10:47, 193.09s/it][2026-02-01 11:06:43,787] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 94%|█████████▍| 315/336 [16:54:31<1:07:20, 192.41s/it]                                                       {'loss': 0.0132, 'learning_rate': 2.053287539484405e-06, 'epoch': 2.79}
 94%|█████████▍| 315/336 [16:54:31<1:07:20, 192.41s/it][2026-02-01 11:09:55,765] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 94%|█████████▍| 316/336 [16:57:43<1:04:05, 192.28s/it]                                                       {'loss': 0.0178, 'learning_rate': 1.8629873860586566e-06, 'epoch': 2.8}
 94%|█████████▍| 316/336 [16:57:43<1:04:05, 192.28s/it][2026-02-01 11:13:08,374] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 94%|█████████▍| 317/336 [17:00:55<1:00:55, 192.38s/it]                                                       {'loss': 0.0108, 'learning_rate': 1.6818570877776718e-06, 'epoch': 2.81}
 94%|█████████▍| 317/336 [17:00:55<1:00:55, 192.38s/it][2026-02-01 11:16:20,669] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 95%|█████████▍| 318/336 [17:04:08<57:42, 192.35s/it]                                                       {'loss': 0.0097, 'learning_rate': 1.5099135693322774e-06, 'epoch': 2.82}
 95%|█████████▍| 318/336 [17:04:08<57:42, 192.35s/it][2026-02-01 11:19:31,405] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 95%|█████████▍| 319/336 [17:07:18<54:21, 191.87s/it]                                                     {'loss': 0.0226, 'learning_rate': 1.3471728970068987e-06, 'epoch': 2.83}
 95%|█████████▍| 319/336 [17:07:18<54:21, 191.87s/it][2026-02-01 11:22:45,641] [WARNING] [stage3.py:1991:step] 52 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 95%|█████████▌| 320/336 [17:10:33<51:21, 192.58s/it]                                                     {'loss': 0.0092, 'learning_rate': 1.1936502771783486e-06, 'epoch': 2.84}
 95%|█████████▌| 320/336 [17:10:33<51:21, 192.58s/it][2026-02-01 11:25:56,574] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 96%|█████████▌| 321/336 [17:13:44<48:01, 192.08s/it]                                                     {'loss': 0.011, 'learning_rate': 1.0493600548948878e-06, 'epoch': 2.85}
 96%|█████████▌| 321/336 [17:13:44<48:01, 192.08s/it][2026-02-01 11:29:08,935] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 96%|█████████▌| 322/336 [17:16:56<44:50, 192.17s/it]                                                     {'loss': 0.0039, 'learning_rate': 9.143157125359514e-07, 'epoch': 2.86}
 96%|█████████▌| 322/336 [17:16:56<44:50, 192.17s/it][2026-02-01 11:32:20,885] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 96%|█████████▌| 323/336 [17:20:08<41:37, 192.10s/it]                                                     {'loss': 0.0224, 'learning_rate': 7.885298685522235e-07, 'epoch': 2.86}
 96%|█████████▌| 323/336 [17:20:08<41:37, 192.10s/it][2026-02-01 11:35:32,566] [WARNING] [stage3.py:1991:step] 47 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 96%|█████████▋| 324/336 [17:23:20<38:23, 191.98s/it]                                                     {'loss': 0.0086, 'learning_rate': 6.720142762867032e-07, 'epoch': 2.87}
 96%|█████████▋| 324/336 [17:23:20<38:23, 191.98s/it][2026-02-01 11:38:45,796] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 97%|█████████▋| 325/336 [17:26:33<35:15, 192.35s/it]                                                     {'loss': 0.0192, 'learning_rate': 5.647798228764156e-07, 'epoch': 2.88}
 97%|█████████▋| 325/336 [17:26:33<35:15, 192.35s/it][2026-02-01 11:41:57,791] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 97%|█████████▋| 326/336 [17:29:45<32:02, 192.25s/it]                                                     {'loss': 0.0157, 'learning_rate': 4.668365282351372e-07, 'epoch': 2.89}
 97%|█████████▋| 326/336 [17:29:45<32:02, 192.25s/it][2026-02-01 11:45:09,586] [WARNING] [stage3.py:1991:step] 52 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 97%|█████████▋| 327/336 [17:32:57<28:48, 192.11s/it]                                                     {'loss': 0.0168, 'learning_rate': 3.781935441171336e-07, 'epoch': 2.9}
 97%|█████████▋| 327/336 [17:32:57<28:48, 192.11s/it][2026-02-01 11:48:21,769] [WARNING] [stage3.py:1991:step] 48 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 98%|█████████▊| 328/336 [17:36:09<25:37, 192.13s/it]                                                     {'loss': 0.0107, 'learning_rate': 2.988591532620322e-07, 'epoch': 2.91}
 98%|█████████▊| 328/336 [17:36:09<25:37, 192.13s/it][2026-02-01 11:51:34,265] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 98%|█████████▊| 329/336 [17:39:21<22:25, 192.24s/it]                                                     {'loss': 0.0141, 'learning_rate': 2.288407686208971e-07, 'epoch': 2.92}
 98%|█████████▊| 329/336 [17:39:21<22:25, 192.24s/it][2026-02-01 11:54:45,598] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 98%|█████████▊| 330/336 [17:42:33<19:11, 191.97s/it]                                                     {'loss': 0.0279, 'learning_rate': 1.68144932663572e-07, 'epoch': 2.93}
 98%|█████████▊| 330/336 [17:42:33<19:11, 191.97s/it][2026-02-01 11:57:58,791] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 99%|█████████▊| 331/336 [17:45:46<16:01, 192.34s/it]                                                     {'loss': 0.0049, 'learning_rate': 1.1677731676733584e-07, 'epoch': 2.94}
 99%|█████████▊| 331/336 [17:45:46<16:01, 192.34s/it][2026-02-01 12:01:12,479] [WARNING] [stage3.py:1991:step] 51 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 99%|█████████▉| 332/336 [17:48:59<12:51, 192.75s/it]                                                     {'loss': 0.006, 'learning_rate': 7.474272068698218e-08, 'epoch': 2.94}
 99%|█████████▉| 332/336 [17:48:59<12:51, 192.75s/it][2026-02-01 12:04:33,848] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 99%|█████████▉| 333/336 [17:52:21<09:45, 195.33s/it]                                                     {'loss': 0.0103, 'learning_rate': 4.2045072106333684e-08, 'epoch': 2.95}
 99%|█████████▉| 333/336 [17:52:21<09:45, 195.33s/it][2026-02-01 12:07:47,938] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 99%|█████████▉| 334/336 [17:55:35<06:29, 194.96s/it]                                                     {'loss': 0.0047, 'learning_rate': 1.8687426271246645e-08, 'epoch': 2.96}
 99%|█████████▉| 334/336 [17:55:35<06:29, 194.96s/it][2026-02-01 12:11:00,390] [WARNING] [stage3.py:1991:step] 49 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
100%|█████████▉| 335/336 [17:58:47<03:14, 194.20s/it]                                                     {'loss': 0.0161, 'learning_rate': 4.6719657041283114e-09, 'epoch': 2.97}
100%|█████████▉| 335/336 [17:58:47<03:14, 194.20s/it][2026-02-01 12:14:10,732] [WARNING] [stage3.py:1991:step] 50 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
100%|██████████| 336/336 [18:01:58<00:00, 193.05s/it]                                                     {'loss': 0.0046, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 336/336 [18:01:58<00:00, 193.05s/it]                                                     {'train_runtime': 64918.3937, 'train_samples_per_second': 0.5, 'train_steps_per_second': 0.005, 'train_loss': 0.06385672194405093, 'epoch': 2.98}
100%|██████████| 336/336 [18:01:58<00:00, 193.05s/it]100%|██████████| 336/336 [18:01:58<00:00, 193.21s/it]
[2026-02-01 12:14:37,153] [INFO] [launch.py:347:main] Process 3742604 exits successfully.
[2026-02-01 12:14:39,154] [INFO] [launch.py:347:main] Process 3742607 exits successfully.
[2026-02-01 12:14:41,155] [INFO] [launch.py:347:main] Process 3742609 exits successfully.
[2026-02-01 12:14:43,155] [INFO] [launch.py:347:main] Process 3742608 exits successfully.
[2026-02-01 12:14:46,156] [INFO] [launch.py:347:main] Process 3742605 exits successfully.
[2026-02-01 12:14:48,156] [INFO] [launch.py:347:main] Process 3742606 exits successfully.
