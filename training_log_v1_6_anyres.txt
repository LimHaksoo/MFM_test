/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-02-03 14:20:16,553] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-03 14:20:26,559] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5: setting --include=localhost:0,1,2,3,4,5
[2026-02-03 14:20:32,651] [INFO] [runner.py:571:main] cmd = /data3/jisu/miniconda3/envs/mfm-new/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path liuhaotian/llava-v1.6-vicuna-7b --version v1 --data_path /data3/jisu/LLaVA/visa_llava_instruct.json --image_folder /data3/jisu/MFM/datasets/ViSA --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio anyres --mm_patch_merge_type spatial_unpad --image_grid_pinpoints [[336,336],[336,672],[672,336]] --group_by_modality_length True --bf16 False --fp16 True --output_dir ./checkpoints/llava-v1.6-rmfjausvicuna-7b-mfm-lora-anyres --num_train_epochs 3 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --gradient_accumulation_steps 16 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 False --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to none
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-02-03 14:20:37,698] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-03 14:20:47,690] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2026-02-03 14:20:47,690] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2026-02-03 14:20:47,690] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2026-02-03 14:20:47,690] [INFO] [launch.py:163:main] dist_world_size=6
[2026-02-03 14:20:47,690] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-02-03 14:20:55,902] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-03 14:20:55,903] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-03 14:20:55,908] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-03 14:20:55,911] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-03 14:20:55,911] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-03 14:20:55,912] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-03 14:21:51,190] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-02-03 14:21:51,379] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-02-03 14:21:51,379] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2026-02-03 14:21:51,397] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-02-03 14:21:51,398] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-02-03 14:21:51,409] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-02-03 14:21:51,421] [INFO] [comm.py:637:init_distributed] cdb=None
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2026-02-03 14:23:22,544] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 687, num_elems = 7.06B
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:13,  6.50s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:13,  6.51s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:13,  6.51s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:13,  6.50s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:13,  6.50s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.95s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.81s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.80s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.80s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.80s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.82s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:15<00:07,  7.25s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.92s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.69s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.92s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.69s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.92s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.69s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.92s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.69s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.93s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.70s/it]
[META ALL] 0 tensors are on meta (params+buffers)
model class: LlavaLlamaForCausalLM
model name_or_path: liuhaotian/llava-v1.6-vicuna-7b
trainable param groups: 296
top trainables: [('model.image_newline', 0), ('model.embed_tokens.weight', 0), ('model.layers.0.self_attn.q_proj.weight', 0), ('model.layers.0.self_attn.k_proj.weight', 0), ('model.layers.0.self_attn.v_proj.weight', 0), ('model.layers.0.self_attn.o_proj.weight', 0), ('model.layers.0.mlp.gate_proj.weight', 0), ('model.layers.0.mlp.up_proj.weight', 0), ('model.layers.0.mlp.down_proj.weight', 0), ('model.layers.0.input_layernorm.weight', 0), ('model.layers.0.post_attention_layernorm.weight', 0), ('model.layers.1.self_attn.q_proj.weight', 0), ('model.layers.1.self_attn.k_proj.weight', 0), ('model.layers.1.self_attn.v_proj.weight', 0), ('model.layers.1.self_attn.o_proj.weight', 0), ('model.layers.1.mlp.gate_proj.weight', 0), ('model.layers.1.mlp.up_proj.weight', 0), ('model.layers.1.mlp.down_proj.weight', 0), ('model.layers.1.input_layernorm.weight', 0), ('model.layers.1.post_attention_layernorm.weight', 0)]
total trainable params: 0
[META ALL] 0 tensors are on meta (params+buffers)
model class: LlavaLlamaForCausalLM
model name_or_path: liuhaotian/llava-v1.6-vicuna-7b
trainable param groups: 296
top trainables: [('model.image_newline', 0), ('model.embed_tokens.weight', 0), ('model.layers.0.self_attn.q_proj.weight', 0), ('model.layers.0.self_attn.k_proj.weight', 0), ('model.layers.0.self_attn.v_proj.weight', 0), ('model.layers.0.self_attn.o_proj.weight', 0), ('model.layers.0.mlp.gate_proj.weight', 0), ('model.layers.0.mlp.up_proj.weight', 0), ('model.layers.0.mlp.down_proj.weight', 0), ('model.layers.0.input_layernorm.weight', 0), ('model.layers.0.post_attention_layernorm.weight', 0), ('model.layers.1.self_attn.q_proj.weight', 0), ('model.layers.1.self_attn.k_proj.weight', 0), ('model.layers.1.self_attn.v_proj.weight', 0), ('model.layers.1.self_attn.o_proj.weight', 0), ('model.layers.1.mlp.gate_proj.weight', 0), ('model.layers.1.mlp.up_proj.weight', 0), ('model.layers.1.mlp.down_proj.weight', 0), ('model.layers.1.input_layernorm.weight', 0), ('model.layers.1.post_attention_layernorm.weight', 0)]
total trainable params: 0
[META ALL] 0 tensors are on meta (params+buffers)
model class: LlavaLlamaForCausalLM
model name_or_path: liuhaotian/llava-v1.6-vicuna-7b
trainable param groups: 296
top trainables: [('model.image_newline', 0), ('model.embed_tokens.weight', 0), ('model.layers.0.self_attn.q_proj.weight', 0), ('model.layers.0.self_attn.k_proj.weight', 0), ('model.layers.0.self_attn.v_proj.weight', 0), ('model.layers.0.self_attn.o_proj.weight', 0), ('model.layers.0.mlp.gate_proj.weight', 0), ('model.layers.0.mlp.up_proj.weight', 0), ('model.layers.0.mlp.down_proj.weight', 0), ('model.layers.0.input_layernorm.weight', 0), ('model.layers.0.post_attention_layernorm.weight', 0), ('model.layers.1.self_attn.q_proj.weight', 0), ('model.layers.1.self_attn.k_proj.weight', 0), ('model.layers.1.self_attn.v_proj.weight', 0), ('model.layers.1.self_attn.o_proj.weight', 0), ('model.layers.1.mlp.gate_proj.weight', 0), ('model.layers.1.mlp.up_proj.weight', 0), ('model.layers.1.mlp.down_proj.weight', 0), ('model.layers.1.input_layernorm.weight', 0), ('model.layers.1.post_attention_layernorm.weight', 0)]
total trainable params: 0
[META ALL] 0 tensors are on meta (params+buffers)
model class: LlavaLlamaForCausalLM
model name_or_path: liuhaotian/llava-v1.6-vicuna-7b
trainable param groups: 296
top trainables: [('model.image_newline', 0), ('model.embed_tokens.weight', 0), ('model.layers.0.self_attn.q_proj.weight', 0), ('model.layers.0.self_attn.k_proj.weight', 0), ('model.layers.0.self_attn.v_proj.weight', 0), ('model.layers.0.self_attn.o_proj.weight', 0), ('model.layers.0.mlp.gate_proj.weight', 0), ('model.layers.0.mlp.up_proj.weight', 0), ('model.layers.0.mlp.down_proj.weight', 0), ('model.layers.0.input_layernorm.weight', 0), ('model.layers.0.post_attention_layernorm.weight', 0), ('model.layers.1.self_attn.q_proj.weight', 0), ('model.layers.1.self_attn.k_proj.weight', 0), ('model.layers.1.self_attn.v_proj.weight', 0), ('model.layers.1.self_attn.o_proj.weight', 0), ('model.layers.1.mlp.gate_proj.weight', 0), ('model.layers.1.mlp.up_proj.weight', 0), ('model.layers.1.mlp.down_proj.weight', 0), ('model.layers.1.input_layernorm.weight', 0), ('model.layers.1.post_attention_layernorm.weight', 0)]
total trainable params: 0
[META ALL] 0 tensors are on meta (params+buffers)
model class: LlavaLlamaForCausalLM
model name_or_path: liuhaotian/llava-v1.6-vicuna-7b
trainable param groups: 296
top trainables: [('model.image_newline', 0), ('model.embed_tokens.weight', 0), ('model.layers.0.self_attn.q_proj.weight', 0), ('model.layers.0.self_attn.k_proj.weight', 0), ('model.layers.0.self_attn.v_proj.weight', 0), ('model.layers.0.self_attn.o_proj.weight', 0), ('model.layers.0.mlp.gate_proj.weight', 0), ('model.layers.0.mlp.up_proj.weight', 0), ('model.layers.0.mlp.down_proj.weight', 0), ('model.layers.0.input_layernorm.weight', 0), ('model.layers.0.post_attention_layernorm.weight', 0), ('model.layers.1.self_attn.q_proj.weight', 0), ('model.layers.1.self_attn.k_proj.weight', 0), ('model.layers.1.self_attn.v_proj.weight', 0), ('model.layers.1.self_attn.o_proj.weight', 0), ('model.layers.1.mlp.gate_proj.weight', 0), ('model.layers.1.mlp.up_proj.weight', 0), ('model.layers.1.mlp.down_proj.weight', 0), ('model.layers.1.input_layernorm.weight', 0), ('model.layers.1.post_attention_layernorm.weight', 0)]
total trainable params: 0
Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.32s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.84s/it]
[META ALL] 0 tensors are on meta (params+buffers)
model class: LlavaLlamaForCausalLM
model name_or_path: liuhaotian/llava-v1.6-vicuna-7b
trainable param groups: 296
top trainables: [('model.image_newline', 0), ('model.embed_tokens.weight', 0), ('model.layers.0.self_attn.q_proj.weight', 0), ('model.layers.0.self_attn.k_proj.weight', 0), ('model.layers.0.self_attn.v_proj.weight', 0), ('model.layers.0.self_attn.o_proj.weight', 0), ('model.layers.0.mlp.gate_proj.weight', 0), ('model.layers.0.mlp.up_proj.weight', 0), ('model.layers.0.mlp.down_proj.weight', 0), ('model.layers.0.input_layernorm.weight', 0), ('model.layers.0.post_attention_layernorm.weight', 0), ('model.layers.1.self_attn.q_proj.weight', 0), ('model.layers.1.self_attn.k_proj.weight', 0), ('model.layers.1.self_attn.v_proj.weight', 0), ('model.layers.1.self_attn.o_proj.weight', 0), ('model.layers.1.mlp.gate_proj.weight', 0), ('model.layers.1.mlp.up_proj.weight', 0), ('model.layers.1.mlp.down_proj.weight', 0), ('model.layers.1.input_layernorm.weight', 0), ('model.layers.1.post_attention_layernorm.weight', 0)]
total trainable params: 0
Adding LoRA adapters...
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
image_aspect_ratio: anyres
mm_patch_merge_type: spatial_unpad
image_grid_pinpoints: [[336,336],[336,672],[672,336]]
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Formatting inputs...Skip in lazy mode
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Parameter Offload: Total persistent parameters: 603136 in 313 params
  0%|          | 0/168 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[2026-02-03 14:40:01,974] [WARNING] [stage3.py:1991:step] 584 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 1/168 [15:17<42:33:36, 917.46s/it]                                                   {'loss': 3.5348, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.02}
  1%|          | 1/168 [15:17<42:33:36, 917.46s/it][2026-02-03 14:55:41,427] [WARNING] [stage3.py:1991:step] 611 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 2/168 [30:56<42:54:06, 930.40s/it]                                                   {'loss': 3.5806, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.04}
  1%|          | 2/168 [30:56<42:54:06, 930.40s/it][2026-02-03 15:11:24,555] [WARNING] [stage3.py:1991:step] 636 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 3/168 [46:40<42:54:36, 936.22s/it]                                                   {'loss': 1.7659, 'learning_rate': 0.0001, 'epoch': 0.05}
  2%|▏         | 3/168 [46:40<42:54:36, 936.22s/it][2026-02-03 15:27:27,743] [WARNING] [stage3.py:1991:step] 648 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 4/168 [1:02:43<43:08:04, 946.86s/it]                                                     {'loss': 0.5304, 'learning_rate': 0.00013333333333333334, 'epoch': 0.07}
  2%|▏         | 4/168 [1:02:43<43:08:04, 946.86s/it][2026-02-03 15:43:10,616] [WARNING] [stage3.py:1991:step] 648 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 5/168 [1:18:26<42:48:23, 945.42s/it]                                                     {'loss': 0.5373, 'learning_rate': 0.0001666666666666667, 'epoch': 0.09}
  3%|▎         | 5/168 [1:18:26<42:48:23, 945.42s/it][2026-02-03 15:58:52,836] [WARNING] [stage3.py:1991:step] 659 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▎         | 6/168 [1:34:08<42:29:41, 944.33s/it]                                                     {'loss': 0.2833, 'learning_rate': 0.0002, 'epoch': 0.11}
  4%|▎         | 6/168 [1:34:08<42:29:41, 944.33s/it][2026-02-03 16:14:33,729] [WARNING] [stage3.py:1991:step] 583 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 7/168 [1:49:49<42:10:57, 943.22s/it]                                                     {'loss': 0.2604, 'learning_rate': 0.00019998119704485014, 'epoch': 0.12}
  4%|▍         | 7/168 [1:49:49<42:10:57, 943.22s/it][2026-02-03 16:30:13,145] [WARNING] [stage3.py:1991:step] 575 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▍         | 8/168 [2:05:28<41:51:59, 942.00s/it]                                                     {'loss': 0.2868, 'learning_rate': 0.00019992479525042303, 'epoch': 0.14}
  5%|▍         | 8/168 [2:05:28<41:51:59, 942.00s/it][2026-02-03 16:45:54,918] [WARNING] [stage3.py:1991:step] 708 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▌         | 9/168 [2:21:10<41:36:06, 941.93s/it]                                                     {'loss': 0.1435, 'learning_rate': 0.00019983081582712685, 'epoch': 0.16}
  5%|▌         | 9/168 [2:21:10<41:36:06, 941.93s/it][2026-02-03 17:01:38,222] [WARNING] [stage3.py:1991:step] 627 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 10/168 [2:36:53<41:21:33, 942.36s/it]                                                      {'loss': 0.1441, 'learning_rate': 0.0001996992941167792, 'epoch': 0.18}
  6%|▌         | 10/168 [2:36:53<41:21:33, 942.36s/it][2026-02-03 17:17:18,894] [WARNING] [stage3.py:1991:step] 684 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 11/168 [2:52:34<41:04:28, 941.84s/it]                                                      {'loss': 0.1253, 'learning_rate': 0.00019953027957931658, 'epoch': 0.2}
  7%|▋         | 11/168 [2:52:34<41:04:28, 941.84s/it][2026-02-03 17:33:11,836] [WARNING] [stage3.py:1991:step] 628 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 12/168 [3:08:27<40:57:33, 945.21s/it]                                                      {'loss': 0.1265, 'learning_rate': 0.00019932383577419432, 'epoch': 0.21}
  7%|▋         | 12/168 [3:08:27<40:57:33, 945.21s/it][2026-02-03 17:48:51,976] [WARNING] [stage3.py:1991:step] 550 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 13/168 [3:24:07<40:37:49, 943.68s/it]                                                      {'loss': 0.0789, 'learning_rate': 0.00019908004033648453, 'epoch': 0.23}
  8%|▊         | 13/168 [3:24:07<40:37:49, 943.68s/it][2026-02-03 18:04:24,145] [WARNING] [stage3.py:1991:step] 652 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 14/168 [3:39:39<40:13:12, 940.21s/it]                                                      {'loss': 0.1285, 'learning_rate': 0.00019879898494768093, 'epoch': 0.25}
  8%|▊         | 14/168 [3:39:39<40:13:12, 940.21s/it][2026-02-03 18:20:15,925] [WARNING] [stage3.py:1991:step] 676 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 15/168 [3:55:31<40:06:24, 943.69s/it]                                                      {'loss': 0.097, 'learning_rate': 0.00019848077530122083, 'epoch': 0.27}
  9%|▉         | 15/168 [3:55:31<40:06:24, 943.69s/it][2026-02-03 18:35:57,574] [WARNING] [stage3.py:1991:step] 724 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|▉         | 16/168 [4:11:13<39:49:07, 943.07s/it]                                                      {'loss': 0.0811, 'learning_rate': 0.00019812553106273847, 'epoch': 0.28}
 10%|▉         | 16/168 [4:11:13<39:49:07, 943.07s/it][2026-02-03 18:51:29,776] [WARNING] [stage3.py:1991:step] 669 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|█         | 17/168 [4:26:45<39:25:10, 939.81s/it]                                                      {'loss': 0.0863, 'learning_rate': 0.0001977333858250636, 'epoch': 0.3}
 10%|█         | 17/168 [4:26:45<39:25:10, 939.81s/it][2026-02-03 19:07:08,425] [WARNING] [stage3.py:1991:step] 693 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 18/168 [4:42:23<39:08:40, 939.47s/it]                                                      {'loss': 0.058, 'learning_rate': 0.00019730448705798239, 'epoch': 0.32}
 11%|█         | 18/168 [4:42:23<39:08:40, 939.47s/it][2026-02-03 19:22:50,454] [WARNING] [stage3.py:1991:step] 680 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█▏        | 19/168 [4:58:05<38:54:53, 940.23s/it]                                                      {'loss': 0.0632, 'learning_rate': 0.0001968389960527806, 'epoch': 0.34}
 11%|█▏        | 19/168 [4:58:05<38:54:53, 940.23s/it][2026-02-03 19:38:32,041] [WARNING] [stage3.py:1991:step] 655 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 20/168 [5:13:47<38:40:13, 940.64s/it]                                                      {'loss': 0.071, 'learning_rate': 0.00019633708786158806, 'epoch': 0.35}
 12%|█▏        | 20/168 [5:13:47<38:40:13, 940.64s/it][2026-02-03 19:54:11,895] [WARNING] [stage3.py:1991:step] 621 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▎        | 21/168 [5:29:27<38:23:58, 940.40s/it]                                                      {'loss': 0.0977, 'learning_rate': 0.0001957989512315489, 'epoch': 0.37}
 12%|█▎        | 21/168 [5:29:27<38:23:58, 940.40s/it][2026-02-03 20:09:45,673] [WARNING] [stage3.py:1991:step] 595 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 22/168 [5:45:01<38:03:29, 938.42s/it]                                                      {'loss': 0.0673, 'learning_rate': 0.00019522478853384155, 'epoch': 0.39}
 13%|█▎        | 22/168 [5:45:01<38:03:29, 938.42s/it][2026-02-03 20:25:36,151] [WARNING] [stage3.py:1991:step] 628 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▎        | 23/168 [6:00:51<37:56:34, 942.03s/it]                                                      {'loss': 0.0509, 'learning_rate': 0.00019461481568757506, 'epoch': 0.41}
 14%|█▎        | 23/168 [6:00:51<37:56:34, 942.03s/it][2026-02-03 20:41:07,130] [WARNING] [stage3.py:1991:step] 634 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 24/168 [6:16:22<37:32:54, 938.71s/it]                                                      {'loss': 0.0589, 'learning_rate': 0.00019396926207859084, 'epoch': 0.43}
 14%|█▍        | 24/168 [6:16:22<37:32:54, 938.71s/it][2026-02-03 20:56:56,439] [WARNING] [stage3.py:1991:step] 594 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▍        | 25/168 [6:32:11<37:24:50, 941.89s/it]                                                      {'loss': 0.0976, 'learning_rate': 0.0001932883704732001, 'epoch': 0.44}
 15%|█▍        | 25/168 [6:32:11<37:24:50, 941.89s/it][2026-02-03 21:12:41,966] [WARNING] [stage3.py:1991:step] 699 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▌        | 26/168 [6:47:57<37:11:44, 942.99s/it]                                                      {'loss': 0.0764, 'learning_rate': 0.00019257239692688907, 'epoch': 0.46}
 15%|█▌        | 26/168 [6:47:57<37:11:44, 942.99s/it][2026-02-03 21:28:23,505] [WARNING] [stage3.py:1991:step] 644 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 27/168 [7:03:38<36:54:59, 942.55s/it]                                                      {'loss': 0.0608, 'learning_rate': 0.00019182161068802741, 'epoch': 0.48}
 16%|█▌        | 27/168 [7:03:38<36:54:59, 942.55s/it][2026-02-03 21:44:03,925] [WARNING] [stage3.py:1991:step] 657 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 28/168 [7:19:19<36:37:47, 941.91s/it]                                                      {'loss': 0.0416, 'learning_rate': 0.0001910362940966147, 'epoch': 0.5}
 17%|█▋        | 28/168 [7:19:19<36:37:47, 941.91s/it][2026-02-03 21:59:43,118] [WARNING] [stage3.py:1991:step] 586 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 29/168 [7:34:58<36:20:12, 941.09s/it]                                                      {'loss': 0.059, 'learning_rate': 0.0001902167424781038, 'epoch': 0.51}
 17%|█▋        | 29/168 [7:34:58<36:20:12, 941.09s/it][2026-02-03 22:15:21,135] [WARNING] [stage3.py:1991:step] 621 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 30/168 [7:50:36<36:02:23, 940.17s/it]                                                      {'loss': 0.0432, 'learning_rate': 0.00018936326403234125, 'epoch': 0.53}
 18%|█▊        | 30/168 [7:50:36<36:02:23, 940.17s/it][2026-02-03 22:31:03,046] [WARNING] [stage3.py:1991:step] 691 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 31/168 [8:06:18<35:47:54, 940.69s/it]                                                      {'loss': 0.0533, 'learning_rate': 0.00018847617971766577, 'epoch': 0.55}
 18%|█▊        | 31/168 [8:06:18<35:47:54, 940.69s/it][2026-02-03 22:46:34,438] [WARNING] [stage3.py:1991:step] 638 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 32/168 [8:21:49<35:25:54, 937.90s/it]                                                      {'loss': 0.0559, 'learning_rate': 0.0001875558231302091, 'epoch': 0.57}
 19%|█▉        | 32/168 [8:21:49<35:25:54, 937.90s/it][2026-02-03 23:02:17,827] [WARNING] [stage3.py:1991:step] 656 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|█▉        | 33/168 [8:37:33<35:14:00, 939.56s/it]                                                      {'loss': 0.0438, 'learning_rate': 0.00018660254037844388, 'epoch': 0.59}
 20%|█▉        | 33/168 [8:37:33<35:14:00, 939.56s/it][2026-02-03 23:18:01,778] [WARNING] [stage3.py:1991:step] 743 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|██        | 34/168 [8:53:17<35:01:16, 940.87s/it]                                                      {'loss': 0.055, 'learning_rate': 0.00018561668995302667, 'epoch': 0.6}
 20%|██        | 34/168 [8:53:17<35:01:16, 940.87s/it][2026-02-03 23:33:52,705] [WARNING] [stage3.py:1991:step] 650 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 35/168 [9:09:08<34:52:16, 943.88s/it]                                                      {'loss': 0.0521, 'learning_rate': 0.0001845986425919841, 'epoch': 0.62}
 21%|██        | 35/168 [9:09:08<34:52:16, 943.88s/it][2026-02-03 23:49:42,878] [WARNING] [stage3.py:1991:step] 609 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██▏       | 36/168 [9:24:58<34:40:41, 945.77s/it]                                                      {'loss': 0.0413, 'learning_rate': 0.00018354878114129367, 'epoch': 0.64}
 21%|██▏       | 36/168 [9:24:58<34:40:41, 945.77s/it][2026-02-04 00:05:16,541] [WARNING] [stage3.py:1991:step] 546 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 37/168 [9:40:32<34:17:01, 942.15s/it]                                                      {'loss': 0.0311, 'learning_rate': 0.0001824675004109107, 'epoch': 0.66}
 22%|██▏       | 37/168 [9:40:32<34:17:01, 942.15s/it][2026-02-04 00:20:46,405] [WARNING] [stage3.py:1991:step] 620 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 38/168 [9:56:01<33:53:18, 938.45s/it]                                                      {'loss': 0.0342, 'learning_rate': 0.00018135520702629675, 'epoch': 0.67}
 23%|██▎       | 38/168 [9:56:01<33:53:18, 938.45s/it][2026-02-04 00:36:27,066] [WARNING] [stage3.py:1991:step] 700 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 39/168 [10:11:42<33:39:05, 939.12s/it]                                                       {'loss': 0.0361, 'learning_rate': 0.0001802123192755044, 'epoch': 0.69}
 23%|██▎       | 39/168 [10:11:42<33:39:05, 939.12s/it][2026-02-04 00:51:59,503] [WARNING] [stage3.py:1991:step] 632 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 40/168 [10:27:14<33:19:10, 937.11s/it]                                                       {'loss': 0.0297, 'learning_rate': 0.00017903926695187595, 'epoch': 0.71}
 24%|██▍       | 40/168 [10:27:14<33:19:10, 937.11s/it][2026-02-04 01:07:43,895] [WARNING] [stage3.py:1991:step] 647 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 41/168 [10:42:59<33:08:11, 939.30s/it]                                                       {'loss': 0.0581, 'learning_rate': 0.00017783649119241602, 'epoch': 0.73}
 24%|██▍       | 41/168 [10:42:59<33:08:11, 939.30s/it][2026-02-04 01:23:32,176] [WARNING] [stage3.py:1991:step] 677 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▌       | 42/168 [10:58:47<32:58:10, 941.99s/it]                                                       {'loss': 0.0437, 'learning_rate': 0.0001766044443118978, 'epoch': 0.75}
 25%|██▌       | 42/168 [10:58:47<32:58:10, 941.99s/it][2026-02-04 01:39:16,858] [WARNING] [stage3.py:1991:step] 667 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 43/168 [11:14:32<32:44:09, 942.80s/it]                                                       {'loss': 0.0369, 'learning_rate': 0.00017534358963276607, 'epoch': 0.76}
 26%|██▌       | 43/168 [11:14:32<32:44:09, 942.80s/it][2026-02-04 01:54:56,593] [WARNING] [stage3.py:1991:step] 686 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 44/168 [11:30:12<32:26:32, 941.88s/it]                                                       {'loss': 0.0371, 'learning_rate': 0.00017405440131090048, 'epoch': 0.78}
 26%|██▌       | 44/168 [11:30:12<32:26:32, 941.88s/it][2026-02-04 02:10:27,449] [WARNING] [stage3.py:1991:step] 647 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 45/168 [11:45:42<32:04:05, 938.58s/it]                                                       {'loss': 0.0493, 'learning_rate': 0.00017273736415730488, 'epoch': 0.8}
 27%|██▋       | 45/168 [11:45:42<32:04:05, 938.58s/it][2026-02-04 02:26:09,119] [WARNING] [stage3.py:1991:step] 578 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 46/168 [12:01:24<31:50:18, 939.50s/it]                                                       {'loss': 0.0407, 'learning_rate': 0.00017139297345578994, 'epoch': 0.82}
 27%|██▋       | 46/168 [12:01:24<31:50:18, 939.50s/it][2026-02-04 02:42:00,061] [WARNING] [stage3.py:1991:step] 697 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 47/168 [12:17:15<31:41:34, 942.93s/it]                                                       {'loss': 0.0402, 'learning_rate': 0.00017002173477671686, 'epoch': 0.83}
 28%|██▊       | 47/168 [12:17:15<31:41:34, 942.93s/it][2026-02-04 02:57:41,523] [WARNING] [stage3.py:1991:step] 633 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▊       | 48/168 [12:32:57<31:24:58, 942.49s/it]                                                       {'loss': 0.0316, 'learning_rate': 0.0001686241637868734, 'epoch': 0.85}
 29%|██▊       | 48/168 [12:32:57<31:24:58, 942.49s/it][2026-02-04 03:13:11,829] [WARNING] [stage3.py:1991:step] 625 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 49/168 [12:48:27<31:02:02, 938.84s/it]                                                       {'loss': 0.0217, 'learning_rate': 0.00016720078605555224, 'epoch': 0.87}
 29%|██▉       | 49/168 [12:48:27<31:02:02, 938.84s/it][2026-02-04 03:28:42,345] [WARNING] [stage3.py:1991:step] 632 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|██▉       | 50/168 [13:03:57<30:41:27, 936.34s/it]                                                       {'loss': 0.0243, 'learning_rate': 0.0001657521368569064, 'epoch': 0.89}
 30%|██▉       | 50/168 [13:03:57<30:41:27, 936.34s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[2026-02-04 03:45:10,860] [WARNING] [stage3.py:1991:step] 631 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|███       | 51/168 [13:20:26<30:56:22, 951.99s/it]                                                       {'loss': 0.0364, 'learning_rate': 0.00016427876096865394, 'epoch': 0.9}
 30%|███       | 51/168 [13:20:26<30:56:22, 951.99s/it][2026-02-04 04:00:41,780] [WARNING] [stage3.py:1991:step] 617 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 52/168 [13:35:57<30:28:18, 945.68s/it]                                                       {'loss': 0.0465, 'learning_rate': 0.00016278121246720987, 'epoch': 0.92}
 31%|███       | 52/168 [13:35:57<30:28:18, 945.68s/it][2026-02-04 04:16:13,577] [WARNING] [stage3.py:1991:step] 682 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 53/168 [13:51:29<30:04:33, 941.51s/it]                                                       {'loss': 0.0256, 'learning_rate': 0.0001612600545193203, 'epoch': 0.94}
 32%|███▏      | 53/168 [13:51:29<30:04:33, 941.51s/it][2026-02-04 04:31:54,227] [WARNING] [stage3.py:1991:step] 561 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 54/168 [14:07:09<29:48:22, 941.25s/it]                                                       {'loss': 0.0299, 'learning_rate': 0.00015971585917027862, 'epoch': 0.96}
 32%|███▏      | 54/168 [14:07:09<29:48:22, 941.25s/it][2026-02-04 04:47:46,995] [WARNING] [stage3.py:1991:step] 635 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 33%|███▎      | 55/168 [14:23:02<29:39:11, 944.70s/it]                                                       {'loss': 0.0316, 'learning_rate': 0.00015814920712880267, 'epoch': 0.98}
 33%|███▎      | 55/168 [14:23:02<29:39:11, 944.70s/it][2026-02-04 05:03:19,181] [WARNING] [stage3.py:1991:step] 654 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 33%|███▎      | 56/168 [14:38:34<29:16:26, 940.96s/it]                                                       {'loss': 0.0302, 'learning_rate': 0.00015656068754865387, 'epoch': 0.99}
 33%|███▎      | 56/168 [14:38:34<29:16:26, 940.96s/it][2026-02-04 05:18:51,633] [WARNING] [stage3.py:1991:step] 656 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 34%|███▍      | 57/168 [14:54:07<28:56:02, 938.40s/it]                                                       {'loss': 0.0373, 'learning_rate': 0.0001549508978070806, 'epoch': 1.01}
 34%|███▍      | 57/168 [14:54:07<28:56:02, 938.40s/it][2026-02-04 05:34:42,632] [WARNING] [stage3.py:1991:step] 656 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 35%|███▍      | 58/168 [15:09:58<28:47:19, 942.18s/it]                                                       {'loss': 0.0312, 'learning_rate': 0.00015332044328016914, 'epoch': 1.03}
 35%|███▍      | 58/168 [15:09:58<28:47:19, 942.18s/it][2026-02-04 05:50:25,162] [WARNING] [stage3.py:1991:step] 611 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 35%|███▌      | 59/168 [15:25:40<28:31:48, 942.28s/it]                                                       {'loss': 0.034, 'learning_rate': 0.00015166993711518631, 'epoch': 1.05}
 35%|███▌      | 59/168 [15:25:40<28:31:48, 942.28s/it][2026-02-04 06:06:06,041] [WARNING] [stage3.py:1991:step] 616 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 36%|███▌      | 60/168 [15:41:21<28:15:22, 941.87s/it]                                                       {'loss': 0.0198, 'learning_rate': 0.00015000000000000001, 'epoch': 1.06}
 36%|███▌      | 60/168 [15:41:21<28:15:22, 941.87s/it][2026-02-04 06:21:48,112] [WARNING] [stage3.py:1991:step] 620 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 36%|███▋      | 61/168 [15:57:03<27:59:45, 941.92s/it]                                                       {'loss': 0.0301, 'learning_rate': 0.00014831125992966385, 'epoch': 1.08}
 36%|███▋      | 61/168 [15:57:03<27:59:45, 941.92s/it][2026-02-04 06:37:29,555] [WARNING] [stage3.py:1991:step] 597 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 37%|███▋      | 62/168 [16:12:45<27:43:48, 941.78s/it]                                                       {'loss': 0.0298, 'learning_rate': 0.0001466043519702539, 'epoch': 1.1}
 37%|███▋      | 62/168 [16:12:45<27:43:48, 941.78s/it][2026-02-04 06:53:08,849] [WARNING] [stage3.py:1991:step] 631 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 38%|███▊      | 63/168 [16:28:24<27:26:48, 941.03s/it]                                                       {'loss': 0.0269, 'learning_rate': 0.00014487991802004623, 'epoch': 1.12}
 38%|███▊      | 63/168 [16:28:24<27:26:48, 941.03s/it][2026-02-04 07:08:40,366] [WARNING] [stage3.py:1991:step] 666 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 38%|███▊      | 64/168 [16:43:55<27:06:11, 938.19s/it]                                                       {'loss': 0.018, 'learning_rate': 0.00014313860656812536, 'epoch': 1.14}
 38%|███▊      | 64/168 [16:43:55<27:06:11, 938.19s/it][2026-02-04 07:24:10,308] [WARNING] [stage3.py:1991:step] 630 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 39%|███▊      | 65/168 [16:59:25<26:46:17, 935.71s/it]                                                       {'loss': 0.023, 'learning_rate': 0.00014138107245051392, 'epoch': 1.15}
 39%|███▊      | 65/168 [16:59:25<26:46:17, 935.71s/it][2026-02-04 07:39:51,559] [WARNING] [stage3.py:1991:step] 669 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 39%|███▉      | 66/168 [17:15:07<26:33:31, 937.37s/it]                                                       {'loss': 0.0206, 'learning_rate': 0.0001396079766039157, 'epoch': 1.17}
 39%|███▉      | 66/168 [17:15:07<26:33:31, 937.37s/it][2026-02-04 07:55:32,974] [WARNING] [stage3.py:1991:step] 668 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 40%|███▉      | 67/168 [17:30:48<26:19:56, 938.58s/it]                                                       {'loss': 0.0202, 'learning_rate': 0.00013781998581716427, 'epoch': 1.19}
 40%|███▉      | 67/168 [17:30:48<26:19:56, 938.58s/it][2026-02-04 08:11:16,296] [WARNING] [stage3.py:1991:step] 685 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 40%|████      | 68/168 [17:46:31<26:06:41, 940.02s/it]                                                       {'loss': 0.0337, 'learning_rate': 0.00013601777248047105, 'epoch': 1.21}
 40%|████      | 68/168 [17:46:31<26:06:41, 940.02s/it][2026-02-04 08:27:07,705] [WARNING] [stage3.py:1991:step] 692 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 41%|████      | 69/168 [18:02:23<25:56:38, 943.42s/it]                                                       {'loss': 0.0124, 'learning_rate': 0.00013420201433256689, 'epoch': 1.22}
 41%|████      | 69/168 [18:02:23<25:56:38, 943.42s/it][2026-02-04 08:42:48,470] [WARNING] [stage3.py:1991:step] 557 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 42%|████▏     | 70/168 [18:18:03<25:39:37, 942.63s/it]                                                       {'loss': 0.0218, 'learning_rate': 0.00013237339420583212, 'epoch': 1.24}
 42%|████▏     | 70/168 [18:18:03<25:39:37, 942.63s/it][2026-02-04 08:58:41,069] [WARNING] [stage3.py:1991:step] 659 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 42%|████▏     | 71/168 [18:33:56<25:28:44, 945.62s/it]                                                       {'loss': 0.0164, 'learning_rate': 0.00013053259976951133, 'epoch': 1.26}
 42%|████▏     | 71/168 [18:33:56<25:28:44, 945.62s/it][2026-02-04 09:14:14,994] [WARNING] [stage3.py:1991:step] 609 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 43%|████▎     | 72/168 [18:49:30<25:07:23, 942.12s/it]                                                       {'loss': 0.0333, 'learning_rate': 0.00012868032327110904, 'epoch': 1.28}
 43%|████▎     | 72/168 [18:49:30<25:07:23, 942.12s/it][2026-02-04 09:30:04,773] [WARNING] [stage3.py:1991:step] 637 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 43%|████▎     | 73/168 [19:05:20<24:55:18, 944.41s/it]                                                       {'loss': 0.0116, 'learning_rate': 0.00012681726127606376, 'epoch': 1.29}
 43%|████▎     | 73/168 [19:05:20<24:55:18, 944.41s/it][2026-02-04 09:45:46,665] [WARNING] [stage3.py:1991:step] 653 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 44%|████▍     | 74/168 [19:21:02<24:38:23, 943.66s/it]                                                       {'loss': 0.0369, 'learning_rate': 0.00012494411440579814, 'epoch': 1.31}
 44%|████▍     | 74/168 [19:21:02<24:38:23, 943.66s/it][2026-02-04 10:01:19,664] [WARNING] [stage3.py:1991:step] 625 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 45%|████▍     | 75/168 [19:36:35<24:17:43, 940.47s/it]                                                       {'loss': 0.0294, 'learning_rate': 0.00012306158707424403, 'epoch': 1.33}
 45%|████▍     | 75/168 [19:36:35<24:17:43, 940.47s/it][2026-02-04 10:17:11,313] [WARNING] [stage3.py:1991:step] 618 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 45%|████▌     | 76/168 [19:52:26<24:07:10, 943.81s/it]                                                       {'loss': 0.0231, 'learning_rate': 0.0001211703872229411, 'epoch': 1.35}
 45%|████▌     | 76/168 [19:52:26<24:07:10, 943.81s/it][2026-02-04 10:32:54,157] [WARNING] [stage3.py:1991:step] 679 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 46%|████▌     | 77/168 [20:08:09<23:51:00, 943.52s/it]                                                       {'loss': 0.0228, 'learning_rate': 0.00011927122605480898, 'epoch': 1.37}
 46%|████▌     | 77/168 [20:08:09<23:51:00, 943.52s/it][2026-02-04 10:48:45,654] [WARNING] [stage3.py:1991:step] 650 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 46%|████▋     | 78/168 [20:24:01<23:38:52, 945.91s/it]                                                       {'loss': 0.0099, 'learning_rate': 0.00011736481776669306, 'epoch': 1.38}
 46%|████▋     | 78/168 [20:24:01<23:38:52, 945.91s/it][2026-02-04 11:04:20,588] [WARNING] [stage3.py:1991:step] 663 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 47%|████▋     | 79/168 [20:39:36<23:18:13, 942.62s/it]                                                       {'loss': 0.0293, 'learning_rate': 0.00011545187928078406, 'epoch': 1.4}
 47%|████▋     | 79/168 [20:39:36<23:18:13, 942.62s/it][2026-02-04 11:20:12,039] [WARNING] [stage3.py:1991:step] 735 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 48%|████▊     | 80/168 [20:55:27<23:06:23, 945.27s/it]                                                       {'loss': 0.018, 'learning_rate': 0.00011353312997501313, 'epoch': 1.42}
 48%|████▊     | 80/168 [20:55:27<23:06:23, 945.27s/it][2026-02-04 11:35:53,574] [WARNING] [stage3.py:1991:step] 668 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 48%|████▊     | 81/168 [21:11:09<22:49:00, 944.15s/it]                                                       {'loss': 0.0275, 'learning_rate': 0.00011160929141252303, 'epoch': 1.44}
 48%|████▊     | 81/168 [21:11:09<22:49:00, 944.15s/it][2026-02-04 11:51:22,883] [WARNING] [stage3.py:1991:step] 571 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 49%|████▉     | 82/168 [21:26:38<22:26:53, 939.70s/it]                                                       {'loss': 0.0273, 'learning_rate': 0.00010968108707031792, 'epoch': 1.45}
 49%|████▉     | 82/168 [21:26:38<22:26:53, 939.70s/it][2026-02-04 12:07:05,300] [WARNING] [stage3.py:1991:step] 693 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 49%|████▉     | 83/168 [21:42:20<22:12:23, 940.52s/it]                                                       {'loss': 0.0237, 'learning_rate': 0.0001077492420671931, 'epoch': 1.47}
 49%|████▉     | 83/168 [21:42:20<22:12:23, 940.52s/it][2026-02-04 12:22:57,742] [WARNING] [stage3.py:1991:step] 697 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 50%|█████     | 84/168 [21:58:13<22:01:43, 944.09s/it]                                                       {'loss': 0.0193, 'learning_rate': 0.00010581448289104758, 'epoch': 1.49}
 50%|█████     | 84/168 [21:58:13<22:01:43, 944.09s/it][2026-02-04 12:38:40,618] [WARNING] [stage3.py:1991:step] 708 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 51%|█████     | 85/168 [22:13:56<21:45:29, 943.73s/it]                                                       {'loss': 0.0252, 'learning_rate': 0.0001038775371256817, 'epoch': 1.51}
 51%|█████     | 85/168 [22:13:56<21:45:29, 943.73s/it][2026-02-04 12:54:20,189] [WARNING] [stage3.py:1991:step] 650 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 51%|█████     | 86/168 [22:29:35<21:28:03, 942.48s/it]                                                       {'loss': 0.0269, 'learning_rate': 0.00010193913317718244, 'epoch': 1.53}
 51%|█████     | 86/168 [22:29:35<21:28:03, 942.48s/it][2026-02-04 13:09:50,260] [WARNING] [stage3.py:1991:step] 576 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 52%|█████▏    | 87/168 [22:45:05<21:07:19, 938.76s/it]                                                       {'loss': 0.0228, 'learning_rate': 0.0001, 'epoch': 1.54}
 52%|█████▏    | 87/168 [22:45:05<21:07:19, 938.76s/it][2026-02-04 13:25:34,297] [WARNING] [stage3.py:1991:step] 662 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 52%|█████▏    | 88/168 [23:00:49<20:53:47, 940.34s/it]                                                       {'loss': 0.0221, 'learning_rate': 9.806086682281758e-05, 'epoch': 1.56}
 52%|█████▏    | 88/168 [23:00:49<20:53:47, 940.34s/it][2026-02-04 13:41:13,022] [WARNING] [stage3.py:1991:step] 607 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 53%|█████▎    | 89/168 [23:16:28<20:37:28, 939.86s/it]                                                       {'loss': 0.0237, 'learning_rate': 9.612246287431831e-05, 'epoch': 1.58}
 53%|█████▎    | 89/168 [23:16:28<20:37:28, 939.86s/it][2026-02-04 13:57:05,331] [WARNING] [stage3.py:1991:step] 624 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 54%|█████▎    | 90/168 [23:32:20<20:26:40, 943.59s/it]                                                       {'loss': 0.0186, 'learning_rate': 9.418551710895243e-05, 'epoch': 1.6}
 54%|█████▎    | 90/168 [23:32:20<20:26:40, 943.59s/it][2026-02-04 14:12:37,396] [WARNING] [stage3.py:1991:step] 673 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 54%|█████▍    | 91/168 [23:47:52<20:06:31, 940.14s/it]                                                       {'loss': 0.0095, 'learning_rate': 9.225075793280692e-05, 'epoch': 1.61}
 54%|█████▍    | 91/168 [23:47:52<20:06:31, 940.14s/it][2026-02-04 14:28:20,367] [WARNING] [stage3.py:1991:step] 642 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 55%|█████▍    | 92/168 [24:03:35<19:51:54, 940.98s/it]                                                       {'loss': 0.0219, 'learning_rate': 9.03189129296821e-05, 'epoch': 1.63}
 55%|█████▍    | 92/168 [24:03:35<19:51:54, 940.98s/it][2026-02-04 14:43:51,958] [WARNING] [stage3.py:1991:step] 596 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 55%|█████▌    | 93/168 [24:19:07<19:32:42, 938.16s/it]                                                       {'loss': 0.0245, 'learning_rate': 8.839070858747697e-05, 'epoch': 1.65}
 55%|█████▌    | 93/168 [24:19:07<19:32:42, 938.16s/it][2026-02-04 14:59:52,080] [WARNING] [stage3.py:1991:step] 628 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 56%|█████▌    | 94/168 [24:35:07<19:25:11, 944.75s/it]                                                       {'loss': 0.0297, 'learning_rate': 8.646687002498692e-05, 'epoch': 1.67}
 56%|█████▌    | 94/168 [24:35:07<19:25:11, 944.75s/it][2026-02-04 15:15:25,692] [WARNING] [stage3.py:1991:step] 670 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 57%|█████▋    | 95/168 [24:50:41<19:05:22, 941.41s/it]                                                       {'loss': 0.0137, 'learning_rate': 8.454812071921596e-05, 'epoch': 1.69}
 57%|█████▋    | 95/168 [24:50:41<19:05:22, 941.41s/it][2026-02-04 15:31:06,663] [WARNING] [stage3.py:1991:step] 557 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 57%|█████▋    | 96/168 [25:06:22<18:49:32, 941.28s/it]                                                       {'loss': 0.0307, 'learning_rate': 8.263518223330697e-05, 'epoch': 1.7}
 57%|█████▋    | 96/168 [25:06:22<18:49:32, 941.28s/it][2026-02-04 15:46:48,322] [WARNING] [stage3.py:1991:step] 602 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 58%|█████▊    | 97/168 [25:22:03<18:33:58, 941.39s/it]                                                       {'loss': 0.0176, 'learning_rate': 8.072877394519102e-05, 'epoch': 1.72}
 58%|█████▊    | 97/168 [25:22:03<18:33:58, 941.39s/it][2026-02-04 16:02:32,591] [WARNING] [stage3.py:1991:step] 635 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 58%|█████▊    | 98/168 [25:37:48<18:19:18, 942.26s/it]                                                       {'loss': 0.0246, 'learning_rate': 7.882961277705895e-05, 'epoch': 1.74}
 58%|█████▊    | 98/168 [25:37:48<18:19:18, 942.26s/it][2026-02-04 16:18:12,972] [WARNING] [stage3.py:1991:step] 633 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 59%|█████▉    | 99/168 [25:53:28<18:02:56, 941.69s/it]                                                       {'loss': 0.0131, 'learning_rate': 7.693841292575598e-05, 'epoch': 1.76}
 59%|█████▉    | 99/168 [25:53:28<18:02:56, 941.69s/it][2026-02-04 16:33:56,222] [WARNING] [stage3.py:1991:step] 643 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 60%|█████▉    | 100/168 [26:09:11<17:47:46, 942.16s/it]                                                        {'loss': 0.0222, 'learning_rate': 7.505588559420189e-05, 'epoch': 1.77}
 60%|█████▉    | 100/168 [26:09:11<17:47:46, 942.16s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[2026-02-04 16:50:12,657] [WARNING] [stage3.py:1991:step] 677 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 60%|██████    | 101/168 [26:25:28<17:43:33, 952.44s/it]                                                        {'loss': 0.023, 'learning_rate': 7.318273872393625e-05, 'epoch': 1.79}
 60%|██████    | 101/168 [26:25:28<17:43:33, 952.44s/it][2026-02-04 17:05:44,991] [WARNING] [stage3.py:1991:step] 635 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 61%|██████    | 102/168 [26:41:00<17:21:03, 946.42s/it]                                                        {'loss': 0.0144, 'learning_rate': 7.131967672889101e-05, 'epoch': 1.81}
 61%|██████    | 102/168 [26:41:00<17:21:03, 946.42s/it][2026-02-04 17:21:14,019] [WARNING] [stage3.py:1991:step] 610 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 61%|██████▏   | 103/168 [26:56:29<16:59:37, 941.19s/it]                                                        {'loss': 0.0158, 'learning_rate': 6.94674002304887e-05, 'epoch': 1.83}
 61%|██████▏   | 103/168 [26:56:29<16:59:37, 941.19s/it][2026-02-04 17:36:54,961] [WARNING] [stage3.py:1991:step] 629 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 62%|██████▏   | 104/168 [27:12:10<16:43:51, 941.12s/it]                                                        {'loss': 0.0141, 'learning_rate': 6.762660579416791e-05, 'epoch': 1.84}
 62%|██████▏   | 104/168 [27:12:10<16:43:51, 941.12s/it][2026-02-04 17:52:46,477] [WARNING] [stage3.py:1991:step] 660 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 62%|██████▎   | 105/168 [27:28:01<16:31:26, 944.24s/it]                                                        {'loss': 0.0271, 'learning_rate': 6.579798566743314e-05, 'epoch': 1.86}
 62%|██████▎   | 105/168 [27:28:01<16:31:26, 944.24s/it][2026-02-04 18:08:29,154] [WARNING] [stage3.py:1991:step] 684 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 63%|██████▎   | 106/168 [27:43:44<16:15:14, 943.78s/it]                                                        {'loss': 0.0165, 'learning_rate': 6.398222751952899e-05, 'epoch': 1.88}
 63%|██████▎   | 106/168 [27:43:44<16:15:14, 943.78s/it][2026-02-04 18:24:22,956] [WARNING] [stage3.py:1991:step] 721 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 64%|██████▎   | 107/168 [27:59:38<16:02:33, 946.78s/it]                                                        {'loss': 0.0212, 'learning_rate': 6.218001418283576e-05, 'epoch': 1.9}
 64%|██████▎   | 107/168 [27:59:38<16:02:33, 946.78s/it][2026-02-04 18:40:03,129] [WARNING] [stage3.py:1991:step] 655 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 64%|██████▍   | 108/168 [28:15:18<15:44:47, 944.80s/it]                                                        {'loss': 0.0222, 'learning_rate': 6.039202339608432e-05, 'epoch': 1.92}
 64%|██████▍   | 108/168 [28:15:18<15:44:47, 944.80s/it][2026-02-04 18:55:54,823] [WARNING] [stage3.py:1991:step] 618 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 65%|██████▍   | 109/168 [28:31:10<15:31:05, 946.87s/it]                                                        {'loss': 0.0236, 'learning_rate': 5.861892754948609e-05, 'epoch': 1.93}
 65%|██████▍   | 109/168 [28:31:10<15:31:05, 946.87s/it][2026-02-04 19:11:36,336] [WARNING] [stage3.py:1991:step] 668 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 65%|██████▌   | 110/168 [28:46:51<15:13:45, 945.27s/it]                                                        {'loss': 0.018, 'learning_rate': 5.6861393431874675e-05, 'epoch': 1.95}
 65%|██████▌   | 110/168 [28:46:51<15:13:45, 945.27s/it][2026-02-04 19:27:18,269] [WARNING] [stage3.py:1991:step] 620 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 66%|██████▌   | 111/168 [29:02:33<14:57:02, 944.26s/it]                                                        {'loss': 0.0191, 'learning_rate': 5.5120081979953785e-05, 'epoch': 1.97}
 66%|██████▌   | 111/168 [29:02:33<14:57:02, 944.26s/it][2026-02-04 19:45:29,877] [WARNING] [stage3.py:1991:step] 693 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 67%|██████▋   | 112/168 [29:20:45<15:22:33, 988.46s/it]                                                        {'loss': 0.0223, 'learning_rate': 5.339564802974615e-05, 'epoch': 1.99}
 67%|██████▋   | 112/168 [29:20:45<15:22:33, 988.46s/it][2026-02-04 20:01:25,269] [WARNING] [stage3.py:1991:step] 653 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 67%|██████▋   | 113/168 [29:36:40<14:57:00, 978.55s/it]                                                        {'loss': 0.0124, 'learning_rate': 5.168874007033615e-05, 'epoch': 2.0}
 67%|██████▋   | 113/168 [29:36:40<14:57:00, 978.55s/it][2026-02-04 20:17:08,132] [WARNING] [stage3.py:1991:step] 621 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 68%|██████▊   | 114/168 [29:52:23<14:31:03, 967.84s/it]                                                        {'loss': 0.0194, 'learning_rate': 5.000000000000002e-05, 'epoch': 2.02}
 68%|██████▊   | 114/168 [29:52:23<14:31:03, 967.84s/it][2026-02-04 20:33:27,977] [WARNING] [stage3.py:1991:step] 702 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 68%|██████▊   | 115/168 [30:08:43<14:18:06, 971.44s/it]                                                        {'loss': 0.0171, 'learning_rate': 4.833006288481371e-05, 'epoch': 2.04}
 68%|██████▊   | 115/168 [30:08:43<14:18:06, 971.44s/it][2026-02-04 20:51:56,357] [WARNING] [stage3.py:1991:step] 632 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 69%|██████▉   | 116/168 [30:27:11<14:37:31, 1012.52s/it]                                                         {'loss': 0.0072, 'learning_rate': 4.66795567198309e-05, 'epoch': 2.06}
 69%|██████▉   | 116/168 [30:27:11<14:37:31, 1012.52s/it][2026-02-04 21:07:38,496] [WARNING] [stage3.py:1991:step] 639 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 70%|██████▉   | 117/168 [30:42:54<14:02:42, 991.42s/it]                                                         {'loss': 0.0241, 'learning_rate': 4.50491021929194e-05, 'epoch': 2.08}
 70%|██████▉   | 117/168 [30:42:54<14:02:42, 991.42s/it][2026-02-04 21:23:10,892] [WARNING] [stage3.py:1991:step] 631 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 70%|███████   | 118/168 [30:58:26<13:31:24, 973.70s/it]                                                        {'loss': 0.0208, 'learning_rate': 4.343931245134616e-05, 'epoch': 2.09}
 70%|███████   | 118/168 [30:58:26<13:31:24, 973.70s/it][2026-02-04 21:39:04,767] [WARNING] [stage3.py:1991:step] 630 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 71%|███████   | 119/168 [31:14:20<13:10:19, 967.75s/it]                                                        {'loss': 0.0158, 'learning_rate': 4.185079287119733e-05, 'epoch': 2.11}
 71%|███████   | 119/168 [31:14:20<13:10:19, 967.75s/it][2026-02-04 21:54:56,029] [WARNING] [stage3.py:1991:step] 665 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 71%|███████▏  | 120/168 [31:30:11<12:50:14, 962.81s/it]                                                        {'loss': 0.0155, 'learning_rate': 4.028414082972141e-05, 'epoch': 2.13}
 71%|███████▏  | 120/168 [31:30:11<12:50:14, 962.81s/it][2026-02-04 22:10:30,392] [WARNING] [stage3.py:1991:step] 652 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 72%|███████▏  | 121/168 [31:45:45<12:27:31, 954.28s/it]                                                        {'loss': 0.0164, 'learning_rate': 3.873994548067972e-05, 'epoch': 2.15}
 72%|███████▏  | 121/168 [31:45:45<12:27:31, 954.28s/it][2026-02-04 22:26:12,106] [WARNING] [stage3.py:1991:step] 667 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 73%|███████▎  | 122/168 [32:01:27<12:08:43, 950.50s/it]                                                        {'loss': 0.0164, 'learning_rate': 3.721878753279017e-05, 'epoch': 2.16}
 73%|███████▎  | 122/168 [32:01:27<12:08:43, 950.50s/it][2026-02-04 22:41:50,912] [WARNING] [stage3.py:1991:step] 583 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 73%|███████▎  | 123/168 [32:17:06<11:50:14, 946.99s/it]                                                        {'loss': 0.0103, 'learning_rate': 3.5721239031346066e-05, 'epoch': 2.18}
 73%|███████▎  | 123/168 [32:17:06<11:50:14, 946.99s/it][2026-02-04 22:57:32,997] [WARNING] [stage3.py:1991:step] 661 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 74%|███████▍  | 124/168 [32:32:48<11:33:22, 945.52s/it]                                                        {'loss': 0.0145, 'learning_rate': 3.424786314309365e-05, 'epoch': 2.2}
 74%|███████▍  | 124/168 [32:32:48<11:33:22, 945.52s/it][2026-02-04 23:13:07,224] [WARNING] [stage3.py:1991:step] 664 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 74%|███████▍  | 125/168 [32:48:22<11:15:12, 942.14s/it]                                                        {'loss': 0.0189, 'learning_rate': 3.279921394444776e-05, 'epoch': 2.22}
 74%|███████▍  | 125/168 [32:48:22<11:15:12, 942.14s/it][2026-02-04 23:28:49,652] [WARNING] [stage3.py:1991:step] 690 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 75%|███████▌  | 126/168 [33:04:05<10:59:33, 942.22s/it]                                                        {'loss': 0.013, 'learning_rate': 3.137583621312665e-05, 'epoch': 2.24}
 75%|███████▌  | 126/168 [33:04:05<10:59:33, 942.22s/it][2026-02-04 23:44:40,592] [WARNING] [stage3.py:1991:step] 684 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 76%|███████▌  | 127/168 [33:19:56<10:45:38, 944.83s/it]                                                        {'loss': 0.0142, 'learning_rate': 2.997826522328315e-05, 'epoch': 2.25}
 76%|███████▌  | 127/168 [33:19:56<10:45:38, 944.83s/it][2026-02-05 00:00:23,445] [WARNING] [stage3.py:1991:step] 609 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 76%|███████▌  | 128/168 [33:35:38<10:29:30, 944.25s/it]                                                        {'loss': 0.0138, 'learning_rate': 2.8607026544210114e-05, 'epoch': 2.27}
 76%|███████▌  | 128/168 [33:35:38<10:29:30, 944.25s/it][2026-02-05 00:15:54,445] [WARNING] [stage3.py:1991:step] 596 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 77%|███████▋  | 129/168 [33:51:09<10:11:10, 940.27s/it]                                                        {'loss': 0.017, 'learning_rate': 2.7262635842695127e-05, 'epoch': 2.29}
 77%|███████▋  | 129/168 [33:51:09<10:11:10, 940.27s/it][2026-02-05 00:31:35,638] [WARNING] [stage3.py:1991:step] 621 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 77%|███████▋  | 130/168 [34:06:51<9:55:40, 940.54s/it]                                                        {'loss': 0.0142, 'learning_rate': 2.594559868909956e-05, 'epoch': 2.31}
 77%|███████▋  | 130/168 [34:06:51<9:55:40, 940.54s/it][2026-02-05 00:47:28,662] [WARNING] [stage3.py:1991:step] 690 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 78%|███████▊  | 131/168 [34:22:44<9:42:18, 944.29s/it]                                                       {'loss': 0.0166, 'learning_rate': 2.465641036723393e-05, 'epoch': 2.32}
 78%|███████▊  | 131/168 [34:22:44<9:42:18, 944.29s/it][2026-02-05 01:03:22,490] [WARNING] [stage3.py:1991:step] 669 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 79%|███████▊  | 132/168 [34:38:38<9:28:17, 947.16s/it]                                                       {'loss': 0.0107, 'learning_rate': 2.339555568810221e-05, 'epoch': 2.34}
 79%|███████▊  | 132/168 [34:38:38<9:28:17, 947.16s/it][2026-02-05 01:19:05,095] [WARNING] [stage3.py:1991:step] 692 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 79%|███████▉  | 133/168 [34:54:20<9:11:42, 945.78s/it]                                                       {'loss': 0.0132, 'learning_rate': 2.2163508807583998e-05, 'epoch': 2.36}
 79%|███████▉  | 133/168 [34:54:20<9:11:42, 945.78s/it][2026-02-05 01:34:43,362] [WARNING] [stage3.py:1991:step] 560 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 80%|███████▉  | 134/168 [35:09:58<8:54:39, 943.53s/it]                                                       {'loss': 0.0102, 'learning_rate': 2.0960733048124083e-05, 'epoch': 2.38}
 80%|███████▉  | 134/168 [35:09:58<8:54:39, 943.53s/it][2026-02-05 01:50:24,128] [WARNING] [stage3.py:1991:step] 656 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 80%|████████  | 135/168 [35:25:39<8:38:29, 942.70s/it]                                                       {'loss': 0.0134, 'learning_rate': 1.9787680724495617e-05, 'epoch': 2.39}
 80%|████████  | 135/168 [35:25:39<8:38:29, 942.70s/it][2026-02-05 02:05:58,415] [WARNING] [stage3.py:1991:step] 603 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 81%|████████  | 136/168 [35:41:13<8:21:25, 940.18s/it]                                                       {'loss': 0.0112, 'learning_rate': 1.864479297370325e-05, 'epoch': 2.41}
 81%|████████  | 136/168 [35:41:13<8:21:25, 940.18s/it][2026-02-05 02:21:38,845] [WARNING] [stage3.py:1991:step] 619 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 82%|████████▏ | 137/168 [35:56:54<8:05:47, 940.25s/it]                                                       {'loss': 0.0137, 'learning_rate': 1.7532499589089323e-05, 'epoch': 2.43}
 82%|████████▏ | 137/168 [35:56:54<8:05:47, 940.25s/it][2026-02-05 02:37:20,670] [WARNING] [stage3.py:1991:step] 690 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 82%|████████▏ | 138/168 [36:12:36<7:50:21, 940.72s/it]                                                       {'loss': 0.0167, 'learning_rate': 1.6451218858706374e-05, 'epoch': 2.45}
 82%|████████▏ | 138/168 [36:12:36<7:50:21, 940.72s/it][2026-02-05 02:53:01,767] [WARNING] [stage3.py:1991:step] 655 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 83%|████████▎ | 139/168 [36:28:17<7:34:44, 940.83s/it]                                                       {'loss': 0.0187, 'learning_rate': 1.5401357408015893e-05, 'epoch': 2.47}
 83%|████████▎ | 139/168 [36:28:17<7:34:44, 940.83s/it][2026-02-05 03:08:54,074] [WARNING] [stage3.py:1991:step] 627 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 83%|████████▎ | 140/168 [36:44:09<7:20:39, 944.28s/it]                                                       {'loss': 0.012, 'learning_rate': 1.4383310046973365e-05, 'epoch': 2.48}
 83%|████████▎ | 140/168 [36:44:09<7:20:39, 944.28s/it][2026-02-05 03:24:36,740] [WARNING] [stage3.py:1991:step] 672 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 84%|████████▍ | 141/168 [36:59:52<7:04:42, 943.79s/it]                                                       {'loss': 0.0067, 'learning_rate': 1.339745962155613e-05, 'epoch': 2.5}
 84%|████████▍ | 141/168 [36:59:52<7:04:42, 943.79s/it][2026-02-05 03:40:16,837] [WARNING] [stage3.py:1991:step] 689 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 85%|████████▍ | 142/168 [37:15:32<6:48:29, 942.68s/it]                                                       {'loss': 0.019, 'learning_rate': 1.2444176869790925e-05, 'epoch': 2.52}
 85%|████████▍ | 142/168 [37:15:32<6:48:29, 942.68s/it][2026-02-05 03:55:48,194] [WARNING] [stage3.py:1991:step] 682 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 85%|████████▌ | 143/168 [37:31:03<6:31:22, 939.29s/it]                                                       {'loss': 0.0083, 'learning_rate': 1.1523820282334219e-05, 'epoch': 2.54}
 85%|████████▌ | 143/168 [37:31:03<6:31:22, 939.29s/it][2026-02-05 04:11:30,160] [WARNING] [stage3.py:1991:step] 589 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 86%|████████▌ | 144/168 [37:46:45<6:16:02, 940.10s/it]                                                       {'loss': 0.0154, 'learning_rate': 1.0636735967658784e-05, 'epoch': 2.55}
 86%|████████▌ | 144/168 [37:46:45<6:16:02, 940.10s/it][2026-02-05 04:27:21,805] [WARNING] [stage3.py:1991:step] 642 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 86%|████████▋ | 145/168 [38:02:37<6:01:41, 943.55s/it]                                                       {'loss': 0.0111, 'learning_rate': 9.783257521896227e-06, 'epoch': 2.57}
 86%|████████▋ | 145/168 [38:02:37<6:01:41, 943.55s/it][2026-02-05 04:43:01,490] [WARNING] [stage3.py:1991:step] 615 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 87%|████████▋ | 146/168 [38:18:16<5:45:32, 942.39s/it]                                                       {'loss': 0.0112, 'learning_rate': 8.963705903385345e-06, 'epoch': 2.59}
 87%|████████▋ | 146/168 [38:18:16<5:45:32, 942.39s/it][2026-02-05 04:58:32,676] [WARNING] [stage3.py:1991:step] 590 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 88%|████████▊ | 147/168 [38:33:48<5:28:39, 939.03s/it]                                                       {'loss': 0.016, 'learning_rate': 8.178389311972612e-06, 'epoch': 2.61}
 88%|████████▊ | 147/168 [38:33:48<5:28:39, 939.03s/it][2026-02-05 05:14:01,789] [WARNING] [stage3.py:1991:step] 613 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 88%|████████▊ | 148/168 [38:49:17<5:12:01, 936.06s/it]                                                       {'loss': 0.0129, 'learning_rate': 7.427603073110967e-06, 'epoch': 2.63}
 88%|████████▊ | 148/168 [38:49:17<5:12:01, 936.06s/it][2026-02-05 05:29:43,598] [WARNING] [stage3.py:1991:step] 685 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 89%|████████▊ | 149/168 [39:04:59<4:56:57, 937.78s/it]                                                       {'loss': 0.0131, 'learning_rate': 6.7116295267999455e-06, 'epoch': 2.64}
 89%|████████▊ | 149/168 [39:04:59<4:56:57, 937.78s/it][2026-02-05 05:45:25,108] [WARNING] [stage3.py:1991:step] 628 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 89%|████████▉ | 150/168 [39:20:40<4:41:40, 938.90s/it]                                                       {'loss': 0.0149, 'learning_rate': 6.030737921409169e-06, 'epoch': 2.66}
 89%|████████▉ | 150/168 [39:20:40<4:41:40, 938.90s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[2026-02-05 06:01:45,970] [WARNING] [stage3.py:1991:step] 546 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 90%|████████▉ | 151/168 [39:37:01<4:29:35, 951.50s/it]                                                       {'loss': 0.022, 'learning_rate': 5.385184312424974e-06, 'epoch': 2.68}
 90%|████████▉ | 151/168 [39:37:01<4:29:35, 951.50s/it][2026-02-05 06:17:26,091] [WARNING] [stage3.py:1991:step] 634 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 90%|█████████ | 152/168 [39:52:41<4:12:49, 948.07s/it]                                                       {'loss': 0.0093, 'learning_rate': 4.775211466158469e-06, 'epoch': 2.7}
 90%|█████████ | 152/168 [39:52:41<4:12:49, 948.07s/it][2026-02-05 06:33:07,634] [WARNING] [stage3.py:1991:step] 646 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 91%|█████████ | 153/168 [40:08:23<3:56:31, 946.12s/it]                                                       {'loss': 0.0228, 'learning_rate': 4.20104876845111e-06, 'epoch': 2.71}
 91%|█████████ | 153/168 [40:08:23<3:56:31, 946.12s/it][2026-02-05 06:48:47,894] [WARNING] [stage3.py:1991:step] 645 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 92%|█████████▏| 154/168 [40:24:03<3:40:21, 944.36s/it]                                                       {'loss': 0.0143, 'learning_rate': 3.662912138411967e-06, 'epoch': 2.73}
 92%|█████████▏| 154/168 [40:24:03<3:40:21, 944.36s/it][2026-02-05 07:04:31,821] [WARNING] [stage3.py:1991:step] 664 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 92%|█████████▏| 155/168 [40:39:47<3:24:35, 944.24s/it]                                                       {'loss': 0.0151, 'learning_rate': 3.161003947219421e-06, 'epoch': 2.75}
 92%|█████████▏| 155/168 [40:39:47<3:24:35, 944.24s/it][2026-02-05 07:20:12,819] [WARNING] [stage3.py:1991:step] 640 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 93%|█████████▎| 156/168 [40:55:28<3:08:39, 943.26s/it]                                                       {'loss': 0.0151, 'learning_rate': 2.6955129420176196e-06, 'epoch': 2.77}
 93%|█████████▎| 156/168 [40:55:28<3:08:39, 943.26s/it][2026-02-05 07:36:03,188] [WARNING] [stage3.py:1991:step] 640 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 93%|█████████▎| 157/168 [41:11:18<2:53:19, 945.39s/it]                                                       {'loss': 0.0129, 'learning_rate': 2.266614174936443e-06, 'epoch': 2.78}
 93%|█████████▎| 157/168 [41:11:18<2:53:19, 945.39s/it][2026-02-05 07:51:44,458] [WARNING] [stage3.py:1991:step] 569 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 94%|█████████▍| 158/168 [41:26:59<2:37:21, 944.15s/it]                                                       {'loss': 0.0175, 'learning_rate': 1.874468937261531e-06, 'epoch': 2.8}
 94%|█████████▍| 158/168 [41:26:59<2:37:21, 944.15s/it][2026-02-05 08:07:27,759] [WARNING] [stage3.py:1991:step] 677 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 95%|█████████▍| 159/168 [41:42:43<2:21:35, 943.91s/it]                                                       {'loss': 0.0143, 'learning_rate': 1.5192246987791981e-06, 'epoch': 2.82}
 95%|█████████▍| 159/168 [41:42:43<2:21:35, 943.91s/it][2026-02-05 08:23:09,601] [WARNING] [stage3.py:1991:step] 610 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 95%|█████████▌| 160/168 [41:58:25<2:05:46, 943.28s/it]                                                       {'loss': 0.0117, 'learning_rate': 1.201015052319099e-06, 'epoch': 2.84}
 95%|█████████▌| 160/168 [41:58:25<2:05:46, 943.28s/it][2026-02-05 08:38:52,167] [WARNING] [stage3.py:1991:step] 617 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 96%|█████████▌| 161/168 [42:14:07<1:50:01, 943.07s/it]                                                       {'loss': 0.0101, 'learning_rate': 9.199596635154683e-07, 'epoch': 2.86}
 96%|█████████▌| 161/168 [42:14:07<1:50:01, 943.07s/it][2026-02-05 08:54:44,622] [WARNING] [stage3.py:1991:step] 628 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 96%|█████████▋| 162/168 [42:30:00<1:34:35, 945.88s/it]                                                       {'loss': 0.0131, 'learning_rate': 6.761642258056978e-07, 'epoch': 2.87}
 96%|█████████▋| 162/168 [42:30:00<1:34:35, 945.88s/it][2026-02-05 09:10:26,077] [WARNING] [stage3.py:1991:step] 589 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 97%|█████████▋| 163/168 [42:45:41<1:18:42, 944.56s/it]                                                       {'loss': 0.0178, 'learning_rate': 4.6972042068341714e-07, 'epoch': 2.89}
 97%|█████████▋| 163/168 [42:45:41<1:18:42, 944.56s/it][2026-02-05 09:26:06,873] [WARNING] [stage3.py:1991:step] 698 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 98%|█████████▊| 164/168 [43:01:22<1:02:53, 943.42s/it]                                                       {'loss': 0.0147, 'learning_rate': 3.007058832207976e-07, 'epoch': 2.91}
 98%|█████████▊| 164/168 [43:01:22<1:02:53, 943.42s/it][2026-02-05 09:41:37,250] [WARNING] [stage3.py:1991:step] 604 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 98%|█████████▊| 165/168 [43:16:52<46:58, 939.51s/it]                                                       {'loss': 0.0149, 'learning_rate': 1.6918417287318245e-07, 'epoch': 2.93}
 98%|█████████▊| 165/168 [43:16:52<46:58, 939.51s/it][2026-02-05 09:57:18,334] [WARNING] [stage3.py:1991:step] 589 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 99%|█████████▉| 166/168 [43:32:33<31:19, 939.98s/it]                                                     {'loss': 0.0112, 'learning_rate': 7.520474957699586e-08, 'epoch': 2.94}
 99%|█████████▉| 166/168 [43:32:33<31:19, 939.98s/it][2026-02-05 10:12:57,938] [WARNING] [stage3.py:1991:step] 588 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 99%|█████████▉| 167/168 [43:48:13<15:39, 939.88s/it]                                                     {'loss': 0.0116, 'learning_rate': 1.8802955149865852e-08, 'epoch': 2.96}
 99%|█████████▉| 167/168 [43:48:13<15:39, 939.88s/it][2026-02-05 10:28:57,939] [WARNING] [stage3.py:1991:step] 682 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
100%|██████████| 168/168 [44:04:13<00:00, 945.90s/it]                                                     {'loss': 0.0107, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 168/168 [44:04:13<00:00, 945.90s/it]                                                     {'train_runtime': 158653.6124, 'train_samples_per_second': 0.205, 'train_steps_per_second': 0.001, 'train_loss': 0.09362231421151332, 'epoch': 2.98}
100%|██████████| 168/168 [44:04:13<00:00, 945.90s/it]100%|██████████| 168/168 [44:04:13<00:00, 944.37s/it]
[2026-02-05 10:29:27,162] [INFO] [launch.py:347:main] Process 3976708 exits successfully.
[2026-02-05 10:29:29,163] [INFO] [launch.py:347:main] Process 3976706 exits successfully.
[2026-02-05 10:29:31,164] [INFO] [launch.py:347:main] Process 3976707 exits successfully.
[2026-02-05 10:29:33,164] [INFO] [launch.py:347:main] Process 3976704 exits successfully.
[2026-02-05 10:29:35,165] [INFO] [launch.py:347:main] Process 3976703 exits successfully.
[2026-02-05 10:29:37,165] [INFO] [launch.py:347:main] Process 3976705 exits successfully.
