/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-10 22:56:55,893] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-10 22:57:05,904] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=2,3,4,5: setting --include=localhost:2,3,4,5
[2026-01-10 22:57:11,450] [INFO] [runner.py:571:main] cmd = /data3/jisu/miniconda3/envs/mfm-new/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero2.json --model_name_or_path liuhaotian/llava-v1.5-7b --version v1 --data_path /data3/jisu/LLaVA/visa_llava_instruct.json --image_folder /data3/jisu/MFM/datasets/ViSA --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bits 4 --bf16 False --fp16 True --output_dir ./checkpoints/llava-v1.5-7b-mfm-lora --num_train_epochs 3 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 False --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to none
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-10 22:57:17,543] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-10 22:57:30,540] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2026-01-10 22:57:30,540] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [2, 3, 4, 5]}
[2026-01-10 22:57:30,540] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2026-01-10 22:57:30,540] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2026-01-10 22:57:30,540] [INFO] [launch.py:163:main] dist_world_size=4
[2026-01-10 22:57:30,540] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=2,3,4,5
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2026-01-10 22:57:37,077] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-10 22:57:38,081] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-10 22:57:38,084] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-10 22:57:38,085] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-10 22:57:47,496] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-10 22:58:14,926] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-10 22:58:14,950] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-01-10 22:58:14,950] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2026-01-10 22:58:14,964] [INFO] [comm.py:637:init_distributed] cdb=None
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [02:40<02:40, 160.58s/it]Loading checkpoint shards:  50%|█████     | 1/2 [02:36<02:36, 156.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [02:49<02:49, 169.92s/it]Loading checkpoint shards:  50%|█████     | 1/2 [02:45<02:45, 165.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [03:32<00:00, 97.29s/it] Loading checkpoint shards: 100%|██████████| 2/2 [03:32<00:00, 106.16s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [03:37<00:00, 99.57s/it] Loading checkpoint shards: 100%|██████████| 2/2 [03:37<00:00, 108.72s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [03:46<00:00, 102.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [03:41<00:00, 100.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [03:46<00:00, 113.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [03:41<00:00, 110.61s/it]
Adding LoRA adapters...
Formatting inputs...Skip in lazy mode
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
  0%|          | 0/507 [00:00<?, ?it/s]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/507 [00:28<3:57:03, 28.11s/it]                                                 {'loss': 2.3151, 'learning_rate': 1.25e-05, 'epoch': 0.01}
  0%|          | 1/507 [00:35<3:57:03, 28.11s/it]  0%|          | 2/507 [00:53<3:43:41, 26.58s/it]                                                 {'loss': 2.2569, 'learning_rate': 2.5e-05, 'epoch': 0.01}
  0%|          | 2/507 [00:53<3:43:41, 26.58s/it]  1%|          | 3/507 [01:10<3:07:48, 22.36s/it]                                                 {'loss': 1.5749, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.02}
  1%|          | 3/507 [01:10<3:07:48, 22.36s/it]  1%|          | 4/507 [01:26<2:44:17, 19.60s/it]                                                 {'loss': 0.8815, 'learning_rate': 5e-05, 'epoch': 0.02}
  1%|          | 4/507 [01:26<2:44:17, 19.60s/it]  1%|          | 5/507 [01:41<2:31:27, 18.10s/it]                                                 {'loss': 0.7681, 'learning_rate': 6.25e-05, 'epoch': 0.03}
  1%|          | 5/507 [01:41<2:31:27, 18.10s/it]  1%|          | 6/507 [01:57<2:23:52, 17.23s/it]                                                 {'loss': 0.7407, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.04}
  1%|          | 6/507 [01:57<2:23:52, 17.23s/it]  1%|▏         | 7/507 [02:12<2:19:04, 16.69s/it]                                                 {'loss': 0.1971, 'learning_rate': 8.75e-05, 'epoch': 0.04}
  1%|▏         | 7/507 [02:12<2:19:04, 16.69s/it]  2%|▏         | 8/507 [02:28<2:15:41, 16.32s/it]                                                 {'loss': 0.2049, 'learning_rate': 0.0001, 'epoch': 0.05}
  2%|▏         | 8/507 [02:28<2:15:41, 16.32s/it]  2%|▏         | 9/507 [02:44<2:13:35, 16.09s/it]                                                 {'loss': 0.1067, 'learning_rate': 0.00011250000000000001, 'epoch': 0.05}
  2%|▏         | 9/507 [02:44<2:13:35, 16.09s/it]  2%|▏         | 10/507 [02:59<2:12:03, 15.94s/it]                                                  {'loss': 0.1315, 'learning_rate': 0.000125, 'epoch': 0.06}
  2%|▏         | 10/507 [02:59<2:12:03, 15.94s/it]  2%|▏         | 11/507 [03:15<2:10:58, 15.84s/it]                                                  {'loss': 0.1997, 'learning_rate': 0.0001375, 'epoch': 0.06}
  2%|▏         | 11/507 [03:15<2:10:58, 15.84s/it]  2%|▏         | 12/507 [03:30<2:09:58, 15.75s/it]                                                  {'loss': 0.0841, 'learning_rate': 0.00015000000000000001, 'epoch': 0.07}
  2%|▏         | 12/507 [03:30<2:09:58, 15.75s/it]  3%|▎         | 13/507 [03:46<2:10:05, 15.80s/it]                                                  {'loss': 0.1905, 'learning_rate': 0.00016250000000000002, 'epoch': 0.08}
  3%|▎         | 13/507 [03:46<2:10:05, 15.80s/it]  3%|▎         | 14/507 [04:02<2:09:19, 15.74s/it]                                                  {'loss': 0.1012, 'learning_rate': 0.000175, 'epoch': 0.08}
  3%|▎         | 14/507 [04:02<2:09:19, 15.74s/it]  3%|▎         | 15/507 [04:17<2:08:48, 15.71s/it]                                                  {'loss': 0.2267, 'learning_rate': 0.0001875, 'epoch': 0.09}
  3%|▎         | 15/507 [04:17<2:08:48, 15.71s/it]  3%|▎         | 16/507 [04:33<2:08:45, 15.74s/it]                                                  {'loss': 0.1534, 'learning_rate': 0.0002, 'epoch': 0.09}
  3%|▎         | 16/507 [04:33<2:08:45, 15.74s/it]  3%|▎         | 17/507 [04:49<2:08:25, 15.73s/it]                                                  {'loss': 0.1253, 'learning_rate': 0.00019999795305919378, 'epoch': 0.1}
  3%|▎         | 17/507 [04:49<2:08:25, 15.73s/it]  4%|▎         | 18/507 [05:05<2:07:57, 15.70s/it]                                                  {'loss': 0.1275, 'learning_rate': 0.0001999918123205744, 'epoch': 0.11}
  4%|▎         | 18/507 [05:05<2:07:57, 15.70s/it]  4%|▎         | 19/507 [05:20<2:07:35, 15.69s/it]                                                  {'loss': 0.095, 'learning_rate': 0.00019998157803553638, 'epoch': 0.11}
  4%|▎         | 19/507 [05:20<2:07:35, 15.69s/it]  4%|▍         | 20/507 [05:36<2:07:13, 15.68s/it]                                                  {'loss': 0.0883, 'learning_rate': 0.00019996725062305934, 'epoch': 0.12}
  4%|▍         | 20/507 [05:36<2:07:13, 15.68s/it]  4%|▍         | 21/507 [05:52<2:07:00, 15.68s/it]                                                  {'loss': 0.112, 'learning_rate': 0.00019994883066969053, 'epoch': 0.12}
  4%|▍         | 21/507 [05:52<2:07:00, 15.68s/it]  4%|▍         | 22/507 [06:07<2:06:54, 15.70s/it]                                                  {'loss': 0.185, 'learning_rate': 0.00019992631892952107, 'epoch': 0.13}
  4%|▍         | 22/507 [06:07<2:06:54, 15.70s/it]  5%|▍         | 23/507 [06:23<2:06:42, 15.71s/it]                                                  {'loss': 0.071, 'learning_rate': 0.0001998997163241549, 'epoch': 0.14}
  5%|▍         | 23/507 [06:23<2:06:42, 15.71s/it]  5%|▍         | 24/507 [06:39<2:06:29, 15.71s/it]                                                  {'loss': 0.1366, 'learning_rate': 0.00019986902394267118, 'epoch': 0.14}
  5%|▍         | 24/507 [06:39<2:06:29, 15.71s/it]  5%|▍         | 25/507 [06:54<2:06:17, 15.72s/it]                                                  {'loss': 0.132, 'learning_rate': 0.00019983424304157973, 'epoch': 0.15}
  5%|▍         | 25/507 [06:54<2:06:17, 15.72s/it]  5%|▌         | 26/507 [07:10<2:06:08, 15.74s/it]                                                  {'loss': 0.0461, 'learning_rate': 0.00019979537504476944, 'epoch': 0.15}
  5%|▌         | 26/507 [07:10<2:06:08, 15.74s/it]  5%|▌         | 27/507 [07:26<2:05:58, 15.75s/it]                                                  {'loss': 0.0652, 'learning_rate': 0.00019975242154345008, 'epoch': 0.16}
  5%|▌         | 27/507 [07:26<2:05:58, 15.75s/it]  6%|▌         | 28/507 [07:42<2:05:47, 15.76s/it]                                                  {'loss': 0.0868, 'learning_rate': 0.00019970538429608714, 'epoch': 0.17}
  6%|▌         | 28/507 [07:42<2:05:47, 15.76s/it]  6%|▌         | 29/507 [08:01<2:14:05, 16.83s/it]                                                  {'loss': 0.0725, 'learning_rate': 0.00019965426522832984, 'epoch': 0.17}
  6%|▌         | 29/507 [08:01<2:14:05, 16.83s/it]  6%|▌         | 30/507 [08:17<2:11:09, 16.50s/it]                                                  {'loss': 0.046, 'learning_rate': 0.0001995990664329323, 'epoch': 0.18}
  6%|▌         | 30/507 [08:17<2:11:09, 16.50s/it]  6%|▌         | 31/507 [08:33<2:09:05, 16.27s/it]                                                  {'loss': 0.0547, 'learning_rate': 0.00019953979016966788, 'epoch': 0.18}
  6%|▌         | 31/507 [08:33<2:09:05, 16.27s/it]  6%|▋         | 32/507 [08:48<2:07:48, 16.14s/it]                                                  {'loss': 0.0614, 'learning_rate': 0.0001994764388652366, 'epoch': 0.19}
  6%|▋         | 32/507 [08:48<2:07:48, 16.14s/it]  7%|▋         | 33/507 [09:04<2:06:47, 16.05s/it]                                                  {'loss': 0.0845, 'learning_rate': 0.00019940901511316582, 'epoch': 0.19}
  7%|▋         | 33/507 [09:04<2:06:47, 16.05s/it]  7%|▋         | 34/507 [09:26<2:18:53, 17.62s/it]                                                  {'loss': 0.1028, 'learning_rate': 0.0001993375216737042, 'epoch': 0.2}
  7%|▋         | 34/507 [09:26<2:18:53, 17.62s/it]  7%|▋         | 35/507 [09:41<2:14:12, 17.06s/it]                                                  {'loss': 0.0578, 'learning_rate': 0.00019926196147370849, 'epoch': 0.21}
  7%|▋         | 35/507 [09:41<2:14:12, 17.06s/it]  7%|▋         | 36/507 [09:57<2:10:53, 16.67s/it]                                                  {'loss': 0.0427, 'learning_rate': 0.0001991823376065238, 'epoch': 0.21}
  7%|▋         | 36/507 [09:57<2:10:53, 16.67s/it]  7%|▋         | 37/507 [10:13<2:08:29, 16.40s/it]                                                  {'loss': 0.0484, 'learning_rate': 0.00019909865333185702, 'epoch': 0.22}
  7%|▋         | 37/507 [10:13<2:08:29, 16.40s/it]  7%|▋         | 38/507 [10:29<2:06:54, 16.23s/it]                                                  {'loss': 0.0589, 'learning_rate': 0.00019901091207564324, 'epoch': 0.22}
  7%|▋         | 38/507 [10:29<2:06:54, 16.23s/it]  8%|▊         | 39/507 [10:45<2:05:42, 16.12s/it]                                                  {'loss': 0.0127, 'learning_rate': 0.00019891911742990565, 'epoch': 0.23}
  8%|▊         | 39/507 [10:45<2:05:42, 16.12s/it]  8%|▊         | 40/507 [11:00<2:04:42, 16.02s/it]                                                  {'loss': 0.0552, 'learning_rate': 0.00019882327315260838, 'epoch': 0.24}
  8%|▊         | 40/507 [11:00<2:04:42, 16.02s/it]  8%|▊         | 41/507 [11:20<2:12:02, 17.00s/it]                                                  {'loss': 0.0848, 'learning_rate': 0.00019872338316750265, 'epoch': 0.24}
  8%|▊         | 41/507 [11:20<2:12:02, 17.00s/it]  8%|▊         | 42/507 [11:35<2:08:43, 16.61s/it]                                                  {'loss': 0.0725, 'learning_rate': 0.0001986194515639662, 'epoch': 0.25}
  8%|▊         | 42/507 [11:35<2:08:43, 16.61s/it]  8%|▊         | 43/507 [11:51<2:06:33, 16.37s/it]                                                  {'loss': 0.0469, 'learning_rate': 0.00019851148259683585, 'epoch': 0.25}
  8%|▊         | 43/507 [11:51<2:06:33, 16.37s/it]  9%|▊         | 44/507 [12:07<2:04:57, 16.19s/it]                                                  {'loss': 0.0686, 'learning_rate': 0.0001983994806862333, 'epoch': 0.26}
  9%|▊         | 44/507 [12:07<2:04:57, 16.19s/it]  9%|▉         | 45/507 [12:23<2:03:43, 16.07s/it]                                                  {'loss': 0.0698, 'learning_rate': 0.00019828345041738413, 'epoch': 0.27}
  9%|▉         | 45/507 [12:23<2:03:43, 16.07s/it]  9%|▉         | 46/507 [12:38<2:02:44, 15.98s/it]                                                  {'loss': 0.0644, 'learning_rate': 0.00019816339654043022, 'epoch': 0.27}
  9%|▉         | 46/507 [12:38<2:02:44, 15.98s/it]  9%|▉         | 47/507 [12:54<2:01:59, 15.91s/it]                                                  {'loss': 0.0572, 'learning_rate': 0.0001980393239702351, 'epoch': 0.28}
  9%|▉         | 47/507 [12:54<2:01:59, 15.91s/it]  9%|▉         | 48/507 [13:10<2:01:20, 15.86s/it]                                                  {'loss': 0.0684, 'learning_rate': 0.00019791123778618305, 'epoch': 0.28}
  9%|▉         | 48/507 [13:10<2:01:20, 15.86s/it] 10%|▉         | 49/507 [13:26<2:00:52, 15.83s/it]                                                  {'loss': 0.0606, 'learning_rate': 0.00019777914323197064, 'epoch': 0.29}
 10%|▉         | 49/507 [13:26<2:00:52, 15.83s/it] 10%|▉         | 50/507 [13:41<2:00:14, 15.79s/it]                                                  {'loss': 0.0347, 'learning_rate': 0.00019764304571539266, 'epoch': 0.3}
 10%|▉         | 50/507 [13:41<2:00:14, 15.79s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 10%|█         | 51/507 [14:05<2:18:32, 18.23s/it]                                                  {'loss': 0.0277, 'learning_rate': 0.00019750295080812023, 'epoch': 0.3}
 10%|█         | 51/507 [14:05<2:18:32, 18.23s/it] 10%|█         | 52/507 [14:21<2:12:13, 17.44s/it]                                                  {'loss': 0.0332, 'learning_rate': 0.00019735886424547306, 'epoch': 0.31}
 10%|█         | 52/507 [14:21<2:12:13, 17.44s/it] 10%|█         | 53/507 [14:37<2:09:29, 17.11s/it]                                                  {'loss': 0.0836, 'learning_rate': 0.0001972107919261844, 'epoch': 0.31}
 10%|█         | 53/507 [14:37<2:09:29, 17.11s/it] 11%|█         | 54/507 [14:53<2:05:51, 16.67s/it]                                                  {'loss': 0.0198, 'learning_rate': 0.00019705873991215974, 'epoch': 0.32}
 11%|█         | 54/507 [14:53<2:05:51, 16.67s/it] 11%|█         | 55/507 [15:09<2:03:18, 16.37s/it]                                                  {'loss': 0.0305, 'learning_rate': 0.00019690271442822848, 'epoch': 0.32}
 11%|█         | 55/507 [15:09<2:03:18, 16.37s/it] 11%|█         | 56/507 [15:24<2:01:33, 16.17s/it]                                                  {'loss': 0.0345, 'learning_rate': 0.0001967427218618893, 'epoch': 0.33}
 11%|█         | 56/507 [15:24<2:01:33, 16.17s/it] 11%|█         | 57/507 [15:40<2:00:17, 16.04s/it]                                                  {'loss': 0.0551, 'learning_rate': 0.00019657876876304835, 'epoch': 0.34}
 11%|█         | 57/507 [15:40<2:00:17, 16.04s/it] 11%|█▏        | 58/507 [15:56<1:59:20, 15.95s/it]                                                  {'loss': 0.0374, 'learning_rate': 0.00019641086184375145, 'epoch': 0.34}
 11%|█▏        | 58/507 [15:56<1:59:20, 15.95s/it] 12%|█▏        | 59/507 [16:11<1:58:33, 15.88s/it]                                                  {'loss': 0.0372, 'learning_rate': 0.00019623900797790912, 'epoch': 0.35}
 12%|█▏        | 59/507 [16:11<1:58:33, 15.88s/it] 12%|█▏        | 60/507 [16:27<1:58:00, 15.84s/it]                                                  {'loss': 0.0437, 'learning_rate': 0.00019606321420101512, 'epoch': 0.35}
 12%|█▏        | 60/507 [16:27<1:58:00, 15.84s/it] 12%|█▏        | 61/507 [16:43<1:57:28, 15.80s/it]                                                  {'loss': 0.0511, 'learning_rate': 0.0001958834877098586, 'epoch': 0.36}
 12%|█▏        | 61/507 [16:43<1:57:28, 15.80s/it] 12%|█▏        | 62/507 [16:59<1:57:08, 15.79s/it]                                                  {'loss': 0.0648, 'learning_rate': 0.0001956998358622293, 'epoch': 0.37}
 12%|█▏        | 62/507 [16:59<1:57:08, 15.79s/it] 12%|█▏        | 63/507 [17:14<1:56:49, 15.79s/it]                                                  {'loss': 0.0432, 'learning_rate': 0.00019551226617661648, 'epoch': 0.37}
 12%|█▏        | 63/507 [17:14<1:56:49, 15.79s/it] 13%|█▎        | 64/507 [17:30<1:56:39, 15.80s/it]                                                  {'loss': 0.0275, 'learning_rate': 0.00019532078633190095, 'epoch': 0.38}
 13%|█▎        | 64/507 [17:30<1:56:39, 15.80s/it] 13%|█▎        | 65/507 [17:46<1:56:25, 15.80s/it]                                                  {'loss': 0.0543, 'learning_rate': 0.00019512540416704094, 'epoch': 0.38}
 13%|█▎        | 65/507 [17:46<1:56:25, 15.80s/it] 13%|█▎        | 66/507 [18:02<1:56:14, 15.81s/it]                                                  {'loss': 0.0477, 'learning_rate': 0.00019492612768075092, 'epoch': 0.39}
 13%|█▎        | 66/507 [18:02<1:56:14, 15.81s/it] 13%|█▎        | 67/507 [18:18<1:56:02, 15.82s/it]                                                  {'loss': 0.0284, 'learning_rate': 0.00019472296503117437, 'epoch': 0.4}
 13%|█▎        | 67/507 [18:18<1:56:02, 15.82s/it] 13%|█▎        | 68/507 [18:34<1:55:45, 15.82s/it]                                                  {'loss': 0.0399, 'learning_rate': 0.00019451592453554955, 'epoch': 0.4}
 13%|█▎        | 68/507 [18:34<1:55:45, 15.82s/it] 14%|█▎        | 69/507 [18:50<1:55:36, 15.84s/it]                                                  {'loss': 0.0382, 'learning_rate': 0.00019430501466986933, 'epoch': 0.41}
 14%|█▎        | 69/507 [18:50<1:55:36, 15.84s/it] 14%|█▍        | 70/507 [19:05<1:55:23, 15.84s/it]                                                  {'loss': 0.0481, 'learning_rate': 0.0001940902440685339, 'epoch': 0.41}
 14%|█▍        | 70/507 [19:05<1:55:23, 15.84s/it] 14%|█▍        | 71/507 [19:21<1:55:06, 15.84s/it]                                                  {'loss': 0.0412, 'learning_rate': 0.0001938716215239974, 'epoch': 0.42}
 14%|█▍        | 71/507 [19:21<1:55:06, 15.84s/it] 14%|█▍        | 72/507 [19:37<1:54:50, 15.84s/it]                                                  {'loss': 0.0187, 'learning_rate': 0.00019364915598640793, 'epoch': 0.43}
 14%|█▍        | 72/507 [19:37<1:54:50, 15.84s/it] 14%|█▍        | 73/507 [19:53<1:54:31, 15.83s/it]                                                  {'loss': 0.0899, 'learning_rate': 0.00019342285656324135, 'epoch': 0.43}
 14%|█▍        | 73/507 [19:53<1:54:31, 15.83s/it] 15%|█▍        | 74/507 [20:09<1:54:19, 15.84s/it]                                                  {'loss': 0.0579, 'learning_rate': 0.00019319273251892805, 'epoch': 0.44}
 15%|█▍        | 74/507 [20:09<1:54:19, 15.84s/it] 15%|█▍        | 75/507 [20:24<1:53:43, 15.79s/it]                                                  {'loss': 0.0467, 'learning_rate': 0.000192958793274474, 'epoch': 0.44}
 15%|█▍        | 75/507 [20:24<1:53:43, 15.79s/it] 15%|█▍        | 76/507 [20:40<1:53:26, 15.79s/it]                                                  {'loss': 0.046, 'learning_rate': 0.00019272104840707487, 'epoch': 0.45}
 15%|█▍        | 76/507 [20:40<1:53:26, 15.79s/it] 15%|█▌        | 77/507 [20:56<1:53:12, 15.80s/it]                                                  {'loss': 0.0428, 'learning_rate': 0.0001924795076497241, 'epoch': 0.45}
 15%|█▌        | 77/507 [20:56<1:53:12, 15.80s/it] 15%|█▌        | 78/507 [21:12<1:53:10, 15.83s/it]                                                  {'loss': 0.0721, 'learning_rate': 0.0001922341808908144, 'epoch': 0.46}
 15%|█▌        | 78/507 [21:12<1:53:10, 15.83s/it] 16%|█▌        | 79/507 [21:28<1:52:48, 15.81s/it]                                                  {'loss': 0.0605, 'learning_rate': 0.00019198507817373272, 'epoch': 0.47}
 16%|█▌        | 79/507 [21:28<1:52:48, 15.81s/it] 16%|█▌        | 80/507 [21:43<1:52:25, 15.80s/it]                                                  {'loss': 0.0486, 'learning_rate': 0.00019173220969644948, 'epoch': 0.47}
 16%|█▌        | 80/507 [21:43<1:52:25, 15.80s/it] 16%|█▌        | 81/507 [21:59<1:52:01, 15.78s/it]                                                  {'loss': 0.0339, 'learning_rate': 0.00019147558581110078, 'epoch': 0.48}
 16%|█▌        | 81/507 [21:59<1:52:01, 15.78s/it] 16%|█▌        | 82/507 [22:15<1:51:33, 15.75s/it]                                                  {'loss': 0.0473, 'learning_rate': 0.0001912152170235646, 'epoch': 0.48}
 16%|█▌        | 82/507 [22:15<1:51:33, 15.75s/it] 16%|█▋        | 83/507 [22:31<1:51:15, 15.74s/it]                                                  {'loss': 0.0286, 'learning_rate': 0.0001909511139930309, 'epoch': 0.49}
 16%|█▋        | 83/507 [22:31<1:51:15, 15.74s/it] 17%|█▋        | 84/507 [22:46<1:50:57, 15.74s/it]                                                  {'loss': 0.0164, 'learning_rate': 0.00019068328753156513, 'epoch': 0.5}
 17%|█▋        | 84/507 [22:46<1:50:57, 15.74s/it] 17%|█▋        | 85/507 [23:02<1:50:42, 15.74s/it]                                                  {'loss': 0.04, 'learning_rate': 0.0001904117486036655, 'epoch': 0.5}
 17%|█▋        | 85/507 [23:02<1:50:42, 15.74s/it] 17%|█▋        | 86/507 [23:18<1:50:22, 15.73s/it]                                                  {'loss': 0.0628, 'learning_rate': 0.00019013650832581423, 'epoch': 0.51}
 17%|█▋        | 86/507 [23:18<1:50:22, 15.73s/it] 17%|█▋        | 87/507 [23:33<1:49:56, 15.71s/it]                                                  {'loss': 0.0376, 'learning_rate': 0.00018985757796602252, 'epoch': 0.51}
 17%|█▋        | 87/507 [23:33<1:49:56, 15.71s/it] 17%|█▋        | 88/507 [23:49<1:49:42, 15.71s/it]                                                  {'loss': 0.0351, 'learning_rate': 0.00018957496894336898, 'epoch': 0.52}
 17%|█▋        | 88/507 [23:49<1:49:42, 15.71s/it] 18%|█▊        | 89/507 [24:05<1:49:27, 15.71s/it]                                                  {'loss': 0.0268, 'learning_rate': 0.0001892886928275325, 'epoch': 0.53}
 18%|█▊        | 89/507 [24:05<1:49:27, 15.71s/it] 18%|█▊        | 90/507 [24:21<1:49:09, 15.71s/it]                                                  {'loss': 0.0454, 'learning_rate': 0.00018899876133831835, 'epoch': 0.53}
 18%|█▊        | 90/507 [24:21<1:49:09, 15.71s/it] 18%|█▊        | 91/507 [24:36<1:48:53, 15.71s/it]                                                  {'loss': 0.0331, 'learning_rate': 0.0001887051863451784, 'epoch': 0.54}
 18%|█▊        | 91/507 [24:36<1:48:53, 15.71s/it] 18%|█▊        | 92/507 [24:52<1:48:34, 15.70s/it]                                                  {'loss': 0.0246, 'learning_rate': 0.00018840797986672538, 'epoch': 0.54}
 18%|█▊        | 92/507 [24:52<1:48:34, 15.70s/it] 18%|█▊        | 93/507 [25:08<1:49:20, 15.85s/it]                                                  {'loss': 0.0409, 'learning_rate': 0.0001881071540702406, 'epoch': 0.55}
 18%|█▊        | 93/507 [25:08<1:49:20, 15.85s/it] 19%|█▊        | 94/507 [25:24<1:48:44, 15.80s/it]                                                  {'loss': 0.0315, 'learning_rate': 0.00018780272127117607, 'epoch': 0.56}
 19%|█▊        | 94/507 [25:24<1:48:44, 15.80s/it] 19%|█▊        | 95/507 [25:40<1:48:22, 15.78s/it]                                                  {'loss': 0.037, 'learning_rate': 0.00018749469393265016, 'epoch': 0.56}
 19%|█▊        | 95/507 [25:40<1:48:22, 15.78s/it] 19%|█▉        | 96/507 [25:55<1:47:59, 15.76s/it]                                                  {'loss': 0.0608, 'learning_rate': 0.00018718308466493744, 'epoch': 0.57}
 19%|█▉        | 96/507 [25:55<1:47:59, 15.76s/it] 19%|█▉        | 97/507 [26:11<1:47:46, 15.77s/it]                                                  {'loss': 0.0366, 'learning_rate': 0.0001868679062249524, 'epoch': 0.57}
 19%|█▉        | 97/507 [26:11<1:47:46, 15.77s/it] 19%|█▉        | 98/507 [26:27<1:47:36, 15.79s/it]                                                  {'loss': 0.0324, 'learning_rate': 0.00018654917151572729, 'epoch': 0.58}
 19%|█▉        | 98/507 [26:27<1:47:36, 15.79s/it] 20%|█▉        | 99/507 [26:43<1:47:20, 15.78s/it]                                                  {'loss': 0.041, 'learning_rate': 0.00018622689358588373, 'epoch': 0.58}
 20%|█▉        | 99/507 [26:43<1:47:20, 15.78s/it] 20%|█▉        | 100/507 [26:59<1:48:49, 16.04s/it]                                                   {'loss': 0.0489, 'learning_rate': 0.00018590108562909863, 'epoch': 0.59}
 20%|█▉        | 100/507 [26:59<1:48:49, 16.04s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 20%|█▉        | 101/507 [27:24<2:06:07, 18.64s/it]                                                   {'loss': 0.0301, 'learning_rate': 0.00018557176098356405, 'epoch': 0.6}
 20%|█▉        | 101/507 [27:24<2:06:07, 18.64s/it] 20%|██        | 102/507 [27:40<1:59:44, 17.74s/it]                                                   {'loss': 0.041, 'learning_rate': 0.0001852389331314411, 'epoch': 0.6}
 20%|██        | 102/507 [27:40<1:59:44, 17.74s/it] 20%|██        | 103/507 [27:55<1:55:19, 17.13s/it]                                                   {'loss': 0.0341, 'learning_rate': 0.00018490261569830798, 'epoch': 0.61}
 20%|██        | 103/507 [27:55<1:55:19, 17.13s/it] 21%|██        | 104/507 [28:11<1:52:25, 16.74s/it]                                                   {'loss': 0.0241, 'learning_rate': 0.0001845628224526023, 'epoch': 0.61}
 21%|██        | 104/507 [28:11<1:52:25, 16.74s/it] 21%|██        | 105/507 [28:27<1:50:21, 16.47s/it]                                                   {'loss': 0.0826, 'learning_rate': 0.0001842195673050572, 'epoch': 0.62}
 21%|██        | 105/507 [28:27<1:50:21, 16.47s/it] 21%|██        | 106/507 [28:43<1:48:42, 16.27s/it]                                                   {'loss': 0.0331, 'learning_rate': 0.00018387286430813208, 'epoch': 0.63}
 21%|██        | 106/507 [28:43<1:48:42, 16.27s/it] 21%|██        | 107/507 [28:59<1:47:28, 16.12s/it]                                                   {'loss': 0.039, 'learning_rate': 0.00018352272765543722, 'epoch': 0.63}
 21%|██        | 107/507 [28:59<1:47:28, 16.12s/it] 21%|██▏       | 108/507 [29:14<1:46:31, 16.02s/it]                                                   {'loss': 0.0237, 'learning_rate': 0.0001831691716811526, 'epoch': 0.64}
 21%|██▏       | 108/507 [29:14<1:46:31, 16.02s/it] 21%|██▏       | 109/507 [29:30<1:45:46, 15.95s/it]                                                   {'loss': 0.0196, 'learning_rate': 0.00018281221085944126, 'epoch': 0.64}
 21%|██▏       | 109/507 [29:30<1:45:46, 15.95s/it] 22%|██▏       | 110/507 [29:46<1:45:10, 15.90s/it]                                                   {'loss': 0.0281, 'learning_rate': 0.0001824518598038567, 'epoch': 0.65}
 22%|██▏       | 110/507 [29:46<1:45:10, 15.90s/it] 22%|██▏       | 111/507 [30:02<1:44:38, 15.85s/it]                                                   {'loss': 0.0253, 'learning_rate': 0.00018208813326674444, 'epoch': 0.66}
 22%|██▏       | 111/507 [30:02<1:44:38, 15.85s/it] 22%|██▏       | 112/507 [30:17<1:44:06, 15.81s/it]                                                   {'loss': 0.0293, 'learning_rate': 0.00018172104613863835, 'epoch': 0.66}
 22%|██▏       | 112/507 [30:17<1:44:06, 15.81s/it] 22%|██▏       | 113/507 [30:33<1:43:40, 15.79s/it]                                                   {'loss': 0.0139, 'learning_rate': 0.00018135061344765088, 'epoch': 0.67}
 22%|██▏       | 113/507 [30:33<1:43:40, 15.79s/it] 22%|██▏       | 114/507 [30:49<1:43:02, 15.73s/it]                                                   {'loss': 0.0412, 'learning_rate': 0.0001809768503588578, 'epoch': 0.67}
 22%|██▏       | 114/507 [30:49<1:43:02, 15.73s/it] 23%|██▎       | 115/507 [31:04<1:42:46, 15.73s/it]                                                   {'loss': 0.0264, 'learning_rate': 0.00018059977217367755, 'epoch': 0.68}
 23%|██▎       | 115/507 [31:04<1:42:46, 15.73s/it] 23%|██▎       | 116/507 [31:20<1:42:26, 15.72s/it]                                                   {'loss': 0.0147, 'learning_rate': 0.00018021939432924454, 'epoch': 0.69}
 23%|██▎       | 116/507 [31:20<1:42:26, 15.72s/it] 23%|██▎       | 117/507 [31:36<1:42:13, 15.73s/it]                                                   {'loss': 0.0465, 'learning_rate': 0.00017983573239777748, 'epoch': 0.69}
 23%|██▎       | 117/507 [31:36<1:42:13, 15.73s/it] 23%|██▎       | 118/507 [31:52<1:41:58, 15.73s/it]                                                   {'loss': 0.0257, 'learning_rate': 0.00017944880208594155, 'epoch': 0.7}
 23%|██▎       | 118/507 [31:52<1:41:58, 15.73s/it] 23%|██▎       | 119/507 [32:07<1:41:41, 15.73s/it]                                                   {'loss': 0.016, 'learning_rate': 0.0001790586192342057, 'epoch': 0.7}
 23%|██▎       | 119/507 [32:07<1:41:41, 15.73s/it] 24%|██▎       | 120/507 [32:23<1:41:22, 15.72s/it]                                                   {'loss': 0.0316, 'learning_rate': 0.00017866519981619394, 'epoch': 0.71}
 24%|██▎       | 120/507 [32:23<1:41:22, 15.72s/it] 24%|██▍       | 121/507 [32:39<1:41:07, 15.72s/it]                                                   {'loss': 0.0388, 'learning_rate': 0.00017826855993803147, 'epoch': 0.71}
 24%|██▍       | 121/507 [32:39<1:41:07, 15.72s/it] 24%|██▍       | 122/507 [32:54<1:40:50, 15.72s/it]                                                   {'loss': 0.0611, 'learning_rate': 0.00017786871583768535, 'epoch': 0.72}
 24%|██▍       | 122/507 [32:54<1:40:50, 15.72s/it] 24%|██▍       | 123/507 [33:10<1:40:32, 15.71s/it]                                                   {'loss': 0.0529, 'learning_rate': 0.00017746568388429966, 'epoch': 0.73}
 24%|██▍       | 123/507 [33:10<1:40:32, 15.71s/it] 24%|██▍       | 124/507 [33:26<1:40:16, 15.71s/it]                                                   {'loss': 0.0402, 'learning_rate': 0.00017705948057752545, 'epoch': 0.73}
 24%|██▍       | 124/507 [33:26<1:40:16, 15.71s/it] 25%|██▍       | 125/507 [33:42<1:40:08, 15.73s/it]                                                   {'loss': 0.0473, 'learning_rate': 0.00017665012254684524, 'epoch': 0.74}
 25%|██▍       | 125/507 [33:42<1:40:08, 15.73s/it] 25%|██▍       | 126/507 [33:57<1:39:49, 15.72s/it]                                                   {'loss': 0.034, 'learning_rate': 0.00017623762655089207, 'epoch': 0.74}
 25%|██▍       | 126/507 [33:57<1:39:49, 15.72s/it] 25%|██▌       | 127/507 [34:13<1:40:07, 15.81s/it]                                                   {'loss': 0.0235, 'learning_rate': 0.0001758220094767638, 'epoch': 0.75}
 25%|██▌       | 127/507 [34:13<1:40:07, 15.81s/it] 25%|██▌       | 128/507 [34:29<1:39:39, 15.78s/it]                                                   {'loss': 0.0383, 'learning_rate': 0.0001754032883393313, 'epoch': 0.76}
 25%|██▌       | 128/507 [34:29<1:39:39, 15.78s/it] 25%|██▌       | 129/507 [34:45<1:39:24, 15.78s/it]                                                   {'loss': 0.0335, 'learning_rate': 0.0001749814802805423, 'epoch': 0.76}
 25%|██▌       | 129/507 [34:45<1:39:24, 15.78s/it] 26%|██▌       | 130/507 [35:01<1:39:29, 15.83s/it]                                                   {'loss': 0.0367, 'learning_rate': 0.00017455660256871931, 'epoch': 0.77}
 26%|██▌       | 130/507 [35:01<1:39:29, 15.83s/it] 26%|██▌       | 131/507 [35:18<1:41:18, 16.17s/it]                                                   {'loss': 0.0436, 'learning_rate': 0.00017412867259785286, 'epoch': 0.77}
 26%|██▌       | 131/507 [35:18<1:41:18, 16.17s/it] 26%|██▌       | 132/507 [35:34<1:40:14, 16.04s/it]                                                   {'loss': 0.0096, 'learning_rate': 0.00017369770788688938, 'epoch': 0.78}
 26%|██▌       | 132/507 [35:34<1:40:14, 16.04s/it] 26%|██▌       | 133/507 [35:49<1:39:33, 15.97s/it]                                                   {'loss': 0.0194, 'learning_rate': 0.00017326372607901386, 'epoch': 0.79}
 26%|██▌       | 133/507 [35:49<1:39:33, 15.97s/it] 26%|██▋       | 134/507 [36:05<1:39:00, 15.93s/it]                                                   {'loss': 0.0603, 'learning_rate': 0.0001728267449409278, 'epoch': 0.79}
 26%|██▋       | 134/507 [36:05<1:39:00, 15.93s/it] 27%|██▋       | 135/507 [36:21<1:38:33, 15.90s/it]                                                   {'loss': 0.0456, 'learning_rate': 0.0001723867823621216, 'epoch': 0.8}
 27%|██▋       | 135/507 [36:21<1:38:33, 15.90s/it] 27%|██▋       | 136/507 [36:37<1:38:23, 15.91s/it]                                                   {'loss': 0.0554, 'learning_rate': 0.00017194385635414244, 'epoch': 0.8}
 27%|██▋       | 136/507 [36:37<1:38:23, 15.91s/it] 27%|██▋       | 137/507 [36:53<1:37:52, 15.87s/it]                                                   {'loss': 0.035, 'learning_rate': 0.00017149798504985665, 'epoch': 0.81}
 27%|██▋       | 137/507 [36:53<1:37:52, 15.87s/it] 27%|██▋       | 138/507 [37:08<1:37:28, 15.85s/it]                                                   {'loss': 0.0344, 'learning_rate': 0.00017104918670270762, 'epoch': 0.82}
 27%|██▋       | 138/507 [37:08<1:37:28, 15.85s/it] 27%|██▋       | 139/507 [37:24<1:37:04, 15.83s/it]                                                   {'loss': 0.0431, 'learning_rate': 0.00017059747968596836, 'epoch': 0.82}
 27%|██▋       | 139/507 [37:24<1:37:04, 15.83s/it] 28%|██▊       | 140/507 [37:40<1:36:38, 15.80s/it]                                                   {'loss': 0.0145, 'learning_rate': 0.00017014288249198934, 'epoch': 0.83}
 28%|██▊       | 140/507 [37:40<1:36:38, 15.80s/it] 28%|██▊       | 141/507 [37:56<1:36:21, 15.80s/it]                                                   {'loss': 0.0572, 'learning_rate': 0.00016968541373144156, 'epoch': 0.83}
 28%|██▊       | 141/507 [37:56<1:36:21, 15.80s/it] 28%|██▊       | 142/507 [38:12<1:35:57, 15.77s/it]                                                   {'loss': 0.0107, 'learning_rate': 0.0001692250921325544, 'epoch': 0.84}
 28%|██▊       | 142/507 [38:12<1:35:57, 15.77s/it] 28%|██▊       | 143/507 [38:27<1:35:21, 15.72s/it]                                                   {'loss': 0.0345, 'learning_rate': 0.0001687619365403492, 'epoch': 0.84}
 28%|██▊       | 143/507 [38:27<1:35:21, 15.72s/it] 28%|██▊       | 144/507 [38:43<1:35:09, 15.73s/it]                                                   {'loss': 0.0511, 'learning_rate': 0.0001682959659158676, 'epoch': 0.85}
 28%|██▊       | 144/507 [38:43<1:35:09, 15.73s/it] 29%|██▊       | 145/507 [38:59<1:34:50, 15.72s/it]                                                   {'loss': 0.0145, 'learning_rate': 0.0001678271993353953, 'epoch': 0.86}
 29%|██▊       | 145/507 [38:59<1:34:50, 15.72s/it] 29%|██▉       | 146/507 [39:14<1:34:31, 15.71s/it]                                                   {'loss': 0.0227, 'learning_rate': 0.00016735565598968114, 'epoch': 0.86}
 29%|██▉       | 146/507 [39:14<1:34:31, 15.71s/it] 29%|██▉       | 147/507 [39:30<1:34:14, 15.71s/it]                                                   {'loss': 0.0303, 'learning_rate': 0.00016688135518315144, 'epoch': 0.87}
 29%|██▉       | 147/507 [39:30<1:34:14, 15.71s/it] 29%|██▉       | 148/507 [39:46<1:33:58, 15.71s/it]                                                   {'loss': 0.0305, 'learning_rate': 0.00016640431633311973, 'epoch': 0.87}
 29%|██▉       | 148/507 [39:46<1:33:58, 15.71s/it] 29%|██▉       | 149/507 [40:01<1:33:38, 15.69s/it]                                                   {'loss': 0.0329, 'learning_rate': 0.00016592455896899177, 'epoch': 0.88}
 29%|██▉       | 149/507 [40:01<1:33:38, 15.69s/it] 30%|██▉       | 150/507 [40:17<1:33:21, 15.69s/it]                                                   {'loss': 0.017, 'learning_rate': 0.00016544210273146607, 'epoch': 0.89}
 30%|██▉       | 150/507 [40:17<1:33:21, 15.69s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 30%|██▉       | 151/507 [40:41<1:47:09, 18.06s/it]                                                   {'loss': 0.038, 'learning_rate': 0.0001649569673717298, 'epoch': 0.89}
 30%|██▉       | 151/507 [40:41<1:47:09, 18.06s/it] 30%|██▉       | 152/507 [40:56<1:42:24, 17.31s/it]                                                   {'loss': 0.0451, 'learning_rate': 0.0001644691727506503, 'epoch': 0.9}
 30%|██▉       | 152/507 [40:56<1:42:24, 17.31s/it] 30%|███       | 153/507 [41:12<1:39:10, 16.81s/it]                                                   {'loss': 0.022, 'learning_rate': 0.00016397873883796182, 'epoch': 0.9}
 30%|███       | 153/507 [41:12<1:39:10, 16.81s/it] 30%|███       | 154/507 [41:27<1:36:54, 16.47s/it]                                                   {'loss': 0.0267, 'learning_rate': 0.00016348568571144815, 'epoch': 0.91}
 30%|███       | 154/507 [41:27<1:36:54, 16.47s/it] 31%|███       | 155/507 [41:44<1:36:08, 16.39s/it]                                                   {'loss': 0.0459, 'learning_rate': 0.0001629900335561206, 'epoch': 0.92}
 31%|███       | 155/507 [41:44<1:36:08, 16.39s/it] 31%|███       | 156/507 [41:59<1:34:45, 16.20s/it]                                                   {'loss': 0.0505, 'learning_rate': 0.0001624918026633916, 'epoch': 0.92}
 31%|███       | 156/507 [41:59<1:34:45, 16.20s/it] 31%|███       | 157/507 [42:15<1:33:40, 16.06s/it]                                                   {'loss': 0.0258, 'learning_rate': 0.00016199101343024403, 'epoch': 0.93}
 31%|███       | 157/507 [42:15<1:33:40, 16.06s/it] 31%|███       | 158/507 [42:31<1:32:56, 15.98s/it]                                                   {'loss': 0.0291, 'learning_rate': 0.00016148768635839623, 'epoch': 0.93}
 31%|███       | 158/507 [42:31<1:32:56, 15.98s/it] 31%|███▏      | 159/507 [42:47<1:32:20, 15.92s/it]                                                   {'loss': 0.0339, 'learning_rate': 0.0001609818420534627, 'epoch': 0.94}
 31%|███▏      | 159/507 [42:47<1:32:20, 15.92s/it] 32%|███▏      | 160/507 [43:03<1:31:50, 15.88s/it]                                                   {'loss': 0.0351, 'learning_rate': 0.00016047350122411037, 'epoch': 0.95}
 32%|███▏      | 160/507 [43:03<1:31:50, 15.88s/it] 32%|███▏      | 161/507 [43:18<1:31:27, 15.86s/it]                                                   {'loss': 0.045, 'learning_rate': 0.00015996268468121102, 'epoch': 0.95}
 32%|███▏      | 161/507 [43:18<1:31:27, 15.86s/it] 32%|███▏      | 162/507 [43:34<1:31:04, 15.84s/it]                                                   {'loss': 0.0314, 'learning_rate': 0.00015944941333698913, 'epoch': 0.96}
 32%|███▏      | 162/507 [43:34<1:31:04, 15.84s/it] 32%|███▏      | 163/507 [43:50<1:30:48, 15.84s/it]                                                   {'loss': 0.0588, 'learning_rate': 0.00015893370820416593, 'epoch': 0.96}
 32%|███▏      | 163/507 [43:50<1:30:48, 15.84s/it] 32%|███▏      | 164/507 [44:06<1:30:27, 15.83s/it]                                                   {'loss': 0.0349, 'learning_rate': 0.00015841559039509896, 'epoch': 0.97}
 32%|███▏      | 164/507 [44:06<1:30:27, 15.83s/it] 33%|███▎      | 165/507 [44:22<1:30:20, 15.85s/it]                                                   {'loss': 0.027, 'learning_rate': 0.00015789508112091803, 'epoch': 0.97}
 33%|███▎      | 165/507 [44:22<1:30:20, 15.85s/it] 33%|███▎      | 166/507 [44:38<1:30:13, 15.88s/it]                                                   {'loss': 0.0386, 'learning_rate': 0.00015737220169065655, 'epoch': 0.98}
 33%|███▎      | 166/507 [44:38<1:30:13, 15.88s/it] 33%|███▎      | 167/507 [44:53<1:29:49, 15.85s/it]                                                   {'loss': 0.0311, 'learning_rate': 0.00015684697351037936, 'epoch': 0.99}
 33%|███▎      | 167/507 [44:53<1:29:49, 15.85s/it] 33%|███▎      | 168/507 [45:09<1:29:33, 15.85s/it]                                                   {'loss': 0.0126, 'learning_rate': 0.00015631941808230638, 'epoch': 0.99}
 33%|███▎      | 168/507 [45:09<1:29:33, 15.85s/it] 33%|███▎      | 169/507 [45:25<1:29:20, 15.86s/it]                                                   {'loss': 0.0259, 'learning_rate': 0.00015578955700393227, 'epoch': 1.0}
 33%|███▎      | 169/507 [45:25<1:29:20, 15.86s/it] 34%|███▎      | 170/507 [45:44<1:33:39, 16.67s/it]                                                   {'loss': 0.0445, 'learning_rate': 0.0001552574119671423, 'epoch': 1.0}
 34%|███▎      | 170/507 [45:44<1:33:39, 16.67s/it] 34%|███▎      | 171/507 [45:59<1:31:48, 16.39s/it]                                                   {'loss': 0.0226, 'learning_rate': 0.00015472300475732426, 'epoch': 1.01}
 34%|███▎      | 171/507 [45:59<1:31:48, 16.39s/it] 34%|███▍      | 172/507 [46:15<1:30:19, 16.18s/it]                                                   {'loss': 0.0377, 'learning_rate': 0.00015418635725247666, 'epoch': 1.02}
 34%|███▍      | 172/507 [46:15<1:30:19, 16.18s/it] 34%|███▍      | 173/507 [46:31<1:29:28, 16.07s/it]                                                   {'loss': 0.0225, 'learning_rate': 0.00015364749142231303, 'epoch': 1.02}
 34%|███▍      | 173/507 [46:31<1:29:28, 16.07s/it] 34%|███▍      | 174/507 [46:47<1:28:43, 15.99s/it]                                                   {'loss': 0.0467, 'learning_rate': 0.00015310642932736253, 'epoch': 1.03}
 34%|███▍      | 174/507 [46:47<1:28:43, 15.99s/it] 35%|███▍      | 175/507 [47:02<1:28:02, 15.91s/it]                                                   {'loss': 0.0269, 'learning_rate': 0.00015256319311806671, 'epoch': 1.03}
 35%|███▍      | 175/507 [47:02<1:28:02, 15.91s/it] 35%|███▍      | 176/507 [47:18<1:27:37, 15.89s/it]                                                   {'loss': 0.0363, 'learning_rate': 0.0001520178050338729, 'epoch': 1.04}
 35%|███▍      | 176/507 [47:18<1:27:37, 15.89s/it] 35%|███▍      | 177/507 [47:34<1:27:07, 15.84s/it]                                                   {'loss': 0.0161, 'learning_rate': 0.0001514702874023236, 'epoch': 1.05}
 35%|███▍      | 177/507 [47:34<1:27:07, 15.84s/it] 35%|███▌      | 178/507 [47:50<1:26:45, 15.82s/it]                                                   {'loss': 0.0293, 'learning_rate': 0.00015092066263814243, 'epoch': 1.05}
 35%|███▌      | 178/507 [47:50<1:26:45, 15.82s/it] 35%|███▌      | 179/507 [48:06<1:26:23, 15.80s/it]                                                   {'loss': 0.0121, 'learning_rate': 0.0001503689532423166, 'epoch': 1.06}
 35%|███▌      | 179/507 [48:06<1:26:23, 15.80s/it] 36%|███▌      | 180/507 [48:21<1:26:00, 15.78s/it]                                                   {'loss': 0.0248, 'learning_rate': 0.00014981518180117557, 'epoch': 1.06}
 36%|███▌      | 180/507 [48:21<1:26:00, 15.78s/it] 36%|███▌      | 181/507 [48:37<1:25:41, 15.77s/it]                                                   {'loss': 0.0378, 'learning_rate': 0.00014925937098546652, 'epoch': 1.07}
 36%|███▌      | 181/507 [48:37<1:25:41, 15.77s/it] 36%|███▌      | 182/507 [48:53<1:25:17, 15.75s/it]                                                   {'loss': 0.0386, 'learning_rate': 0.0001487015435494263, 'epoch': 1.08}
 36%|███▌      | 182/507 [48:53<1:25:17, 15.75s/it] 36%|███▌      | 183/507 [49:08<1:24:57, 15.73s/it]                                                   {'loss': 0.0178, 'learning_rate': 0.00014814172232984968, 'epoch': 1.08}
 36%|███▌      | 183/507 [49:08<1:24:57, 15.73s/it] 36%|███▋      | 184/507 [49:24<1:24:41, 15.73s/it]                                                   {'loss': 0.0092, 'learning_rate': 0.0001475799302451547, 'epoch': 1.09}
 36%|███▋      | 184/507 [49:24<1:24:41, 15.73s/it] 36%|███▋      | 185/507 [49:40<1:24:24, 15.73s/it]                                                   {'loss': 0.0343, 'learning_rate': 0.0001470161902944442, 'epoch': 1.09}
 36%|███▋      | 185/507 [49:40<1:24:24, 15.73s/it] 37%|███▋      | 186/507 [49:56<1:24:06, 15.72s/it]                                                   {'loss': 0.0366, 'learning_rate': 0.00014645052555656431, 'epoch': 1.1}
 37%|███▋      | 186/507 [49:56<1:24:06, 15.72s/it] 37%|███▋      | 187/507 [50:13<1:25:52, 16.10s/it]                                                   {'loss': 0.0143, 'learning_rate': 0.00014588295918915978, 'epoch': 1.1}
 37%|███▋      | 187/507 [50:13<1:25:52, 16.10s/it] 37%|███▋      | 188/507 [50:28<1:24:54, 15.97s/it]                                                   {'loss': 0.0191, 'learning_rate': 0.0001453135144277257, 'epoch': 1.11}
 37%|███▋      | 188/507 [50:28<1:24:54, 15.97s/it] 37%|███▋      | 189/507 [50:44<1:24:16, 15.90s/it]                                                   {'loss': 0.0316, 'learning_rate': 0.0001447422145846565, 'epoch': 1.12}
 37%|███▋      | 189/507 [50:44<1:24:16, 15.90s/it] 37%|███▋      | 190/507 [51:00<1:23:47, 15.86s/it]                                                   {'loss': 0.0222, 'learning_rate': 0.00014416908304829142, 'epoch': 1.12}
 37%|███▋      | 190/507 [51:00<1:23:47, 15.86s/it] 38%|███▊      | 191/507 [51:15<1:23:19, 15.82s/it]                                                   {'loss': 0.0177, 'learning_rate': 0.00014359414328195703, 'epoch': 1.13}
 38%|███▊      | 191/507 [51:15<1:23:19, 15.82s/it] 38%|███▊      | 192/507 [51:31<1:22:58, 15.80s/it]                                                   {'loss': 0.0152, 'learning_rate': 0.00014301741882300672, 'epoch': 1.13}
 38%|███▊      | 192/507 [51:31<1:22:58, 15.80s/it] 38%|███▊      | 193/507 [51:47<1:22:27, 15.76s/it]                                                   {'loss': 0.0296, 'learning_rate': 0.000142438933281857, 'epoch': 1.14}
 38%|███▊      | 193/507 [51:47<1:22:27, 15.76s/it] 38%|███▊      | 194/507 [52:03<1:22:10, 15.75s/it]                                                   {'loss': 0.0099, 'learning_rate': 0.00014185871034102116, 'epoch': 1.15}
 38%|███▊      | 194/507 [52:03<1:22:10, 15.75s/it] 38%|███▊      | 195/507 [52:18<1:21:42, 15.71s/it]                                                   {'loss': 0.0541, 'learning_rate': 0.00014127677375413942, 'epoch': 1.15}
 38%|███▊      | 195/507 [52:18<1:21:42, 15.71s/it] 39%|███▊      | 196/507 [52:34<1:21:33, 15.73s/it]                                                   {'loss': 0.0107, 'learning_rate': 0.00014069314734500675, 'epoch': 1.16}
 39%|███▊      | 196/507 [52:34<1:21:33, 15.73s/it] 39%|███▉      | 197/507 [52:50<1:21:24, 15.76s/it]                                                   {'loss': 0.0112, 'learning_rate': 0.00014010785500659736, 'epoch': 1.16}
 39%|███▉      | 197/507 [52:50<1:21:24, 15.76s/it] 39%|███▉      | 198/507 [53:06<1:21:10, 15.76s/it]                                                   {'loss': 0.0111, 'learning_rate': 0.0001395209207000867, 'epoch': 1.17}
 39%|███▉      | 198/507 [53:06<1:21:10, 15.76s/it] 39%|███▉      | 199/507 [53:21<1:21:03, 15.79s/it]                                                   {'loss': 0.0223, 'learning_rate': 0.00013893236845387042, 'epoch': 1.18}
 39%|███▉      | 199/507 [53:21<1:21:03, 15.79s/it] 39%|███▉      | 200/507 [53:37<1:20:52, 15.81s/it]                                                   {'loss': 0.0049, 'learning_rate': 0.0001383422223625807, 'epoch': 1.18}
 39%|███▉      | 200/507 [53:37<1:20:52, 15.81s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 40%|███▉      | 201/507 [54:01<1:32:27, 18.13s/it]                                                   {'loss': 0.019, 'learning_rate': 0.00013775050658609988, 'epoch': 1.19}
 40%|███▉      | 201/507 [54:01<1:32:27, 18.13s/it] 40%|███▉      | 202/507 [54:17<1:28:29, 17.41s/it]                                                   {'loss': 0.0383, 'learning_rate': 0.00013715724534857127, 'epoch': 1.19}
 40%|███▉      | 202/507 [54:17<1:28:29, 17.41s/it] 40%|████      | 203/507 [54:32<1:25:42, 16.91s/it]                                                   {'loss': 0.0567, 'learning_rate': 0.00013656246293740766, 'epoch': 1.2}
 40%|████      | 203/507 [54:32<1:25:42, 16.91s/it] 40%|████      | 204/507 [54:48<1:23:37, 16.56s/it]                                                   {'loss': 0.0344, 'learning_rate': 0.0001359661837022968, 'epoch': 1.21}
 40%|████      | 204/507 [54:48<1:23:37, 16.56s/it] 40%|████      | 205/507 [55:04<1:22:10, 16.33s/it]                                                   {'loss': 0.0126, 'learning_rate': 0.0001353684320542046, 'epoch': 1.21}
 40%|████      | 205/507 [55:04<1:22:10, 16.33s/it] 41%|████      | 206/507 [55:20<1:22:10, 16.38s/it]                                                   {'loss': 0.0158, 'learning_rate': 0.0001347692324643759, 'epoch': 1.22}
 41%|████      | 206/507 [55:20<1:22:10, 16.38s/it] 41%|████      | 207/507 [55:36<1:20:53, 16.18s/it]                                                   {'loss': 0.0205, 'learning_rate': 0.00013416860946333255, 'epoch': 1.22}
 41%|████      | 207/507 [55:36<1:20:53, 16.18s/it] 41%|████      | 208/507 [55:52<1:20:10, 16.09s/it]                                                   {'loss': 0.0318, 'learning_rate': 0.00013356658763986917, 'epoch': 1.23}
 41%|████      | 208/507 [55:52<1:20:10, 16.09s/it] 41%|████      | 209/507 [56:08<1:19:22, 15.98s/it]                                                   {'loss': 0.0139, 'learning_rate': 0.00013296319164004644, 'epoch': 1.23}
 41%|████      | 209/507 [56:08<1:19:22, 15.98s/it] 41%|████▏     | 210/507 [56:23<1:18:45, 15.91s/it]                                                   {'loss': 0.0215, 'learning_rate': 0.0001323584461661823, 'epoch': 1.24}
 41%|████▏     | 210/507 [56:23<1:18:45, 15.91s/it] 42%|████▏     | 211/507 [56:39<1:18:14, 15.86s/it]                                                   {'loss': 0.0302, 'learning_rate': 0.00013175237597584045, 'epoch': 1.25}
 42%|████▏     | 211/507 [56:39<1:18:14, 15.86s/it] 42%|████▏     | 212/507 [56:55<1:17:47, 15.82s/it]                                                   {'loss': 0.0074, 'learning_rate': 0.00013114500588081698, 'epoch': 1.25}
 42%|████▏     | 212/507 [56:55<1:17:47, 15.82s/it] 42%|████▏     | 213/507 [57:11<1:17:27, 15.81s/it]                                                   {'loss': 0.0088, 'learning_rate': 0.00013053636074612457, 'epoch': 1.26}
 42%|████▏     | 213/507 [57:11<1:17:27, 15.81s/it] 42%|████▏     | 214/507 [57:26<1:16:59, 15.77s/it]                                                   {'loss': 0.0192, 'learning_rate': 0.00012992646548897442, 'epoch': 1.26}
 42%|████▏     | 214/507 [57:26<1:16:59, 15.77s/it] 42%|████▏     | 215/507 [57:42<1:16:32, 15.73s/it]                                                   {'loss': 0.0375, 'learning_rate': 0.0001293153450777564, 'epoch': 1.27}
 42%|████▏     | 215/507 [57:42<1:16:32, 15.73s/it] 43%|████▎     | 216/507 [57:58<1:16:20, 15.74s/it]                                                   {'loss': 0.0197, 'learning_rate': 0.00012870302453101657, 'epoch': 1.28}
 43%|████▎     | 216/507 [57:58<1:16:20, 15.74s/it] 43%|████▎     | 217/507 [58:13<1:16:02, 15.73s/it]                                                   {'loss': 0.0331, 'learning_rate': 0.00012808952891643326, 'epoch': 1.28}
 43%|████▎     | 217/507 [58:13<1:16:02, 15.73s/it] 43%|████▎     | 218/507 [58:29<1:15:48, 15.74s/it]                                                   {'loss': 0.0239, 'learning_rate': 0.00012747488334979062, 'epoch': 1.29}
 43%|████▎     | 218/507 [58:29<1:15:48, 15.74s/it] 43%|████▎     | 219/507 [58:45<1:15:32, 15.74s/it]                                                   {'loss': 0.0222, 'learning_rate': 0.00012685911299395046, 'epoch': 1.29}
 43%|████▎     | 219/507 [58:45<1:15:32, 15.74s/it] 43%|████▎     | 220/507 [59:01<1:15:13, 15.73s/it]                                                   {'loss': 0.0146, 'learning_rate': 0.00012624224305782215, 'epoch': 1.3}
 43%|████▎     | 220/507 [59:01<1:15:13, 15.73s/it] 44%|████▎     | 221/507 [59:16<1:14:54, 15.72s/it]                                                   {'loss': 0.0231, 'learning_rate': 0.0001256242987953306, 'epoch': 1.31}
 44%|████▎     | 221/507 [59:16<1:14:54, 15.72s/it] 44%|████▍     | 222/507 [59:32<1:14:41, 15.72s/it]                                                   {'loss': 0.0129, 'learning_rate': 0.00012500530550438232, 'epoch': 1.31}
 44%|████▍     | 222/507 [59:32<1:14:41, 15.72s/it] 44%|████▍     | 223/507 [59:48<1:14:31, 15.74s/it]                                                   {'loss': 0.0514, 'learning_rate': 0.00012438528852582988, 'epoch': 1.32}
 44%|████▍     | 223/507 [59:48<1:14:31, 15.74s/it] 44%|████▍     | 224/507 [1:00:04<1:14:17, 15.75s/it]                                                     {'loss': 0.0334, 'learning_rate': 0.00012376427324243432, 'epoch': 1.32}
 44%|████▍     | 224/507 [1:00:04<1:14:17, 15.75s/it] 44%|████▍     | 225/507 [1:00:19<1:14:02, 15.75s/it]                                                     {'loss': 0.0144, 'learning_rate': 0.00012314228507782614, 'epoch': 1.33}
 44%|████▍     | 225/507 [1:00:19<1:14:02, 15.75s/it] 45%|████▍     | 226/507 [1:00:35<1:13:49, 15.76s/it]                                                     {'loss': 0.0438, 'learning_rate': 0.00012251934949546447, 'epoch': 1.34}
 45%|████▍     | 226/507 [1:00:35<1:13:49, 15.76s/it] 45%|████▍     | 227/507 [1:00:51<1:13:31, 15.75s/it]                                                     {'loss': 0.0053, 'learning_rate': 0.00012189549199759453, 'epoch': 1.34}
 45%|████▍     | 227/507 [1:00:51<1:13:31, 15.75s/it] 45%|████▍     | 228/507 [1:01:07<1:13:18, 15.77s/it]                                                     {'loss': 0.0483, 'learning_rate': 0.00012127073812420375, 'epoch': 1.35}
 45%|████▍     | 228/507 [1:01:07<1:13:18, 15.77s/it] 45%|████▌     | 229/507 [1:01:23<1:13:08, 15.79s/it]                                                     {'loss': 0.0242, 'learning_rate': 0.00012064511345197607, 'epoch': 1.35}
 45%|████▌     | 229/507 [1:01:23<1:13:08, 15.79s/it] 45%|████▌     | 230/507 [1:01:38<1:12:52, 15.79s/it]                                                     {'loss': 0.0178, 'learning_rate': 0.00012001864359324489, 'epoch': 1.36}
 45%|████▌     | 230/507 [1:01:38<1:12:52, 15.79s/it] 46%|████▌     | 231/507 [1:01:54<1:12:40, 15.80s/it]                                                     {'loss': 0.0296, 'learning_rate': 0.00011939135419494456, 'epoch': 1.36}
 46%|████▌     | 231/507 [1:01:54<1:12:40, 15.80s/it] 46%|████▌     | 232/507 [1:02:10<1:12:27, 15.81s/it]                                                     {'loss': 0.0143, 'learning_rate': 0.00011876327093756047, 'epoch': 1.37}
 46%|████▌     | 232/507 [1:02:10<1:12:27, 15.81s/it] 46%|████▌     | 233/507 [1:02:26<1:12:15, 15.82s/it]                                                     {'loss': 0.0107, 'learning_rate': 0.00011813441953407754, 'epoch': 1.38}
 46%|████▌     | 233/507 [1:02:26<1:12:15, 15.82s/it] 46%|████▌     | 234/507 [1:02:42<1:11:53, 15.80s/it]                                                     {'loss': 0.0132, 'learning_rate': 0.0001175048257289278, 'epoch': 1.38}
 46%|████▌     | 234/507 [1:02:42<1:11:53, 15.80s/it] 46%|████▋     | 235/507 [1:02:57<1:11:26, 15.76s/it]                                                     {'loss': 0.017, 'learning_rate': 0.00011687451529693624, 'epoch': 1.39}
 46%|████▋     | 235/507 [1:02:57<1:11:26, 15.76s/it] 47%|████▋     | 236/507 [1:03:13<1:11:18, 15.79s/it]                                                     {'loss': 0.0353, 'learning_rate': 0.00011624351404226572, 'epoch': 1.39}
 47%|████▋     | 236/507 [1:03:13<1:11:18, 15.79s/it] 47%|████▋     | 237/507 [1:03:29<1:11:04, 15.80s/it]                                                     {'loss': 0.0123, 'learning_rate': 0.00011561184779736061, 'epoch': 1.4}
 47%|████▋     | 237/507 [1:03:29<1:11:04, 15.80s/it] 47%|████▋     | 238/507 [1:03:46<1:11:53, 16.03s/it]                                                     {'loss': 0.0247, 'learning_rate': 0.00011497954242188913, 'epoch': 1.41}
 47%|████▋     | 238/507 [1:03:46<1:11:53, 16.03s/it] 47%|████▋     | 239/507 [1:04:01<1:11:10, 15.94s/it]                                                     {'loss': 0.0303, 'learning_rate': 0.00011434662380168486, 'epoch': 1.41}
 47%|████▋     | 239/507 [1:04:01<1:11:10, 15.94s/it] 47%|████▋     | 240/507 [1:04:17<1:10:39, 15.88s/it]                                                     {'loss': 0.0191, 'learning_rate': 0.00011371311784768673, 'epoch': 1.42}
 47%|████▋     | 240/507 [1:04:17<1:10:39, 15.88s/it] 48%|████▊     | 241/507 [1:04:33<1:10:13, 15.84s/it]                                                     {'loss': 0.0142, 'learning_rate': 0.00011307905049487855, 'epoch': 1.42}
 48%|████▊     | 241/507 [1:04:33<1:10:13, 15.84s/it] 48%|████▊     | 242/507 [1:04:48<1:09:48, 15.81s/it]                                                     {'loss': 0.0165, 'learning_rate': 0.00011244444770122707, 'epoch': 1.43}
 48%|████▊     | 242/507 [1:04:48<1:09:48, 15.81s/it] 48%|████▊     | 243/507 [1:05:04<1:09:29, 15.80s/it]                                                     {'loss': 0.0333, 'learning_rate': 0.00011180933544661936, 'epoch': 1.44}
 48%|████▊     | 243/507 [1:05:04<1:09:29, 15.80s/it] 48%|████▊     | 244/507 [1:05:20<1:09:09, 15.78s/it]                                                     {'loss': 0.0227, 'learning_rate': 0.00011117373973179925, 'epoch': 1.44}
 48%|████▊     | 244/507 [1:05:20<1:09:09, 15.78s/it] 48%|████▊     | 245/507 [1:05:36<1:08:48, 15.76s/it]                                                     {'loss': 0.0285, 'learning_rate': 0.00011053768657730284, 'epoch': 1.45}
 48%|████▊     | 245/507 [1:05:36<1:08:48, 15.76s/it] 49%|████▊     | 246/507 [1:05:51<1:08:31, 15.75s/it]                                                     {'loss': 0.0128, 'learning_rate': 0.00010990120202239324, 'epoch': 1.45}
 49%|████▊     | 246/507 [1:05:51<1:08:31, 15.75s/it] 49%|████▊     | 247/507 [1:06:07<1:08:09, 15.73s/it]                                                     {'loss': 0.0171, 'learning_rate': 0.00010926431212399466, 'epoch': 1.46}
 49%|████▊     | 247/507 [1:06:07<1:08:09, 15.73s/it] 49%|████▉     | 248/507 [1:06:23<1:07:52, 15.72s/it]                                                     {'loss': 0.045, 'learning_rate': 0.0001086270429556255, 'epoch': 1.47}
 49%|████▉     | 248/507 [1:06:23<1:07:52, 15.72s/it] 49%|████▉     | 249/507 [1:06:38<1:07:32, 15.71s/it]                                                     {'loss': 0.0182, 'learning_rate': 0.00010798942060633108, 'epoch': 1.47}
 49%|████▉     | 249/507 [1:06:38<1:07:32, 15.71s/it] 49%|████▉     | 250/507 [1:06:54<1:07:17, 15.71s/it]                                                     {'loss': 0.0079, 'learning_rate': 0.0001073514711796155, 'epoch': 1.48}
 49%|████▉     | 250/507 [1:06:54<1:07:17, 15.71s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 50%|████▉     | 251/507 [1:07:18<1:17:36, 18.19s/it]                                                     {'loss': 0.0124, 'learning_rate': 0.00010671322079237307, 'epoch': 1.48}
 50%|████▉     | 251/507 [1:07:18<1:17:36, 18.19s/it] 50%|████▉     | 252/507 [1:07:34<1:13:57, 17.40s/it]                                                     {'loss': 0.0393, 'learning_rate': 0.00010607469557381899, 'epoch': 1.49}
 50%|████▉     | 252/507 [1:07:34<1:13:57, 17.40s/it] 50%|████▉     | 253/507 [1:07:49<1:11:22, 16.86s/it]                                                     {'loss': 0.0064, 'learning_rate': 0.00010543592166441983, 'epoch': 1.49}
 50%|████▉     | 253/507 [1:07:49<1:11:22, 16.86s/it] 50%|█████     | 254/507 [1:08:05<1:09:36, 16.51s/it]                                                     {'loss': 0.0135, 'learning_rate': 0.00010479692521482316, 'epoch': 1.5}
 50%|█████     | 254/507 [1:08:05<1:09:36, 16.51s/it] 50%|█████     | 255/507 [1:08:21<1:08:19, 16.27s/it]                                                     {'loss': 0.0291, 'learning_rate': 0.00010415773238478715, 'epoch': 1.51}
 50%|█████     | 255/507 [1:08:21<1:08:19, 16.27s/it] 50%|█████     | 256/507 [1:08:36<1:07:23, 16.11s/it]                                                     {'loss': 0.0293, 'learning_rate': 0.00010351836934210957, 'epoch': 1.51}
 50%|█████     | 256/507 [1:08:36<1:07:23, 16.11s/it] 51%|█████     | 257/507 [1:08:52<1:06:40, 16.00s/it]                                                     {'loss': 0.0248, 'learning_rate': 0.00010287886226155641, 'epoch': 1.52}
 51%|█████     | 257/507 [1:08:52<1:06:40, 16.00s/it] 51%|█████     | 258/507 [1:09:09<1:07:14, 16.20s/it]                                                     {'loss': 0.0169, 'learning_rate': 0.00010223923732379048, 'epoch': 1.52}
 51%|█████     | 258/507 [1:09:09<1:07:14, 16.20s/it] 51%|█████     | 259/507 [1:09:25<1:06:22, 16.06s/it]                                                     {'loss': 0.0272, 'learning_rate': 0.00010159952071429952, 'epoch': 1.53}
 51%|█████     | 259/507 [1:09:25<1:06:22, 16.06s/it] 51%|█████▏    | 260/507 [1:09:40<1:05:42, 15.96s/it]                                                     {'loss': 0.0227, 'learning_rate': 0.0001009597386223241, 'epoch': 1.54}
 51%|█████▏    | 260/507 [1:09:40<1:05:42, 15.96s/it] 51%|█████▏    | 261/507 [1:09:57<1:06:49, 16.30s/it]                                                     {'loss': 0.025, 'learning_rate': 0.00010031991723978574, 'epoch': 1.54}
 51%|█████▏    | 261/507 [1:09:57<1:06:49, 16.30s/it] 52%|█████▏    | 262/507 [1:10:13<1:05:49, 16.12s/it]                                                     {'loss': 0.0394, 'learning_rate': 9.96800827602143e-05, 'epoch': 1.55}
 52%|█████▏    | 262/507 [1:10:13<1:05:49, 16.12s/it] 52%|█████▏    | 263/507 [1:10:29<1:05:03, 16.00s/it]                                                     {'loss': 0.0035, 'learning_rate': 9.90402613776759e-05, 'epoch': 1.55}
 52%|█████▏    | 263/507 [1:10:29<1:05:03, 16.00s/it] 52%|█████▏    | 264/507 [1:10:45<1:04:33, 15.94s/it]                                                     {'loss': 0.0389, 'learning_rate': 9.84004792857005e-05, 'epoch': 1.56}
 52%|█████▏    | 264/507 [1:10:45<1:04:33, 15.94s/it] 52%|█████▏    | 265/507 [1:11:00<1:04:10, 15.91s/it]                                                     {'loss': 0.0207, 'learning_rate': 9.776076267620955e-05, 'epoch': 1.57}
 52%|█████▏    | 265/507 [1:11:00<1:04:10, 15.91s/it] 52%|█████▏    | 266/507 [1:11:16<1:03:50, 15.90s/it]                                                     {'loss': 0.0348, 'learning_rate': 9.712113773844361e-05, 'epoch': 1.57}
 52%|█████▏    | 266/507 [1:11:16<1:03:50, 15.90s/it] 53%|█████▎    | 267/507 [1:11:32<1:03:29, 15.87s/it]                                                     {'loss': 0.0253, 'learning_rate': 9.648163065789045e-05, 'epoch': 1.58}
 53%|█████▎    | 267/507 [1:11:32<1:03:29, 15.87s/it] 53%|█████▎    | 268/507 [1:11:48<1:03:10, 15.86s/it]                                                     {'loss': 0.0283, 'learning_rate': 9.584226761521285e-05, 'epoch': 1.58}
 53%|█████▎    | 268/507 [1:11:48<1:03:10, 15.86s/it] 53%|█████▎    | 269/507 [1:12:04<1:02:47, 15.83s/it]                                                     {'loss': 0.0111, 'learning_rate': 9.520307478517686e-05, 'epoch': 1.59}
 53%|█████▎    | 269/507 [1:12:04<1:02:47, 15.83s/it] 53%|█████▎    | 270/507 [1:12:20<1:02:29, 15.82s/it]                                                     {'loss': 0.0146, 'learning_rate': 9.456407833558018e-05, 'epoch': 1.6}
 53%|█████▎    | 270/507 [1:12:20<1:02:29, 15.82s/it] 53%|█████▎    | 271/507 [1:12:35<1:02:07, 15.80s/it]                                                     {'loss': 0.024, 'learning_rate': 9.3925304426181e-05, 'epoch': 1.6}
 53%|█████▎    | 271/507 [1:12:35<1:02:07, 15.80s/it] 54%|█████▎    | 272/507 [1:12:51<1:01:48, 15.78s/it]                                                     {'loss': 0.0156, 'learning_rate': 9.328677920762697e-05, 'epoch': 1.61}
 54%|█████▎    | 272/507 [1:12:51<1:01:48, 15.78s/it] 54%|█████▍    | 273/507 [1:13:07<1:01:31, 15.78s/it]                                                     {'loss': 0.0111, 'learning_rate': 9.264852882038453e-05, 'epoch': 1.61}
 54%|█████▍    | 273/507 [1:13:07<1:01:31, 15.78s/it] 54%|█████▍    | 274/507 [1:13:23<1:01:10, 15.75s/it]                                                     {'loss': 0.0115, 'learning_rate': 9.201057939366896e-05, 'epoch': 1.62}
 54%|█████▍    | 274/507 [1:13:23<1:01:10, 15.75s/it] 54%|█████▍    | 275/507 [1:13:38<1:00:48, 15.73s/it]                                                     {'loss': 0.027, 'learning_rate': 9.13729570443745e-05, 'epoch': 1.62}
 54%|█████▍    | 275/507 [1:13:38<1:00:48, 15.73s/it] 54%|█████▍    | 276/507 [1:13:54<1:00:30, 15.72s/it]                                                     {'loss': 0.0178, 'learning_rate': 9.073568787600539e-05, 'epoch': 1.63}
 54%|█████▍    | 276/507 [1:13:54<1:00:30, 15.72s/it] 55%|█████▍    | 277/507 [1:14:09<1:00:08, 15.69s/it]                                                     {'loss': 0.0186, 'learning_rate': 9.009879797760678e-05, 'epoch': 1.64}
 55%|█████▍    | 277/507 [1:14:09<1:00:08, 15.69s/it] 55%|█████▍    | 278/507 [1:14:25<59:53, 15.69s/it]                                                     {'loss': 0.0289, 'learning_rate': 8.946231342269718e-05, 'epoch': 1.64}
 55%|█████▍    | 278/507 [1:14:25<59:53, 15.69s/it] 55%|█████▌    | 279/507 [1:14:41<59:39, 15.70s/it]                                                   {'loss': 0.0551, 'learning_rate': 8.882626026820078e-05, 'epoch': 1.65}
 55%|█████▌    | 279/507 [1:14:41<59:39, 15.70s/it] 55%|█████▌    | 280/507 [1:14:57<59:24, 15.70s/it]                                                   {'loss': 0.0222, 'learning_rate': 8.819066455338066e-05, 'epoch': 1.65}
 55%|█████▌    | 280/507 [1:14:57<59:24, 15.70s/it] 55%|█████▌    | 281/507 [1:15:12<59:08, 15.70s/it]                                                   {'loss': 0.0214, 'learning_rate': 8.755555229877294e-05, 'epoch': 1.66}
 55%|█████▌    | 281/507 [1:15:12<59:08, 15.70s/it] 56%|█████▌    | 282/507 [1:15:28<58:50, 15.69s/it]                                                   {'loss': 0.0321, 'learning_rate': 8.692094950512145e-05, 'epoch': 1.67}
 56%|█████▌    | 282/507 [1:15:28<58:50, 15.69s/it] 56%|█████▌    | 283/507 [1:15:44<58:29, 15.67s/it]                                                   {'loss': 0.0233, 'learning_rate': 8.62868821523133e-05, 'epoch': 1.67}
 56%|█████▌    | 283/507 [1:15:44<58:29, 15.67s/it] 56%|█████▌    | 284/507 [1:15:59<58:13, 15.66s/it]                                                   {'loss': 0.0134, 'learning_rate': 8.565337619831516e-05, 'epoch': 1.68}
 56%|█████▌    | 284/507 [1:15:59<58:13, 15.66s/it] 56%|█████▌    | 285/507 [1:16:15<57:54, 15.65s/it]                                                   {'loss': 0.0075, 'learning_rate': 8.502045757811085e-05, 'epoch': 1.68}
 56%|█████▌    | 285/507 [1:16:15<57:54, 15.65s/it] 56%|█████▋    | 286/507 [1:16:31<57:40, 15.66s/it]                                                   {'loss': 0.0171, 'learning_rate': 8.438815220263941e-05, 'epoch': 1.69}
 56%|█████▋    | 286/507 [1:16:31<57:40, 15.66s/it] 57%|█████▋    | 287/507 [1:16:46<57:26, 15.67s/it]                                                   {'loss': 0.0439, 'learning_rate': 8.37564859577343e-05, 'epoch': 1.7}
 57%|█████▋    | 287/507 [1:16:46<57:26, 15.67s/it] 57%|█████▋    | 288/507 [1:17:02<57:14, 15.68s/it]                                                   {'loss': 0.0372, 'learning_rate': 8.312548470306378e-05, 'epoch': 1.7}
 57%|█████▋    | 288/507 [1:17:02<57:14, 15.68s/it] 57%|█████▋    | 289/507 [1:17:18<57:00, 15.69s/it]                                                   {'loss': 0.0377, 'learning_rate': 8.249517427107225e-05, 'epoch': 1.71}
 57%|█████▋    | 289/507 [1:17:18<57:00, 15.69s/it] 57%|█████▋    | 290/507 [1:17:33<56:46, 15.70s/it]                                                   {'loss': 0.0174, 'learning_rate': 8.186558046592247e-05, 'epoch': 1.71}
 57%|█████▋    | 290/507 [1:17:33<56:46, 15.70s/it] 57%|█████▋    | 291/507 [1:17:49<56:31, 15.70s/it]                                                   {'loss': 0.022, 'learning_rate': 8.123672906243955e-05, 'epoch': 1.72}
 57%|█████▋    | 291/507 [1:17:49<56:31, 15.70s/it] 58%|█████▊    | 292/507 [1:18:05<56:21, 15.73s/it]                                                   {'loss': 0.0259, 'learning_rate': 8.060864580505543e-05, 'epoch': 1.73}
 58%|█████▊    | 292/507 [1:18:05<56:21, 15.73s/it] 58%|█████▊    | 293/507 [1:18:21<56:07, 15.74s/it]                                                   {'loss': 0.0282, 'learning_rate': 7.998135640675513e-05, 'epoch': 1.73}
 58%|█████▊    | 293/507 [1:18:21<56:07, 15.74s/it] 58%|█████▊    | 294/507 [1:18:36<55:55, 15.75s/it]                                                   {'loss': 0.0112, 'learning_rate': 7.935488654802394e-05, 'epoch': 1.74}
 58%|█████▊    | 294/507 [1:18:36<55:55, 15.75s/it] 58%|█████▊    | 295/507 [1:18:52<55:44, 15.77s/it]                                                   {'loss': 0.0283, 'learning_rate': 7.872926187579626e-05, 'epoch': 1.74}
 58%|█████▊    | 295/507 [1:18:52<55:44, 15.77s/it] 58%|█████▊    | 296/507 [1:19:09<56:59, 16.20s/it]                                                   {'loss': 0.0055, 'learning_rate': 7.810450800240549e-05, 'epoch': 1.75}
 58%|█████▊    | 296/507 [1:19:09<56:59, 16.20s/it] 59%|█████▊    | 297/507 [1:19:25<56:10, 16.05s/it]                                                   {'loss': 0.0259, 'learning_rate': 7.748065050453557e-05, 'epoch': 1.75}
 59%|█████▊    | 297/507 [1:19:25<56:10, 16.05s/it] 59%|█████▉    | 298/507 [1:19:41<55:40, 15.98s/it]                                                   {'loss': 0.0154, 'learning_rate': 7.685771492217386e-05, 'epoch': 1.76}
 59%|█████▉    | 298/507 [1:19:41<55:40, 15.98s/it] 59%|█████▉    | 299/507 [1:19:57<55:17, 15.95s/it]                                                   {'loss': 0.0202, 'learning_rate': 7.623572675756569e-05, 'epoch': 1.77}
 59%|█████▉    | 299/507 [1:19:57<55:17, 15.95s/it] 59%|█████▉    | 300/507 [1:20:13<54:50, 15.89s/it]                                                   {'loss': 0.0351, 'learning_rate': 7.561471147417016e-05, 'epoch': 1.77}
 59%|█████▉    | 300/507 [1:20:13<54:50, 15.89s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 59%|█████▉    | 301/507 [1:20:36<1:02:17, 18.14s/it]                                                     {'loss': 0.0261, 'learning_rate': 7.499469449561769e-05, 'epoch': 1.78}
 59%|█████▉    | 301/507 [1:20:36<1:02:17, 18.14s/it] 60%|█████▉    | 302/507 [1:20:52<59:25, 17.39s/it]                                                     {'loss': 0.0335, 'learning_rate': 7.437570120466942e-05, 'epoch': 1.78}
 60%|█████▉    | 302/507 [1:20:52<59:25, 17.39s/it] 60%|█████▉    | 303/507 [1:21:07<57:29, 16.91s/it]                                                   {'loss': 0.0297, 'learning_rate': 7.375775694217787e-05, 'epoch': 1.79}
 60%|█████▉    | 303/507 [1:21:07<57:29, 16.91s/it] 60%|█████▉    | 304/507 [1:21:23<56:05, 16.58s/it]                                                   {'loss': 0.0256, 'learning_rate': 7.314088700604958e-05, 'epoch': 1.8}
 60%|█████▉    | 304/507 [1:21:23<56:05, 16.58s/it] 60%|██████    | 305/507 [1:21:39<54:59, 16.34s/it]                                                   {'loss': 0.0129, 'learning_rate': 7.252511665020939e-05, 'epoch': 1.8}
 60%|██████    | 305/507 [1:21:39<54:59, 16.34s/it] 60%|██████    | 306/507 [1:21:55<54:08, 16.16s/it]                                                   {'loss': 0.0131, 'learning_rate': 7.191047108356672e-05, 'epoch': 1.81}
 60%|██████    | 306/507 [1:21:55<54:08, 16.16s/it] 61%|██████    | 307/507 [1:22:10<53:23, 16.02s/it]                                                   {'loss': 0.0293, 'learning_rate': 7.129697546898344e-05, 'epoch': 1.81}
 61%|██████    | 307/507 [1:22:10<53:23, 16.02s/it] 61%|██████    | 308/507 [1:22:26<52:46, 15.91s/it]                                                   {'loss': 0.0109, 'learning_rate': 7.068465492224361e-05, 'epoch': 1.82}
 61%|██████    | 308/507 [1:22:26<52:46, 15.91s/it] 61%|██████    | 309/507 [1:22:42<52:20, 15.86s/it]                                                   {'loss': 0.0332, 'learning_rate': 7.007353451102556e-05, 'epoch': 1.83}
 61%|██████    | 309/507 [1:22:42<52:20, 15.86s/it] 61%|██████    | 310/507 [1:22:58<51:57, 15.83s/it]                                                   {'loss': 0.0218, 'learning_rate': 6.946363925387546e-05, 'epoch': 1.83}
 61%|██████    | 310/507 [1:22:58<51:57, 15.83s/it] 61%|██████▏   | 311/507 [1:23:13<51:37, 15.80s/it]                                                   {'loss': 0.0088, 'learning_rate': 6.885499411918304e-05, 'epoch': 1.84}
 61%|██████▏   | 311/507 [1:23:13<51:37, 15.80s/it] 62%|██████▏   | 312/507 [1:23:29<51:15, 15.77s/it]                                                   {'loss': 0.0166, 'learning_rate': 6.824762402415957e-05, 'epoch': 1.84}
 62%|██████▏   | 312/507 [1:23:29<51:15, 15.77s/it] 62%|██████▏   | 313/507 [1:23:45<50:57, 15.76s/it]                                                   {'loss': 0.0091, 'learning_rate': 6.764155383381771e-05, 'epoch': 1.85}
 62%|██████▏   | 313/507 [1:23:45<50:57, 15.76s/it] 62%|██████▏   | 314/507 [1:24:00<50:35, 15.73s/it]                                                   {'loss': 0.0439, 'learning_rate': 6.703680835995359e-05, 'epoch': 1.86}
 62%|██████▏   | 314/507 [1:24:00<50:35, 15.73s/it] 62%|██████▏   | 315/507 [1:24:16<50:18, 15.72s/it]                                                   {'loss': 0.0117, 'learning_rate': 6.643341236013086e-05, 'epoch': 1.86}
 62%|██████▏   | 315/507 [1:24:16<50:18, 15.72s/it] 62%|██████▏   | 316/507 [1:24:32<50:11, 15.77s/it]                                                   {'loss': 0.0341, 'learning_rate': 6.583139053666745e-05, 'epoch': 1.87}
 62%|██████▏   | 316/507 [1:24:32<50:11, 15.77s/it] 63%|██████▎   | 317/507 [1:24:48<49:49, 15.74s/it]                                                   {'loss': 0.0109, 'learning_rate': 6.523076753562411e-05, 'epoch': 1.87}
 63%|██████▎   | 317/507 [1:24:48<49:49, 15.74s/it] 63%|██████▎   | 318/507 [1:25:03<49:31, 15.72s/it]                                                   {'loss': 0.0165, 'learning_rate': 6.463156794579544e-05, 'epoch': 1.88}
 63%|██████▎   | 318/507 [1:25:03<49:31, 15.72s/it] 63%|██████▎   | 319/507 [1:25:19<49:13, 15.71s/it]                                                   {'loss': 0.0221, 'learning_rate': 6.403381629770325e-05, 'epoch': 1.88}
 63%|██████▎   | 319/507 [1:25:19<49:13, 15.71s/it] 63%|██████▎   | 320/507 [1:25:35<48:55, 15.70s/it]                                                   {'loss': 0.0405, 'learning_rate': 6.343753706259239e-05, 'epoch': 1.89}
 63%|██████▎   | 320/507 [1:25:35<48:55, 15.70s/it] 63%|██████▎   | 321/507 [1:25:51<48:45, 15.73s/it]                                                   {'loss': 0.0511, 'learning_rate': 6.284275465142874e-05, 'epoch': 1.9}
 63%|██████▎   | 321/507 [1:25:51<48:45, 15.73s/it] 64%|██████▎   | 322/507 [1:26:06<48:20, 15.68s/it]                                                   {'loss': 0.024, 'learning_rate': 6.224949341390016e-05, 'epoch': 1.9}
 64%|██████▎   | 322/507 [1:26:06<48:20, 15.68s/it] 64%|██████▎   | 323/507 [1:26:22<48:05, 15.68s/it]                                                   {'loss': 0.0373, 'learning_rate': 6.165777763741931e-05, 'epoch': 1.91}
 64%|██████▎   | 323/507 [1:26:22<48:05, 15.68s/it] 64%|██████▍   | 324/507 [1:26:37<47:46, 15.67s/it]                                                   {'loss': 0.0302, 'learning_rate': 6.106763154612962e-05, 'epoch': 1.91}
 64%|██████▍   | 324/507 [1:26:37<47:46, 15.67s/it] 64%|██████▍   | 325/507 [1:26:53<47:31, 15.67s/it]                                                   {'loss': 0.0233, 'learning_rate': 6.047907929991333e-05, 'epoch': 1.92}
 64%|██████▍   | 325/507 [1:26:53<47:31, 15.67s/it] 64%|██████▍   | 326/507 [1:27:10<48:03, 15.93s/it]                                                   {'loss': 0.0271, 'learning_rate': 5.989214499340267e-05, 'epoch': 1.93}
 64%|██████▍   | 326/507 [1:27:10<48:03, 15.93s/it] 64%|██████▍   | 327/507 [1:27:25<47:32, 15.84s/it]                                                   {'loss': 0.0206, 'learning_rate': 5.9306852654993294e-05, 'epoch': 1.93}
 64%|██████▍   | 327/507 [1:27:25<47:32, 15.84s/it] 65%|██████▍   | 328/507 [1:27:41<47:07, 15.80s/it]                                                   {'loss': 0.0019, 'learning_rate': 5.872322624586061e-05, 'epoch': 1.94}
 65%|██████▍   | 328/507 [1:27:41<47:07, 15.80s/it] 65%|██████▍   | 329/507 [1:27:57<46:44, 15.76s/it]                                                   {'loss': 0.0259, 'learning_rate': 5.814128965897887e-05, 'epoch': 1.94}
 65%|██████▍   | 329/507 [1:27:57<46:44, 15.76s/it] 65%|██████▌   | 330/507 [1:28:12<46:34, 15.79s/it]                                                   {'loss': 0.0405, 'learning_rate': 5.756106671814301e-05, 'epoch': 1.95}
 65%|██████▌   | 330/507 [1:28:12<46:34, 15.79s/it] 65%|██████▌   | 331/507 [1:28:28<46:15, 15.77s/it]                                                   {'loss': 0.0379, 'learning_rate': 5.6982581176993335e-05, 'epoch': 1.96}
 65%|██████▌   | 331/507 [1:28:28<46:15, 15.77s/it] 65%|██████▌   | 332/507 [1:28:44<45:59, 15.77s/it]                                                   {'loss': 0.0239, 'learning_rate': 5.640585671804296e-05, 'epoch': 1.96}
 65%|██████▌   | 332/507 [1:28:44<45:59, 15.77s/it] 66%|██████▌   | 333/507 [1:29:00<45:42, 15.76s/it]                                                   {'loss': 0.0163, 'learning_rate': 5.5830916951708565e-05, 'epoch': 1.97}
 66%|██████▌   | 333/507 [1:29:00<45:42, 15.76s/it] 66%|██████▌   | 334/507 [1:29:15<45:27, 15.77s/it]                                                   {'loss': 0.0065, 'learning_rate': 5.52577854153435e-05, 'epoch': 1.97}
 66%|██████▌   | 334/507 [1:29:15<45:27, 15.77s/it] 66%|██████▌   | 335/507 [1:29:31<45:11, 15.77s/it]                                                   {'loss': 0.0316, 'learning_rate': 5.4686485572274336e-05, 'epoch': 1.98}
 66%|██████▌   | 335/507 [1:29:31<45:11, 15.77s/it] 66%|██████▋   | 336/507 [1:29:47<44:57, 15.77s/it]                                                   {'loss': 0.0219, 'learning_rate': 5.4117040810840246e-05, 'epoch': 1.99}
 66%|██████▋   | 336/507 [1:29:47<44:57, 15.77s/it] 66%|██████▋   | 337/507 [1:30:03<44:42, 15.78s/it]                                                   {'loss': 0.0188, 'learning_rate': 5.354947444343572e-05, 'epoch': 1.99}
 66%|██████▋   | 337/507 [1:30:03<44:42, 15.78s/it] 67%|██████▋   | 338/507 [1:30:19<44:27, 15.78s/it]                                                   {'loss': 0.008, 'learning_rate': 5.298380970555584e-05, 'epoch': 2.0}
 67%|██████▋   | 338/507 [1:30:19<44:27, 15.78s/it] 67%|██████▋   | 339/507 [1:30:35<44:53, 16.03s/it]                                                   {'loss': 0.0333, 'learning_rate': 5.242006975484528e-05, 'epoch': 2.0}
 67%|██████▋   | 339/507 [1:30:35<44:53, 16.03s/it] 67%|██████▋   | 340/507 [1:30:51<44:18, 15.92s/it]                                                   {'loss': 0.0056, 'learning_rate': 5.1858277670150304e-05, 'epoch': 2.01}
 67%|██████▋   | 340/507 [1:30:51<44:18, 15.92s/it] 67%|██████▋   | 341/507 [1:31:07<43:48, 15.84s/it]                                                   {'loss': 0.0292, 'learning_rate': 5.129845645057372e-05, 'epoch': 2.01}
 67%|██████▋   | 341/507 [1:31:07<43:48, 15.84s/it] 67%|██████▋   | 342/507 [1:31:22<43:29, 15.82s/it]                                                   {'loss': 0.0331, 'learning_rate': 5.074062901453351e-05, 'epoch': 2.02}
 67%|██████▋   | 342/507 [1:31:22<43:29, 15.82s/it] 68%|██████▊   | 343/507 [1:31:38<43:08, 15.78s/it]                                                   {'loss': 0.0079, 'learning_rate': 5.0184818198824454e-05, 'epoch': 2.03}
 68%|██████▊   | 343/507 [1:31:38<43:08, 15.78s/it] 68%|██████▊   | 344/507 [1:31:54<42:49, 15.76s/it]                                                   {'loss': 0.0178, 'learning_rate': 4.963104675768345e-05, 'epoch': 2.03}
 68%|██████▊   | 344/507 [1:31:54<42:49, 15.76s/it] 68%|██████▊   | 345/507 [1:32:09<42:31, 15.75s/it]                                                   {'loss': 0.0072, 'learning_rate': 4.907933736185757e-05, 'epoch': 2.04}
 68%|██████▊   | 345/507 [1:32:09<42:31, 15.75s/it] 68%|██████▊   | 346/507 [1:32:25<42:14, 15.74s/it]                                                   {'loss': 0.0077, 'learning_rate': 4.8529712597676426e-05, 'epoch': 2.04}
 68%|██████▊   | 346/507 [1:32:25<42:14, 15.74s/it] 68%|██████▊   | 347/507 [1:32:41<41:58, 15.74s/it]                                                   {'loss': 0.0287, 'learning_rate': 4.79821949661271e-05, 'epoch': 2.05}
 68%|██████▊   | 347/507 [1:32:41<41:58, 15.74s/it] 69%|██████▊   | 348/507 [1:32:57<41:41, 15.73s/it]                                                   {'loss': 0.0097, 'learning_rate': 4.74368068819333e-05, 'epoch': 2.06}
 69%|██████▊   | 348/507 [1:32:57<41:41, 15.73s/it] 69%|██████▉   | 349/507 [1:33:12<41:24, 15.72s/it]                                                   {'loss': 0.0076, 'learning_rate': 4.689357067263751e-05, 'epoch': 2.06}
 69%|██████▉   | 349/507 [1:33:12<41:24, 15.72s/it] 69%|██████▉   | 350/507 [1:33:28<41:08, 15.72s/it]                                                   {'loss': 0.0072, 'learning_rate': 4.635250857768696e-05, 'epoch': 2.07}
 69%|██████▉   | 350/507 [1:33:28<41:08, 15.72s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 69%|██████▉   | 351/507 [1:33:51<46:52, 18.03s/it]                                                   {'loss': 0.0283, 'learning_rate': 4.581364274752338e-05, 'epoch': 2.07}
 69%|██████▉   | 351/507 [1:33:51<46:52, 18.03s/it] 69%|██████▉   | 352/507 [1:34:07<44:37, 17.28s/it]                                                   {'loss': 0.0202, 'learning_rate': 4.527699524267576e-05, 'epoch': 2.08}
 69%|██████▉   | 352/507 [1:34:07<44:37, 17.28s/it] 70%|██████▉   | 353/507 [1:34:23<43:03, 16.77s/it]                                                   {'loss': 0.0296, 'learning_rate': 4.474258803285774e-05, 'epoch': 2.09}
 70%|██████▉   | 353/507 [1:34:23<43:03, 16.77s/it] 70%|██████▉   | 354/507 [1:34:38<41:55, 16.44s/it]                                                   {'loss': 0.0221, 'learning_rate': 4.4210442996067724e-05, 'epoch': 2.09}
 70%|██████▉   | 354/507 [1:34:38<41:55, 16.44s/it] 70%|███████   | 355/507 [1:34:54<41:03, 16.21s/it]                                                   {'loss': 0.0137, 'learning_rate': 4.368058191769363e-05, 'epoch': 2.1}
 70%|███████   | 355/507 [1:34:54<41:03, 16.21s/it] 70%|███████   | 356/507 [1:35:10<40:25, 16.06s/it]                                                   {'loss': 0.0095, 'learning_rate': 4.315302648962066e-05, 'epoch': 2.1}
 70%|███████   | 356/507 [1:35:10<40:25, 16.06s/it] 70%|███████   | 357/507 [1:35:25<39:54, 15.96s/it]                                                   {'loss': 0.0137, 'learning_rate': 4.262779830934346e-05, 'epoch': 2.11}
 70%|███████   | 357/507 [1:35:25<39:54, 15.96s/it] 71%|███████   | 358/507 [1:35:41<39:27, 15.89s/it]                                                   {'loss': 0.0166, 'learning_rate': 4.210491887908201e-05, 'epoch': 2.12}
 71%|███████   | 358/507 [1:35:41<39:27, 15.89s/it] 71%|███████   | 359/507 [1:35:57<39:03, 15.83s/it]                                                   {'loss': 0.0248, 'learning_rate': 4.158440960490103e-05, 'epoch': 2.12}
 71%|███████   | 359/507 [1:35:57<39:03, 15.83s/it] 71%|███████   | 360/507 [1:36:13<38:48, 15.84s/it]                                                   {'loss': 0.023, 'learning_rate': 4.1066291795834114e-05, 'epoch': 2.13}
 71%|███████   | 360/507 [1:36:13<38:48, 15.84s/it] 71%|███████   | 361/507 [1:36:28<38:30, 15.82s/it]                                                   {'loss': 0.0059, 'learning_rate': 4.055058666301087e-05, 'epoch': 2.13}
 71%|███████   | 361/507 [1:36:28<38:30, 15.82s/it] 71%|███████▏  | 362/507 [1:36:44<38:12, 15.81s/it]                                                   {'loss': 0.0019, 'learning_rate': 4.003731531878899e-05, 'epoch': 2.14}
 71%|███████▏  | 362/507 [1:36:44<38:12, 15.81s/it] 72%|███████▏  | 363/507 [1:37:00<37:53, 15.79s/it]                                                   {'loss': 0.0251, 'learning_rate': 3.952649877588964e-05, 'epoch': 2.14}
 72%|███████▏  | 363/507 [1:37:00<37:53, 15.79s/it] 72%|███████▏  | 364/507 [1:37:16<37:38, 15.80s/it]                                                   {'loss': 0.0204, 'learning_rate': 3.901815794653729e-05, 'epoch': 2.15}
 72%|███████▏  | 364/507 [1:37:16<37:38, 15.80s/it] 72%|███████▏  | 365/507 [1:37:32<37:22, 15.79s/it]                                                   {'loss': 0.0223, 'learning_rate': 3.851231364160379e-05, 'epoch': 2.16}
 72%|███████▏  | 365/507 [1:37:32<37:22, 15.79s/it] 72%|███████▏  | 366/507 [1:37:47<37:07, 15.80s/it]                                                   {'loss': 0.0147, 'learning_rate': 3.800898656975599e-05, 'epoch': 2.16}
 72%|███████▏  | 366/507 [1:37:47<37:07, 15.80s/it] 72%|███████▏  | 367/507 [1:38:03<36:50, 15.79s/it]                                                   {'loss': 0.0115, 'learning_rate': 3.750819733660844e-05, 'epoch': 2.17}
 72%|███████▏  | 367/507 [1:38:03<36:50, 15.79s/it] 73%|███████▎  | 368/507 [1:38:19<36:35, 15.79s/it]                                                   {'loss': 0.0148, 'learning_rate': 3.700996644387944e-05, 'epoch': 2.17}
 73%|███████▎  | 368/507 [1:38:19<36:35, 15.79s/it] 73%|███████▎  | 369/507 [1:38:35<36:20, 15.80s/it]                                                   {'loss': 0.0105, 'learning_rate': 3.651431428855188e-05, 'epoch': 2.18}
 73%|███████▎  | 369/507 [1:38:35<36:20, 15.80s/it] 73%|███████▎  | 370/507 [1:38:50<36:03, 15.80s/it]                                                   {'loss': 0.0033, 'learning_rate': 3.602126116203819e-05, 'epoch': 2.19}
 73%|███████▎  | 370/507 [1:38:50<36:03, 15.80s/it] 73%|███████▎  | 371/507 [1:39:06<35:48, 15.80s/it]                                                   {'loss': 0.0029, 'learning_rate': 3.553082724934973e-05, 'epoch': 2.19}
 73%|███████▎  | 371/507 [1:39:06<35:48, 15.80s/it] 73%|███████▎  | 372/507 [1:39:22<35:29, 15.78s/it]                                                   {'loss': 0.0103, 'learning_rate': 3.504303262827022e-05, 'epoch': 2.2}
 73%|███████▎  | 372/507 [1:39:22<35:29, 15.78s/it] 74%|███████▎  | 373/507 [1:39:38<35:10, 15.75s/it]                                                   {'loss': 0.0266, 'learning_rate': 3.4557897268533935e-05, 'epoch': 2.2}
 74%|███████▎  | 373/507 [1:39:38<35:10, 15.75s/it] 74%|███████▍  | 374/507 [1:39:53<34:55, 15.76s/it]                                                   {'loss': 0.0187, 'learning_rate': 3.407544103100824e-05, 'epoch': 2.21}
 74%|███████▍  | 374/507 [1:39:53<34:55, 15.76s/it] 74%|███████▍  | 375/507 [1:40:09<34:39, 15.75s/it]                                                   {'loss': 0.0215, 'learning_rate': 3.359568366688028e-05, 'epoch': 2.22}
 74%|███████▍  | 375/507 [1:40:09<34:39, 15.75s/it] 74%|███████▍  | 376/507 [1:40:25<34:21, 15.74s/it]                                                   {'loss': 0.0154, 'learning_rate': 3.3118644816848574e-05, 'epoch': 2.22}
 74%|███████▍  | 376/507 [1:40:25<34:21, 15.74s/it] 74%|███████▍  | 377/507 [1:40:41<34:04, 15.73s/it]                                                   {'loss': 0.003, 'learning_rate': 3.264434401031887e-05, 'epoch': 2.23}
 74%|███████▍  | 377/507 [1:40:41<34:04, 15.73s/it] 75%|███████▍  | 378/507 [1:40:56<33:48, 15.73s/it]                                                   {'loss': 0.0255, 'learning_rate': 3.217280066460472e-05, 'epoch': 2.23}
 75%|███████▍  | 378/507 [1:40:56<33:48, 15.73s/it] 75%|███████▍  | 379/507 [1:41:12<33:32, 15.72s/it]                                                   {'loss': 0.0189, 'learning_rate': 3.170403408413243e-05, 'epoch': 2.24}
 75%|███████▍  | 379/507 [1:41:12<33:32, 15.72s/it] 75%|███████▍  | 380/507 [1:41:28<33:15, 15.71s/it]                                                   {'loss': 0.0074, 'learning_rate': 3.1238063459650805e-05, 'epoch': 2.25}
 75%|███████▍  | 380/507 [1:41:28<33:15, 15.71s/it] 75%|███████▌  | 381/507 [1:41:43<32:58, 15.70s/it]                                                   {'loss': 0.0081, 'learning_rate': 3.077490786744562e-05, 'epoch': 2.25}
 75%|███████▌  | 381/507 [1:41:43<32:58, 15.70s/it] 75%|███████▌  | 382/507 [1:41:59<32:41, 15.69s/it]                                                   {'loss': 0.0285, 'learning_rate': 3.031458626855849e-05, 'epoch': 2.26}
 75%|███████▌  | 382/507 [1:41:59<32:41, 15.69s/it] 76%|███████▌  | 383/507 [1:42:15<32:24, 15.68s/it]                                                   {'loss': 0.0111, 'learning_rate': 2.985711750801068e-05, 'epoch': 2.26}
 76%|███████▌  | 383/507 [1:42:15<32:24, 15.68s/it] 76%|███████▌  | 384/507 [1:42:30<32:08, 15.68s/it]                                                   {'loss': 0.0202, 'learning_rate': 2.9402520314031644e-05, 'epoch': 2.27}
 76%|███████▌  | 384/507 [1:42:30<32:08, 15.68s/it] 76%|███████▌  | 385/507 [1:42:46<31:52, 15.67s/it]                                                   {'loss': 0.0244, 'learning_rate': 2.895081329729239e-05, 'epoch': 2.27}
 76%|███████▌  | 385/507 [1:42:46<31:52, 15.67s/it] 76%|███████▌  | 386/507 [1:43:02<31:34, 15.66s/it]                                                   {'loss': 0.0195, 'learning_rate': 2.8502014950143373e-05, 'epoch': 2.28}
 76%|███████▌  | 386/507 [1:43:02<31:34, 15.66s/it] 76%|███████▋  | 387/507 [1:43:17<31:18, 15.65s/it]                                                   {'loss': 0.0093, 'learning_rate': 2.805614364585758e-05, 'epoch': 2.29}
 76%|███████▋  | 387/507 [1:43:17<31:18, 15.65s/it] 77%|███████▋  | 388/507 [1:43:33<31:03, 15.66s/it]                                                   {'loss': 0.008, 'learning_rate': 2.7613217637878407e-05, 'epoch': 2.29}
 77%|███████▋  | 388/507 [1:43:33<31:03, 15.66s/it] 77%|███████▋  | 389/507 [1:43:49<30:47, 15.66s/it]                                                   {'loss': 0.0136, 'learning_rate': 2.7173255059072233e-05, 'epoch': 2.3}
 77%|███████▋  | 389/507 [1:43:49<30:47, 15.66s/it] 77%|███████▋  | 390/507 [1:44:04<30:32, 15.67s/it]                                                   {'loss': 0.0153, 'learning_rate': 2.6736273920986167e-05, 'epoch': 2.3}
 77%|███████▋  | 390/507 [1:44:04<30:32, 15.67s/it] 77%|███████▋  | 391/507 [1:44:20<30:18, 15.68s/it]                                                   {'loss': 0.0213, 'learning_rate': 2.6302292113110637e-05, 'epoch': 2.31}
 77%|███████▋  | 391/507 [1:44:20<30:18, 15.68s/it] 77%|███████▋  | 392/507 [1:44:36<30:02, 15.67s/it]                                                   {'loss': 0.0164, 'learning_rate': 2.5871327402147172e-05, 'epoch': 2.32}
 77%|███████▋  | 392/507 [1:44:36<30:02, 15.67s/it] 78%|███████▊  | 393/507 [1:44:51<29:48, 15.69s/it]                                                   {'loss': 0.0234, 'learning_rate': 2.5443397431280702e-05, 'epoch': 2.32}
 78%|███████▊  | 393/507 [1:44:51<29:48, 15.69s/it] 78%|███████▊  | 394/507 [1:45:07<29:35, 15.72s/it]                                                   {'loss': 0.0036, 'learning_rate': 2.5018519719457723e-05, 'epoch': 2.33}
 78%|███████▊  | 394/507 [1:45:07<29:35, 15.72s/it] 78%|███████▊  | 395/507 [1:45:23<29:21, 15.73s/it]                                                   {'loss': 0.0157, 'learning_rate': 2.4596711660668692e-05, 'epoch': 2.33}
 78%|███████▊  | 395/507 [1:45:23<29:21, 15.73s/it] 78%|███████▊  | 396/507 [1:45:39<29:08, 15.75s/it]                                                   {'loss': 0.0019, 'learning_rate': 2.4177990523236216e-05, 'epoch': 2.34}
 78%|███████▊  | 396/507 [1:45:39<29:08, 15.75s/it] 78%|███████▊  | 397/507 [1:45:55<28:54, 15.77s/it]                                                   {'loss': 0.0027, 'learning_rate': 2.3762373449107932e-05, 'epoch': 2.35}
 78%|███████▊  | 397/507 [1:45:55<28:54, 15.77s/it] 79%|███████▊  | 398/507 [1:46:10<28:36, 15.74s/it]                                                   {'loss': 0.0031, 'learning_rate': 2.334987745315478e-05, 'epoch': 2.35}
 79%|███████▊  | 398/507 [1:46:10<28:36, 15.74s/it] 79%|███████▊  | 399/507 [1:46:26<28:22, 15.77s/it]                                                   {'loss': 0.0148, 'learning_rate': 2.2940519422474573e-05, 'epoch': 2.36}
 79%|███████▊  | 399/507 [1:46:26<28:22, 15.77s/it] 79%|███████▉  | 400/507 [1:46:42<28:08, 15.78s/it]                                                   {'loss': 0.0117, 'learning_rate': 2.253431611570035e-05, 'epoch': 2.36}
 79%|███████▉  | 400/507 [1:46:42<28:08, 15.78s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 79%|███████▉  | 401/507 [1:47:05<31:41, 17.94s/it]                                                   {'loss': 0.0021, 'learning_rate': 2.213128416231468e-05, 'epoch': 2.37}
 79%|███████▉  | 401/507 [1:47:05<31:41, 17.94s/it] 79%|███████▉  | 402/507 [1:47:21<30:10, 17.25s/it]                                                   {'loss': 0.0092, 'learning_rate': 2.1731440061968533e-05, 'epoch': 2.38}
 79%|███████▉  | 402/507 [1:47:21<30:10, 17.25s/it] 79%|███████▉  | 403/507 [1:47:36<29:08, 16.82s/it]                                                   {'loss': 0.0071, 'learning_rate': 2.133480018380608e-05, 'epoch': 2.38}
 79%|███████▉  | 403/507 [1:47:36<29:08, 16.82s/it] 80%|███████▉  | 404/507 [1:47:52<28:19, 16.50s/it]                                                   {'loss': 0.0074, 'learning_rate': 2.0941380765794327e-05, 'epoch': 2.39}
 80%|███████▉  | 404/507 [1:47:52<28:19, 16.50s/it] 80%|███████▉  | 405/507 [1:48:09<28:20, 16.67s/it]                                                   {'loss': 0.0088, 'learning_rate': 2.0551197914058464e-05, 'epoch': 2.39}
 80%|███████▉  | 405/507 [1:48:09<28:20, 16.67s/it] 80%|████████  | 406/507 [1:48:25<27:37, 16.41s/it]                                                   {'loss': 0.007, 'learning_rate': 2.0164267602222586e-05, 'epoch': 2.4}
 80%|████████  | 406/507 [1:48:25<27:37, 16.41s/it] 80%|████████  | 407/507 [1:48:41<27:01, 16.21s/it]                                                   {'loss': 0.0056, 'learning_rate': 1.978060567075547e-05, 'epoch': 2.4}
 80%|████████  | 407/507 [1:48:41<27:01, 16.21s/it] 80%|████████  | 408/507 [1:48:56<26:29, 16.06s/it]                                                   {'loss': 0.0173, 'learning_rate': 1.940022782632248e-05, 'epoch': 2.41}
 80%|████████  | 408/507 [1:48:56<26:29, 16.06s/it] 81%|████████  | 409/507 [1:49:12<26:02, 15.94s/it]                                                   {'loss': 0.0166, 'learning_rate': 1.902314964114219e-05, 'epoch': 2.42}
 81%|████████  | 409/507 [1:49:12<26:02, 15.94s/it] 81%|████████  | 410/507 [1:49:28<25:41, 15.89s/it]                                                   {'loss': 0.0201, 'learning_rate': 1.8649386552349134e-05, 'epoch': 2.42}
 81%|████████  | 410/507 [1:49:28<25:41, 15.89s/it] 81%|████████  | 411/507 [1:49:44<25:24, 15.88s/it]                                                   {'loss': 0.025, 'learning_rate': 1.827895386136166e-05, 'epoch': 2.43}
 81%|████████  | 411/507 [1:49:44<25:24, 15.88s/it] 81%|████████▏ | 412/507 [1:49:59<25:03, 15.83s/it]                                                   {'loss': 0.0125, 'learning_rate': 1.7911866733255556e-05, 'epoch': 2.43}
 81%|████████▏ | 412/507 [1:49:59<25:03, 15.83s/it] 81%|████████▏ | 413/507 [1:50:15<24:43, 15.78s/it]                                                   {'loss': 0.0022, 'learning_rate': 1.7548140196143335e-05, 'epoch': 2.44}
 81%|████████▏ | 413/507 [1:50:15<24:43, 15.78s/it] 82%|████████▏ | 414/507 [1:50:31<24:26, 15.77s/it]                                                   {'loss': 0.0437, 'learning_rate': 1.718778914055873e-05, 'epoch': 2.45}
 82%|████████▏ | 414/507 [1:50:31<24:26, 15.77s/it] 82%|████████▏ | 415/507 [1:50:47<24:08, 15.74s/it]                                                   {'loss': 0.0134, 'learning_rate': 1.6830828318847414e-05, 'epoch': 2.45}
 82%|████████▏ | 415/507 [1:50:47<24:08, 15.74s/it] 82%|████████▏ | 416/507 [1:51:02<23:51, 15.73s/it]                                                   {'loss': 0.01, 'learning_rate': 1.647727234456279e-05, 'epoch': 2.46}
 82%|████████▏ | 416/507 [1:51:02<23:51, 15.73s/it] 82%|████████▏ | 417/507 [1:51:18<23:33, 15.71s/it]                                                   {'loss': 0.0077, 'learning_rate': 1.6127135691867945e-05, 'epoch': 2.46}
 82%|████████▏ | 417/507 [1:51:18<23:33, 15.71s/it] 82%|████████▏ | 418/507 [1:51:34<23:17, 15.70s/it]                                                   {'loss': 0.0224, 'learning_rate': 1.5780432694942815e-05, 'epoch': 2.47}
 82%|████████▏ | 418/507 [1:51:34<23:17, 15.70s/it] 83%|████████▎ | 419/507 [1:51:49<23:01, 15.70s/it]                                                   {'loss': 0.012, 'learning_rate': 1.543717754739774e-05, 'epoch': 2.48}
 83%|████████▎ | 419/507 [1:51:49<23:01, 15.70s/it] 83%|████████▎ | 420/507 [1:52:05<22:45, 15.70s/it]                                                   {'loss': 0.0147, 'learning_rate': 1.5097384301692041e-05, 'epoch': 2.48}
 83%|████████▎ | 420/507 [1:52:05<22:45, 15.70s/it] 83%|████████▎ | 421/507 [1:52:21<22:29, 15.69s/it]                                                   {'loss': 0.0041, 'learning_rate': 1.4761066868558914e-05, 'epoch': 2.49}
 83%|████████▎ | 421/507 [1:52:21<22:29, 15.69s/it] 83%|████████▎ | 422/507 [1:52:36<22:13, 15.68s/it]                                                   {'loss': 0.0046, 'learning_rate': 1.4428239016435951e-05, 'epoch': 2.49}
 83%|████████▎ | 422/507 [1:52:36<22:13, 15.68s/it] 83%|████████▎ | 423/507 [1:52:52<21:52, 15.62s/it]                                                   {'loss': 0.0126, 'learning_rate': 1.4098914370901384e-05, 'epoch': 2.5}
 83%|████████▎ | 423/507 [1:52:52<21:52, 15.62s/it] 84%|████████▎ | 424/507 [1:53:07<21:38, 15.64s/it]                                                   {'loss': 0.0195, 'learning_rate': 1.3773106414116299e-05, 'epoch': 2.51}
 84%|████████▎ | 424/507 [1:53:07<21:38, 15.64s/it] 84%|████████▍ | 425/507 [1:53:23<21:23, 15.65s/it]                                                   {'loss': 0.0191, 'learning_rate': 1.3450828484272726e-05, 'epoch': 2.51}
 84%|████████▍ | 425/507 [1:53:23<21:23, 15.65s/it] 84%|████████▍ | 426/507 [1:53:39<21:08, 15.66s/it]                                                   {'loss': 0.0089, 'learning_rate': 1.3132093775047615e-05, 'epoch': 2.52}
 84%|████████▍ | 426/507 [1:53:39<21:08, 15.66s/it] 84%|████████▍ | 427/507 [1:53:54<20:52, 15.66s/it]                                                   {'loss': 0.0076, 'learning_rate': 1.2816915335062595e-05, 'epoch': 2.52}
 84%|████████▍ | 427/507 [1:53:54<20:52, 15.66s/it] 84%|████████▍ | 428/507 [1:54:10<20:37, 15.67s/it]                                                   {'loss': 0.0109, 'learning_rate': 1.2505306067349853e-05, 'epoch': 2.53}
 84%|████████▍ | 428/507 [1:54:10<20:37, 15.67s/it] 85%|████████▍ | 429/507 [1:54:26<20:23, 15.69s/it]                                                   {'loss': 0.0123, 'learning_rate': 1.2197278728823947e-05, 'epoch': 2.53}
 85%|████████▍ | 429/507 [1:54:26<20:23, 15.69s/it] 85%|████████▍ | 430/507 [1:54:42<20:09, 15.70s/it]                                                   {'loss': 0.0109, 'learning_rate': 1.1892845929759412e-05, 'epoch': 2.54}
 85%|████████▍ | 430/507 [1:54:42<20:09, 15.70s/it] 85%|████████▌ | 431/507 [1:54:57<19:53, 15.71s/it]                                                   {'loss': 0.0053, 'learning_rate': 1.1592020133274639e-05, 'epoch': 2.55}
 85%|████████▌ | 431/507 [1:54:57<19:53, 15.71s/it] 85%|████████▌ | 432/507 [1:55:13<19:37, 15.71s/it]                                                   {'loss': 0.0047, 'learning_rate': 1.129481365482159e-05, 'epoch': 2.55}
 85%|████████▌ | 432/507 [1:55:13<19:37, 15.71s/it] 85%|████████▌ | 433/507 [1:55:29<19:23, 15.72s/it]                                                   {'loss': 0.0022, 'learning_rate': 1.1001238661681657e-05, 'epoch': 2.56}
 85%|████████▌ | 433/507 [1:55:29<19:23, 15.72s/it] 86%|████████▌ | 434/507 [1:55:45<19:08, 15.73s/it]                                                   {'loss': 0.0197, 'learning_rate': 1.07113071724675e-05, 'epoch': 2.56}
 86%|████████▌ | 434/507 [1:55:45<19:08, 15.73s/it] 86%|████████▌ | 435/507 [1:56:00<18:51, 15.71s/it]                                                   {'loss': 0.0176, 'learning_rate': 1.0425031056631007e-05, 'epoch': 2.57}
 86%|████████▌ | 435/507 [1:56:00<18:51, 15.71s/it] 86%|████████▌ | 436/507 [1:56:16<18:35, 15.70s/it]                                                   {'loss': 0.0263, 'learning_rate': 1.0142422033977505e-05, 'epoch': 2.58}
 86%|████████▌ | 436/507 [1:56:16<18:35, 15.70s/it] 86%|████████▌ | 437/507 [1:56:32<18:20, 15.72s/it]                                                   {'loss': 0.0025, 'learning_rate': 9.863491674185776e-06, 'epoch': 2.58}
 86%|████████▌ | 437/507 [1:56:32<18:20, 15.72s/it] 86%|████████▋ | 438/507 [1:56:47<18:05, 15.74s/it]                                                   {'loss': 0.0094, 'learning_rate': 9.588251396334524e-06, 'epoch': 2.59}
 86%|████████▋ | 438/507 [1:56:47<18:05, 15.74s/it] 87%|████████▋ | 439/507 [1:57:03<17:49, 15.73s/it]                                                   {'loss': 0.0041, 'learning_rate': 9.316712468434874e-06, 'epoch': 2.59}
 87%|████████▋ | 439/507 [1:57:03<17:49, 15.73s/it] 87%|████████▋ | 440/507 [1:57:19<17:33, 15.73s/it]                                                   {'loss': 0.0057, 'learning_rate': 9.048886006969093e-06, 'epoch': 2.6}
 87%|████████▋ | 440/507 [1:57:19<17:33, 15.73s/it] 87%|████████▋ | 441/507 [1:57:35<17:18, 15.73s/it]                                                   {'loss': 0.0097, 'learning_rate': 8.784782976435424e-06, 'epoch': 2.61}
 87%|████████▋ | 441/507 [1:57:35<17:18, 15.73s/it] 87%|████████▋ | 442/507 [1:57:50<17:02, 15.73s/it]                                                   {'loss': 0.0101, 'learning_rate': 8.524414188899266e-06, 'epoch': 2.61}
 87%|████████▋ | 442/507 [1:57:50<17:02, 15.73s/it] 87%|████████▋ | 443/507 [1:58:06<16:48, 15.76s/it]                                                   {'loss': 0.0196, 'learning_rate': 8.267790303550526e-06, 'epoch': 2.62}
 87%|████████▋ | 443/507 [1:58:06<16:48, 15.76s/it] 88%|████████▊ | 444/507 [1:58:22<16:31, 15.74s/it]                                                   {'loss': 0.0012, 'learning_rate': 8.014921826267285e-06, 'epoch': 2.62}
 88%|████████▊ | 444/507 [1:58:22<16:31, 15.74s/it] 88%|████████▊ | 445/507 [1:58:38<16:19, 15.80s/it]                                                   {'loss': 0.0114, 'learning_rate': 7.765819109185635e-06, 'epoch': 2.63}
 88%|████████▊ | 445/507 [1:58:38<16:19, 15.80s/it] 88%|████████▊ | 446/507 [1:58:54<16:02, 15.78s/it]                                                   {'loss': 0.0274, 'learning_rate': 7.520492350275876e-06, 'epoch': 2.64}
 88%|████████▊ | 446/507 [1:58:54<16:02, 15.78s/it] 88%|████████▊ | 447/507 [1:59:10<16:03, 16.06s/it]                                                   {'loss': 0.0208, 'learning_rate': 7.278951592925154e-06, 'epoch': 2.64}
 88%|████████▊ | 447/507 [1:59:10<16:03, 16.06s/it] 88%|████████▊ | 448/507 [1:59:26<15:40, 15.94s/it]                                                   {'loss': 0.0027, 'learning_rate': 7.041206725526028e-06, 'epoch': 2.65}
 88%|████████▊ | 448/507 [1:59:26<15:40, 15.94s/it] 89%|████████▊ | 449/507 [1:59:42<15:19, 15.86s/it]                                                   {'loss': 0.0133, 'learning_rate': 6.807267481071966e-06, 'epoch': 2.65}
 89%|████████▊ | 449/507 [1:59:42<15:19, 15.86s/it] 89%|████████▉ | 450/507 [1:59:57<15:00, 15.80s/it]                                                   {'loss': 0.0163, 'learning_rate': 6.577143436758659e-06, 'epoch': 2.66}
 89%|████████▉ | 450/507 [1:59:57<15:00, 15.80s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 89%|████████▉ | 451/507 [2:00:20<16:43, 17.92s/it]                                                   {'loss': 0.0034, 'learning_rate': 6.350844013592061e-06, 'epoch': 2.66}
 89%|████████▉ | 451/507 [2:00:20<16:43, 17.92s/it] 89%|████████▉ | 452/507 [2:00:36<15:46, 17.21s/it]                                                   {'loss': 0.0271, 'learning_rate': 6.1283784760026494e-06, 'epoch': 2.67}
 89%|████████▉ | 452/507 [2:00:36<15:46, 17.21s/it] 89%|████████▉ | 453/507 [2:00:51<15:03, 16.73s/it]                                                   {'loss': 0.0349, 'learning_rate': 5.9097559314661214e-06, 'epoch': 2.68}
 89%|████████▉ | 453/507 [2:00:51<15:03, 16.73s/it] 90%|████████▉ | 454/507 [2:01:07<14:29, 16.40s/it]                                                   {'loss': 0.0086, 'learning_rate': 5.694985330130698e-06, 'epoch': 2.68}
 90%|████████▉ | 454/507 [2:01:07<14:29, 16.40s/it] 90%|████████▉ | 455/507 [2:01:23<14:01, 16.18s/it]                                                   {'loss': 0.0056, 'learning_rate': 5.484075464450455e-06, 'epoch': 2.69}
 90%|████████▉ | 455/507 [2:01:23<14:01, 16.18s/it] 90%|████████▉ | 456/507 [2:01:38<13:37, 16.02s/it]                                                   {'loss': 0.0028, 'learning_rate': 5.277034968825667e-06, 'epoch': 2.69}
 90%|████████▉ | 456/507 [2:01:38<13:37, 16.02s/it] 90%|█████████ | 457/507 [2:01:54<13:15, 15.92s/it]                                                   {'loss': 0.0171, 'learning_rate': 5.073872319249073e-06, 'epoch': 2.7}
 90%|█████████ | 457/507 [2:01:54<13:15, 15.92s/it] 90%|█████████ | 458/507 [2:02:10<12:55, 15.84s/it]                                                   {'loss': 0.0159, 'learning_rate': 4.8745958329590615e-06, 'epoch': 2.71}
 90%|█████████ | 458/507 [2:02:10<12:55, 15.84s/it] 91%|█████████ | 459/507 [2:02:25<12:38, 15.80s/it]                                                   {'loss': 0.016, 'learning_rate': 4.679213668099036e-06, 'epoch': 2.71}
 91%|█████████ | 459/507 [2:02:25<12:38, 15.80s/it] 91%|█████████ | 460/507 [2:02:41<12:21, 15.78s/it]                                                   {'loss': 0.0326, 'learning_rate': 4.487733823383522e-06, 'epoch': 2.72}
 91%|█████████ | 460/507 [2:02:41<12:21, 15.78s/it] 91%|█████████ | 461/507 [2:02:57<12:05, 15.78s/it]                                                   {'loss': 0.0146, 'learning_rate': 4.3001641377707125e-06, 'epoch': 2.72}
 91%|█████████ | 461/507 [2:02:57<12:05, 15.78s/it] 91%|█████████ | 462/507 [2:03:13<11:49, 15.76s/it]                                                   {'loss': 0.0116, 'learning_rate': 4.116512290141405e-06, 'epoch': 2.73}
 91%|█████████ | 462/507 [2:03:13<11:49, 15.76s/it] 91%|█████████▏| 463/507 [2:03:28<11:33, 15.77s/it]                                                   {'loss': 0.0149, 'learning_rate': 3.936785798984877e-06, 'epoch': 2.74}
 91%|█████████▏| 463/507 [2:03:28<11:33, 15.77s/it] 92%|█████████▏| 464/507 [2:03:44<11:18, 15.77s/it]                                                   {'loss': 0.0226, 'learning_rate': 3.7609920220908813e-06, 'epoch': 2.74}
 92%|█████████▏| 464/507 [2:03:44<11:18, 15.77s/it] 92%|█████████▏| 465/507 [2:04:00<11:02, 15.77s/it]                                                   {'loss': 0.0059, 'learning_rate': 3.5891381562485504e-06, 'epoch': 2.75}
 92%|█████████▏| 465/507 [2:04:00<11:02, 15.77s/it] 92%|█████████▏| 466/507 [2:04:16<10:45, 15.75s/it]                                                   {'loss': 0.0121, 'learning_rate': 3.4212312369516497e-06, 'epoch': 2.75}
 92%|█████████▏| 466/507 [2:04:16<10:45, 15.75s/it] 92%|█████████▏| 467/507 [2:04:31<10:31, 15.78s/it]                                                   {'loss': 0.0137, 'learning_rate': 3.2572781381107197e-06, 'epoch': 2.76}
 92%|█████████▏| 467/507 [2:04:31<10:31, 15.78s/it] 92%|█████████▏| 468/507 [2:04:47<10:16, 15.81s/it]                                                   {'loss': 0.0058, 'learning_rate': 3.0972855717715134e-06, 'epoch': 2.77}
 92%|█████████▏| 468/507 [2:04:47<10:16, 15.81s/it] 93%|█████████▎| 469/507 [2:05:03<10:00, 15.80s/it]                                                   {'loss': 0.0164, 'learning_rate': 2.9412600878402697e-06, 'epoch': 2.77}
 93%|█████████▎| 469/507 [2:05:03<10:00, 15.80s/it] 93%|█████████▎| 470/507 [2:05:19<09:44, 15.81s/it]                                                   {'loss': 0.0291, 'learning_rate': 2.789208073815608e-06, 'epoch': 2.78}
 93%|█████████▎| 470/507 [2:05:19<09:44, 15.81s/it] 93%|█████████▎| 471/507 [2:05:35<09:28, 15.81s/it]                                                   {'loss': 0.0087, 'learning_rate': 2.6411357545269577e-06, 'epoch': 2.78}
 93%|█████████▎| 471/507 [2:05:35<09:28, 15.81s/it] 93%|█████████▎| 472/507 [2:05:50<09:12, 15.79s/it]                                                   {'loss': 0.0125, 'learning_rate': 2.4970491918797854e-06, 'epoch': 2.79}
 93%|█████████▎| 472/507 [2:05:50<09:12, 15.79s/it] 93%|█████████▎| 473/507 [2:06:06<08:56, 15.78s/it]                                                   {'loss': 0.0124, 'learning_rate': 2.35695428460736e-06, 'epoch': 2.79}
 93%|█████████▎| 473/507 [2:06:06<08:56, 15.78s/it] 93%|█████████▎| 474/507 [2:06:22<08:40, 15.78s/it]                                                   {'loss': 0.0183, 'learning_rate': 2.2208567680293667e-06, 'epoch': 2.8}
 93%|█████████▎| 474/507 [2:06:22<08:40, 15.78s/it] 94%|█████████▎| 475/507 [2:06:38<08:24, 15.77s/it]                                                   {'loss': 0.0153, 'learning_rate': 2.088762213816986e-06, 'epoch': 2.81}
 94%|█████████▎| 475/507 [2:06:38<08:24, 15.77s/it] 94%|█████████▍| 476/507 [2:06:53<08:08, 15.77s/it]                                                   {'loss': 0.0122, 'learning_rate': 1.960676029764874e-06, 'epoch': 2.81}
 94%|█████████▍| 476/507 [2:06:53<08:08, 15.77s/it] 94%|█████████▍| 477/507 [2:07:09<07:52, 15.76s/it]                                                   {'loss': 0.0189, 'learning_rate': 1.8366034595698078e-06, 'epoch': 2.82}
 94%|█████████▍| 477/507 [2:07:09<07:52, 15.76s/it] 94%|█████████▍| 478/507 [2:07:25<07:36, 15.74s/it]                                                   {'loss': 0.0231, 'learning_rate': 1.7165495826158896e-06, 'epoch': 2.82}
 94%|█████████▍| 478/507 [2:07:25<07:36, 15.74s/it] 94%|█████████▍| 479/507 [2:07:41<07:20, 15.73s/it]                                                   {'loss': 0.0032, 'learning_rate': 1.600519313766724e-06, 'epoch': 2.83}
 94%|█████████▍| 479/507 [2:07:41<07:20, 15.73s/it] 95%|█████████▍| 480/507 [2:07:56<07:04, 15.73s/it]                                                   {'loss': 0.0054, 'learning_rate': 1.4885174031641469e-06, 'epoch': 2.84}
 95%|█████████▍| 480/507 [2:07:56<07:04, 15.73s/it] 95%|█████████▍| 481/507 [2:08:12<06:50, 15.77s/it]                                                   {'loss': 0.0076, 'learning_rate': 1.3805484360337906e-06, 'epoch': 2.84}
 95%|█████████▍| 481/507 [2:08:12<06:50, 15.77s/it] 95%|█████████▌| 482/507 [2:08:28<06:33, 15.74s/it]                                                   {'loss': 0.0151, 'learning_rate': 1.276616832497346e-06, 'epoch': 2.85}
 95%|█████████▌| 482/507 [2:08:28<06:33, 15.74s/it] 95%|█████████▌| 483/507 [2:08:44<06:17, 15.73s/it]                                                   {'loss': 0.0049, 'learning_rate': 1.1767268473916182e-06, 'epoch': 2.85}
 95%|█████████▌| 483/507 [2:08:44<06:17, 15.73s/it] 95%|█████████▌| 484/507 [2:08:59<06:01, 15.71s/it]                                                   {'loss': 0.0227, 'learning_rate': 1.0808825700943438e-06, 'epoch': 2.86}
 95%|█████████▌| 484/507 [2:08:59<06:01, 15.71s/it] 96%|█████████▌| 485/507 [2:09:15<05:45, 15.69s/it]                                                   {'loss': 0.0175, 'learning_rate': 9.890879243567686e-07, 'epoch': 2.87}
 96%|█████████▌| 485/507 [2:09:15<05:45, 15.69s/it] 96%|█████████▌| 486/507 [2:09:31<05:29, 15.68s/it]                                                   {'loss': 0.0046, 'learning_rate': 9.013466681429994e-07, 'epoch': 2.87}
 96%|█████████▌| 486/507 [2:09:31<05:29, 15.68s/it] 96%|█████████▌| 487/507 [2:09:46<05:13, 15.68s/it]                                                   {'loss': 0.0125, 'learning_rate': 8.17662393476204e-07, 'epoch': 2.88}
 96%|█████████▌| 487/507 [2:09:46<05:13, 15.68s/it] 96%|█████████▋| 488/507 [2:10:02<04:57, 15.67s/it]                                                   {'loss': 0.0175, 'learning_rate': 7.380385262915179e-07, 'epoch': 2.88}
 96%|█████████▋| 488/507 [2:10:02<04:57, 15.67s/it] 96%|█████████▋| 489/507 [2:10:18<04:42, 15.68s/it]                                                   {'loss': 0.0148, 'learning_rate': 6.624783262958012e-07, 'epoch': 2.89}
 96%|█████████▋| 489/507 [2:10:18<04:42, 15.68s/it] 97%|█████████▋| 490/507 [2:10:35<04:33, 16.10s/it]                                                   {'loss': 0.0132, 'learning_rate': 5.909848868341783e-07, 'epoch': 2.9}
 97%|█████████▋| 490/507 [2:10:35<04:33, 16.10s/it] 97%|█████████▋| 491/507 [2:10:50<04:15, 15.96s/it]                                                   {'loss': 0.0127, 'learning_rate': 5.235611347634172e-07, 'epoch': 2.9}
 97%|█████████▋| 491/507 [2:10:50<04:15, 15.96s/it] 97%|█████████▋| 492/507 [2:11:06<03:58, 15.88s/it]                                                   {'loss': 0.0076, 'learning_rate': 4.602098303321256e-07, 'epoch': 2.91}
 97%|█████████▋| 492/507 [2:11:06<03:58, 15.88s/it] 97%|█████████▋| 493/507 [2:11:22<03:41, 15.82s/it]                                                   {'loss': 0.0037, 'learning_rate': 4.00933567067685e-07, 'epoch': 2.91}
 97%|█████████▋| 493/507 [2:11:22<03:41, 15.82s/it] 97%|█████████▋| 494/507 [2:11:37<03:25, 15.79s/it]                                                   {'loss': 0.0154, 'learning_rate': 3.4573477167015866e-07, 'epoch': 2.92}
 97%|█████████▋| 494/507 [2:11:37<03:25, 15.79s/it] 98%|█████████▊| 495/507 [2:11:53<03:09, 15.77s/it]                                                   {'loss': 0.0314, 'learning_rate': 2.9461570391287055e-07, 'epoch': 2.92}
 98%|█████████▊| 495/507 [2:11:53<03:09, 15.77s/it] 98%|█████████▊| 496/507 [2:12:09<02:53, 15.77s/it]                                                   {'loss': 0.0088, 'learning_rate': 2.4757845654992397e-07, 'epoch': 2.93}
 98%|█████████▊| 496/507 [2:12:09<02:53, 15.77s/it] 98%|█████████▊| 497/507 [2:12:25<02:37, 15.76s/it]                                                   {'loss': 0.005, 'learning_rate': 2.0462495523057011e-07, 'epoch': 2.94}
 98%|█████████▊| 497/507 [2:12:25<02:37, 15.76s/it] 98%|█████████▊| 498/507 [2:12:40<02:21, 15.75s/it]                                                   {'loss': 0.002, 'learning_rate': 1.6575695842027117e-07, 'epoch': 2.94}
 98%|█████████▊| 498/507 [2:12:40<02:21, 15.75s/it] 98%|█████████▊| 499/507 [2:12:56<02:06, 15.75s/it]                                                   {'loss': 0.0135, 'learning_rate': 1.3097605732882435e-07, 'epoch': 2.95}
 98%|█████████▊| 499/507 [2:12:56<02:06, 15.75s/it] 99%|█████████▊| 500/507 [2:13:12<01:50, 15.76s/it]                                                   {'loss': 0.0067, 'learning_rate': 1.0028367584512532e-07, 'epoch': 2.95}
 99%|█████████▊| 500/507 [2:13:12<01:50, 15.76s/it]/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data3/jisu/miniconda3/envs/mfm-new/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 99%|█████████▉| 501/507 [2:13:35<01:47, 17.91s/it]                                                   {'loss': 0.0069, 'learning_rate': 7.368107047894812e-08, 'epoch': 2.96}
 99%|█████████▉| 501/507 [2:13:35<01:47, 17.91s/it] 99%|█████████▉| 502/507 [2:13:50<01:25, 17.19s/it]                                                   {'loss': 0.0176, 'learning_rate': 5.116933030946402e-08, 'epoch': 2.97}
 99%|█████████▉| 502/507 [2:13:50<01:25, 17.19s/it] 99%|█████████▉| 503/507 [2:14:06<01:06, 16.74s/it]                                                   {'loss': 0.0079, 'learning_rate': 3.2749376940655054e-08, 'epoch': 2.97}
 99%|█████████▉| 503/507 [2:14:06<01:06, 16.74s/it] 99%|█████████▉| 504/507 [2:14:22<00:49, 16.43s/it]                                                   {'loss': 0.0146, 'learning_rate': 1.8421964463610774e-08, 'epoch': 2.98}
 99%|█████████▉| 504/507 [2:14:22<00:49, 16.43s/it]100%|█████████▉| 505/507 [2:14:38<00:32, 16.23s/it]                                                   {'loss': 0.0058, 'learning_rate': 8.187679425630812e-09, 'epoch': 2.98}
100%|█████████▉| 505/507 [2:14:38<00:32, 16.23s/it]100%|█████████▉| 506/507 [2:14:53<00:16, 16.08s/it]                                                   {'loss': 0.011, 'learning_rate': 2.046940806244013e-09, 'epoch': 2.99}
100%|█████████▉| 506/507 [2:14:53<00:16, 16.08s/it]100%|██████████| 507/507 [2:15:09<00:00, 16.00s/it]                                                   {'loss': 0.0187, 'learning_rate': 0.0, 'epoch': 3.0}
100%|██████████| 507/507 [2:15:09<00:00, 16.00s/it]                                                   {'train_runtime': 8109.6682, 'train_samples_per_second': 4.003, 'train_steps_per_second': 0.063, 'train_loss': 0.046418563387020544, 'epoch': 3.0}
100%|██████████| 507/507 [2:15:09<00:00, 16.00s/it]100%|██████████| 507/507 [2:15:09<00:00, 16.00s/it]
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[2026-01-11 01:19:30,763] [INFO] [launch.py:347:main] Process 683433 exits successfully.
[2026-01-11 01:19:34,764] [INFO] [launch.py:347:main] Process 683434 exits successfully.
[2026-01-11 01:19:34,764] [INFO] [launch.py:347:main] Process 683431 exits successfully.
[2026-01-11 01:19:36,765] [INFO] [launch.py:347:main] Process 683432 exits successfully.
